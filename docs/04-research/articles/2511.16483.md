# LLM-Guided Reward Design for Cyber Defense

**Source:** [Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense](https://arxiv.org/html/2511.16483)

## Synthesis
This paper explores using **Large Language Models (LLMs)** to design the **Reward Functions** for Deep Reinforcement Learning (DRL) agents in a cyber defense simulation.
*   **The Problem:** Designing reward functions for RL is hard. It requires expert knowledge to balance immediate rewards vs. long-term goals.
*   **The Solution:** Use an LLM (Claude Sonnet) to *write* the reward function configuration (YAML) based on high-level persona descriptions (e.g., "Make the attacker aggressive", "Make the defender proactive").
*   **The Environment:** Cyberwheel, a high-fidelity network simulation.
*   **Result:** LLM-designed rewards produced defense policies that were more robust against diverse attacker personas (Aggressive vs Stealthy) than baseline policies. Specifically, a "Proactive-v2" defender (designed by LLM to value decoy placement but be mindful of costs) delayed attackers significantly longer.

## Core Strategic Ideas
1.  **LLM as Reward Engineer:** Instead of having a human tune the scalars in a reward function ($r_t = \alpha \cdot \text{capture} - \beta \cdot \text{cost}$), ask the LLM: "Given this scenario, write a reward function that encourages stealth."
2.  **Persona-Based Training:** Train different agents against different "Personas" (Aggressive, Stealthy) defined by their reward functions. This prevents overfitting to a single adversary type.
3.  **Mixed Strategy Deployment:** The best defense wasn't a single policy. It was a meta-strategy that switched between "Baseline" and "Proactive" policies depending on the observed attacker behavior.

## Integration Plan (Agno + SurrealDB + Gemini 3)
We can apply "LLM-Guided Rewards" to **Optimization Problems** in CodeSwarm:

1.  **Auto-Tuning Agents:**
    *   If we have a "Scheduler Agent" that assigns tasks to workers, how do we tune its logic?
    *   Use Gemini 3 to write the *scoring function* for the scheduler.
    *   Prompt: "Write a Python function to score task assignments. Prioritize minimizing latency for 'frontend' tasks but throughput for 'backend' tasks."

2.  **SurrealDB as Environment:**
    *   The "State" is the current list of open PRs and Issues in `SurrealDB`.
    *   The "Action" is assigning an issue to an agent.
    *   The "Reward" is defined by the LLM-generated function (e.g., did the issue get closed quickly?).

3.  **Persona-Based Testing:**
    *   Create "Chaos Monkey" agents with different personas (e.g., "The Spammer" who opens 100 issues, "The Nitpicker" who comments on every line of code).
    *   Use these to stress-test our main orchestration agents.
