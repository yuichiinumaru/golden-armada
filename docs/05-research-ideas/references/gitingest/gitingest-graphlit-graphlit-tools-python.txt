Directory structure:
└── graphlit-graphlit-tools-python/
    ├── README.md
    ├── __init__.py
    ├── azure-pipelines.yml
    ├── LICENSE
    ├── pyproject.toml
    ├── setup.py
    ├── .pylintrc
    └── graphlit_tools/
        ├── __init__.py
        ├── base_tool.py
        ├── crewai_converter.py
        ├── exceptions.py
        ├── griptape_converter.py
        ├── helpers.py
        ├── extraction/
        │   ├── __init__.py
        │   ├── extract_text_tool.py
        │   ├── extract_url_tool.py
        │   └── extract_web_page_tool.py
        ├── generation/
        │   ├── __init__.py
        │   ├── describe_image_tool.py
        │   ├── describe_web_page_tool.py
        │   ├── generate_bullets_tool.py
        │   ├── generate_chapters_tool.py
        │   ├── generate_headlines_tool.py
        │   ├── generate_keywords_tool.py
        │   ├── generate_questions_tool.py
        │   ├── generate_social_media_posts_tool.py
        │   ├── generate_summary_tool.py
        │   └── prompt_tool.py
        ├── ingestion/
        │   ├── __init__.py
        │   ├── discord_ingest_tool.py
        │   ├── github_issue_ingest_tool.py
        │   ├── google_email_ingest_tool.py
        │   ├── jira_issue_ingest_tool.py
        │   ├── linear_issue_ingest_tool.py
        │   ├── local_ingest_tool.py
        │   ├── microsoft_email_ingest_tool.py
        │   ├── microsoft_teams_ingest_tool.py
        │   ├── notion_ingest_tool.py
        │   ├── reddit_ingest_tool.py
        │   ├── rss_ingest_tool.py
        │   ├── slack_ingest_tool.py
        │   ├── url_ingest_tool.py
        │   ├── web_crawl_tool.py
        │   ├── web_map_tool.py
        │   ├── web_scrape_tool.py
        │   └── web_search_tool.py
        └── retrieval/
            ├── __init__.py
            ├── content_retrieval_tool.py
            ├── organization_retrieval_tool.py
            └── person_retrieval_tool.py

================================================
FILE: README.md
================================================
[![PyPI version](https://badge.fury.io/py/graphlit-tools.svg)](https://badge.fury.io/py/graphlit-tools)

# Python Agent Tools for Graphlit Platform

## Overview

The Graphlit Agent Tools for Python enables easy interaction with agent frameworks such as [CrewAI](https://crewai.com) or [Griptape](https://www.griptape.ai/), allowing developers to easily integrate the Graphlit service with agentic workflows. This document outlines the setup process and provides a basic example of using the tools.

## Prerequisites

Before you begin, ensure you have the following:

- Python 3.x installed on your system.
- An active account on the [Graphlit Platform](https://portal.graphlit.dev) with access to the API settings dashboard.

## Installation

To install the Graphlit Agent Tools with CrewAI, use pip:

```bash
pip install graphlit-tools[crewai]
```

To install the Graphlit Agent Tools with Griptape, use pip:

```bash
pip install graphlit-tools[griptape]
```

### Using the Graphlit agent tools

We have example Google Colab notebooks using CrewAI, which provide an example for [analyzing the web marketing strategy of a company](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_07_CrewAI_Web_Marketing_Analyzer.ipynb), and for [structured data extraction of products from scraped web pages](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_08_CrewAI_Product_Data_Extraction.ipynb).

Once you have configured the Graphlit client, as shown below, you will pass the client to the tool constructor.

For use in CrewAI, you will need to convert the tool to the CrewAI tool schema with the `CrewAIConverter.from_tool()` function.  

For use in Griptape, you will need to convert the tool to the CrewAI tool schema with the `GriptapeConverter.from_tool()` function.

We will provide support for additional agent frameworks, such as LangGraph and AutoGen in future.

#### CrewAI

```python
from graphlit_tools import WebSearchTool, CrewAIConverter

web_search_tool = CrewAIConverter.from_tool(WebSearchTool(graphlit))

web_search_agent = Agent(
    role="Web Researcher",
    goal="Find the {company} website.",
    backstory="",
    verbose=True,
    allow_delegation=False,
    tools=[web_search_tool],
)
```

#### Griptape

```python
from graphlit_tools import WebSearchTool, CrewAIConverter

web_search_tool = GriptapeConverter.from_tool(WebSearchTool(graphlit))

web_search_agent = Agent(
    role="Web Researcher",
    goal="Find the {company} website.",
    backstory="",
    verbose=True,
    allow_delegation=False,
    tools=[web_search_tool],
)
```

## Configuration

The Graphlit Client supports environment variables to be set for authentication and configuration:

- `GRAPHLIT_ENVIRONMENT_ID`: Your environment ID.
- `GRAPHLIT_ORGANIZATION_ID`: Your organization ID.
- `GRAPHLIT_JWT_SECRET`: Your JWT secret for signing the JWT token.

Alternately, you can pass these values with the constructor of the Graphlit client.

You can find these values in the API settings dashboard on the [Graphlit Platform](https://portal.graphlit.dev).

For example, to use Graphlit in a Google Colab notebook, you need to assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.

```python
import os
from google.colab import userdata
from graphlit import Graphlit

os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')
os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')
os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')

graphlit = Graphlit()
```

### Setting Environment Variables

To set these environment variables on your system, use the following commands, replacing `your_value` with the actual values from your account.

For Unix/Linux/macOS:

```bash
export GRAPHLIT_ENVIRONMENT_ID=your_environment_id_value
export GRAPHLIT_ORGANIZATION_ID=your_organization_id_value
export GRAPHLIT_JWT_SECRET=your_secret_key_value
```

For Windows Command Prompt (CMD):

```cmd
set GRAPHLIT_ENVIRONMENT_ID=your_environment_id_value
set GRAPHLIT_ORGANIZATION_ID=your_organization_id_value
set GRAPHLIT_JWT_SECRET=your_secret_key_value
```

For Windows PowerShell:

```powershell
$env:GRAPHLIT_ENVIRONMENT_ID="your_environment_id_value"
$env:GRAPHLIT_ORGANIZATION_ID="your_organization_id_value"
$env:GRAPHLIT_JWT_SECRET="your_secret_key_value"
```

## Tools

- [Content Ingestion](#content-ingestion)
- [RAG](#rag)
- [Data Retrieval](#data-retrieval)
- [Content Generation](#content-generation)
- [Image Description](#image-description)
- [Data Extraction](#data-extraction)

### Content Ingestion

#### URLIngestTool: Graphlit URL ingest tool
##### Description
Ingests content from URL.
Returns extracted Markdown text and metadata from content.
Can ingest individual Word documents, PDFs, audio recordings, videos, images, or any other unstructured data.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| url | str | URL of cloud-hosted file to be ingested into knowledge base |

#### LocalIngestTool: Graphlit local file ingest tool
##### Description
Ingests content from local file.
Returns extracted Markdown text and metadata from content.
Can ingest individual Word documents, PDFs, audio recordings, videos, images, or any other unstructured data.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| file_path | str | Path of local file to be ingested into knowledge base |

#### WebScrapeTool: Graphlit web scrape tool
##### Description
Scrapes web page into knowledge base.
Returns Markdown text and metadata extracted from web page.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| url | str | URL of web page to be scraped and ingested into knowledge base |

#### WebCrawlTool: Graphlit web crawl tool
##### Description
Crawls web pages from web site into knowledge base.
Returns Markdown text and metadata extracted from web pages.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| url | str | URL of web site to be crawled and ingested into knowledge base |
| search | Optional[str] | Text to search for within ingested web pages |
| read_limit | Optional[int] | Maximum number of web pages from web site to be crawled |

#### WebSearchTool: Graphlit web search tool
##### Description
Accepts search query text as string.
Performs web search based on search query.
Returns Markdown text and metadata extracted from web pages.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| search | str | Text to search for within web pages across the Internet |
| search_limit | Optional[int] | Maximum number of web pages to be returned from web search |

#### WebMapTool: Graphlit web map tool
##### Description
Accepts web page URL as string.
Enumerates the web pages at or beneath the provided URL using web sitemap.
Returns list of mapped URIs from web site.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| url | str | URL of the web page to be mapped |

#### RedditIngestTool: Graphlit Reddit ingest tool
##### Description
Ingests posts from Reddit subreddit into knowledge base.
Returns extracted Markdown text and metadata from Reddit posts.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| subreddit_name | str | Reddit subreddit name to be read and ingested into knowledge base |
| search | Optional[str] | Text to search for within ingested posts |
| read_limit | Optional[int] | Maximum number of posts from Reddit subreddit to be read, defaults to 10 |

#### NotionIngestTool: Graphlit Notion ingest tool
##### Description
Ingests pages from Notion database into knowledge base.
Returns extracted Markdown text and metadata from Notion pages.

Requires NOTION_API_KEY to be assigned as environment variable.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| search | Optional[str] | Text to search for within ingested pages |
| read_limit | Optional[int] | Maximum number of pages from Notion database to be read, defaults to 10 |

#### RSSIngestTool: Graphlit RSS ingest tool
##### Description
Ingests posts from RSS feed into knowledge base.
For podcast RSS feeds, audio will be transcribed and ingested into knowledge base.
Returns extracted or transcribed Markdown text and metadata from RSS posts.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| url | str | RSS URL to be read and ingested into knowledge base |
| search | Optional[str] | Text to search for within ingested posts and/or transcripts |
| read_limit | Optional[int] | Maximum number of posts from RSS feed to be read, defaults to 10 |

#### MicrosoftEmailIngestTool: Graphlit Microsoft Email ingest tool
##### Description
Ingests emails from Microsoft Email account into knowledge base.
Returns extracted Markdown text and metadata from emails.

Requires MICROSOFT_EMAIL_CLIENT_ID, MICROSOFT_EMAIL_CLIENT_SECRET and MICROSOFT_EMAIL_REFRESH_TOKEN to be assigned as environment variables.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| search | Optional[str] | Text to search for within ingested email |
| read_limit | Optional[int] | Maximum number of emails from Microsoft Email account to be read, defaults to 10 |

#### GoogleEmailIngestTool: Graphlit Google Email ingest tool
##### Description
Ingests emails from Google Email account into knowledge base.
Returns extracted Markdown text and metadata from emails.

Requires GOOGLE_EMAIL_CLIENT_ID, GOOGLE_EMAIL_CLIENT_SECRET and GOOGLE_EMAIL_REFRESH_TOKEN to be assigned as environment variables.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| search | Optional[str] | Text to search for within ingested email |
| read_limit | Optional[int] | Maximum number of emails from Google Email account to be read, defaults to 10 |

#### GitHubIssueIngestTool: Graphlit GitHub Issue ingest tool
##### Description
Ingests issues from GitHub repository into knowledge base.
Accepts GitHub repository owner and repository name.
For example, for GitHub repository (https://github.com/openai/tiktoken), 'openai' is the repository owner, and 'tiktoken' is the repository name.
Returns extracted Markdown text and metadata from issues.

Requires GITHUB_PERSONAL_ACCESS_TOKEN to be assigned as environment variable.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| repository_name | str | GitHub repository name |
| repository_owner | str | GitHub repository owner |
| search | Optional[str] | Text to search for within ingested issues |
| read_limit | Optional[int] | Maximum number of issues from GitHub repository to be read, defaults to 10 |

#### JiraIssueIngestTool: Graphlit Jira ingest tool
##### Description
Ingests issues from Atlassian Jira into knowledge base.
Accepts Atlassian Jira server URL and project name.
Returns extracted Markdown text and metadata from issues.

Requires JIRA_TOKEN and JIRA_EMAIL to be assigned as environment variables.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| url | str | Atlassian Jira server URL |
| project | str | Atlassian Jira project name |
| search | Optional[str] | Text to search for within ingested issues |
| read_limit | Optional[int] | Maximum number of issues from Jira project to be read, defaults to 10 |

#### LinearIssueIngestTool: Graphlit Linear ingest tool
##### Description
Ingests issues from Linear project into knowledge base.
Accepts Linear project name.
Returns extracted Markdown text and metadata from issues.

Requires LINEAR_API_KEY to be assigned as environment variable.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| project | str | Linear project name |
| search | Optional[str] | Text to search for within ingested issues |
| read_limit | Optional[int] | Maximum number of issues from Linear project to be read, defaults to 10 |

#### MicrosoftTeamsIngestTool: Graphlit Microsoft Teams ingest tool
##### Description
Ingests messages from Microsoft Teams channel into knowledge base.
Returns extracted Markdown text and metadata from messages.

Requires MICROSOFT_TEAMS_CLIENT_ID, MICROSOFT_TEAMS_CLIENT_SECRET and MICROSOFT_TEAMS_REFRESH_TOKEN to be assigned as environment variables.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| team_name | str | Microsoft Teams team name |
| channel_name | str | Microsoft Teams channel name |
| search | Optional[str] | Text to search for within ingested messages |
| read_limit | Optional[int] | Maximum number of messages from Microsoft Teams channel to be read, defaults to 10 |

#### DiscordIngestTool: Graphlit Discord ingest tool
##### Description
Ingests messages from Discord channel into knowledge base.
Accepts Discord channel name.
Returns extracted Markdown text and metadata from messages.

Requires DISCORD_BOT_TOKEN to be assigned as environment variable.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| channel_name | str | Discord channel name |
| search | Optional[str] | Text to search for within ingested messages |
| read_limit | Optional[int] | Maximum number of messages from Discord channel to be read, defaults to 10 |

#### SlackIngestTool: Graphlit Slack ingest tool
##### Description
Ingests messages from Slack channel into knowledge base.
Accepts Slack channel name.
Returns extracted Markdown text and metadata from messages.

Requires SLACK_BOT_TOKEN to be assigned as environment variable.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| channel_name | str | Slack channel name |
| search | Optional[str] | Text to search for within ingested messages |
| read_limit | Optional[int] | Maximum number of messages from Slack channel to be read, defaults to 10 |

### RAG

#### PromptTool: Graphlit RAG prompt tool
##### Description
Accepts user prompt as string.
Prompts LLM with relevant content and returns completion from RAG pipeline. Returns Markdown text from LLM completion.
Uses vector embeddings and similarity search to retrieve relevant content from knowledge base.
Can search through web pages, PDFs, audio transcripts, and other unstructured data.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| prompt | str | Text prompt which is provided to LLM for completion, via RAG pipeline |

### Data Retrieval

#### ContentRetrievalTool: Graphlit content retrieval tool
##### Description
Accepts search text as string.
Optionally accepts a list of content types (i.e. FILE, PAGE, EMAIL, ISSUE, MESSAGE) for filtering the result set.
Retrieves contents based on similarity search from knowledge base.
Returns extracted Markdown text and metadata from contents relevant to the search text.
Can search through web pages, PDFs, audio transcripts, Slack messages, emails, or any unstructured data ingested into the knowledge base.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| text | str | Text to search for within the knowledge base |
| types | Optional[List[ContentTypes]] | List of content types (i.e. FILE, PAGE, EMAIL, ISSUE, MESSAGE) to be returned from knowledge base |
| limit | Optional[int] | Number of contents to return from search query |

#### PersonRetrievalTool: Graphlit person retrieval tool
##### Description
Accepts search text as string.
Retrieves persons based on similarity search from knowledge base.
Returns metadata from persons relevant to the search text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| search | str | Text to search for within the knowledge base |
| limit | Optional[int] | Number of persons to return from search query |

#### OrganizationRetrievalTool: Graphlit organization retrieval tool
##### Description
Accepts search text as string.
Retrieves organizations based on similarity search from knowledge base.
Returns metadata from organizations relevant to the search text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| search | str | Text to search for within the knowledge base |
| limit | Optional[int] | Number of organizations to return from search query |

### Image Description

#### DescribeImageTool: Graphlit image description tool
##### Description
Accepts image URL as string.
Prompts vision LLM and returns completion. Returns Markdown text from LLM completion.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| url | str | URL for image to be described with vision LLM |
| prompt | str | Text prompt which is provided to vision LLM for completion |

#### DescribeWebPageTool: Graphlit screenshot web page tool
##### Description
Screenshots web page from URL and describes web page with vision LLM.
Returns Markdown description of screenshot and extracted Markdown text from image.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| url | str | URL of web page to screenshot and ingest into knowledge base |
| prompt | Optional[str] | Text prompt which is provided to vision LLM for screenshot description |

### Content Generation

#### GenerateSummaryTool: Graphlit summary generation tool
##### Description
Accepts text as string.
Optionally accepts text prompt to be provided to LLM for text summarization.
Returns summary as text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| text | str | Text to be summarized |
| prompt | Optional[str] | Text prompt which is provided to LLM for text summarization |

#### GenerateBulletsTool: Graphlit bullet points generation tool
##### Description
Accepts text as string.
Optionally accepts the count of bullet points to be generated.
Returns bullet points as text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| text | str | Text to be summarized into bullet points |
| count | Optional[int] | Number of bullet points to be generated |

#### GenerateHeadlinesTool: Graphlit headlines generation tool
##### Description
Accepts text as string.
Optionally accepts the count of headlines to be generated.
Returns headlines as text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| text | str | Text to be summarized into headlines |
| count | Optional[int] | Number of headlines to be generated |

#### GenerateSocialMediaPostsTool: : Graphlit social media posts generation tool
##### Description
Accepts text as string.
Optionally accepts the count of social media posts to be generated.
Returns social media posts as text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| text | str | Text to be summarized into social media posts |
| count | Optional[int] | Number of social media posts to be generated |

#### GenerateQuestionsTool: Graphlit followup questions generation tool
##### Description
Accepts text as string.
Optionally accepts the count of followup questions to be generated.
Returns followup questions as text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| text | str | Text to be summarized into followup questions |
| count | Optional[int] | Number of followup questions to be generated |

#### GenerateKeywordsTool: Graphlit keywords generation tool
##### Description
Accepts text as string.
Optionally accepts the count of keywords to be generated.
Returns keywords as text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| text | str | Text to be summarized into keywords |
| count | Optional[int] | Number of keywords to be generated |

#### GenerateChaptersTool: Graphlit transcript chapters generation tool
##### Description
Accepts transcript as string.
Returns chapters as text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| text | str | Transcript to be summarized into chapters. Assumes transcript contains time-stamped text. |

### Data Extraction

#### ExtractURLTool: Graphlit JSON URL data extraction tool
##### Description
Extracts JSON data from ingested file using LLM.
Accepts URL to be ingested, and JSON schema of Pydantic model to be extracted into. JSON schema needs be of type 'object' and include 'properties' and 'required' fields.
Returns extracted JSON from file.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| uri | str | URL of cloud-hosted file to be ingested into knowledge base |
| model_schema | str | Pydantic model JSON schema which describes the data which will be extracted. JSON schema needs be of type 'object' and include 'properties' and 'required' fields. |
| prompt | Optional[str] | Text prompt which is provided to LLM to guide data extraction |

#### ExtractWebPageTool: Graphlit JSON web page data extraction tool
##### Description
Extracts JSON data from ingested web page using LLM.
Accepts URL to be scraped, and JSON schema of Pydantic model to be extracted into. JSON schema needs be of type 'object' and include 'properties' and 'required' fields.
Returns extracted JSON from web page.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| uri | str | URL of web page to be scraped and ingested into knowledge base |
| model_schema | str | Pydantic model JSON schema which describes the data which will be extracted. JSON schema needs be of type 'object' and include 'properties' and 'required' fields. |
| prompt | Optional[str] | Text prompt which is provided to LLM to guide data extraction |

#### ExtractTextTool: Graphlit JSON text data extraction tool
##### Description
Extracts JSON data from text using LLM.
Accepts text to be scraped, and JSON schema of Pydantic model to be extracted into. JSON schema needs be of type 'object' and include 'properties' and 'required' fields.
Returns extracted JSON from text.

##### Parameters
| Name | Type | Description |
| ---- | ---- | ---- |
| text | str | Text to be extracted with LLM |
| model_schema | str | Pydantic model JSON schema which describes the data which will be extracted. JSON schema needs be of type 'object' and include 'properties' and 'required' fields. |
| prompt | Optional[str] | Text prompt which is provided to LLM to guide data extraction |

## Support

Please refer to the [Graphlit API Documentation](https://docs.graphlit.dev/).

For support with the Graphlit Agent Tools or to request an additional tool, please submit a [GitHub Issue](https://github.com/graphlit/graphlit-tools-python/issues).  

For further support with the Graphlit Platform, please join our [Discord](https://discord.gg/ygFmfjy3Qx) community.




================================================
FILE: __init__.py
================================================
from .graphlit_tools import (
    DescribeImageTool,
    DescribeWebPageTool,
    GenerateSummaryTool,
    GenerateBulletsTool,
    GenerateHeadlinesTool,
    GenerateSocialMediaPostsTool,
    GenerateQuestionsTool,
    GenerateKeywordsTool,
    GenerateChaptersTool,
    PromptTool,
    PromptToolInput,
    ExtractURLTool,
    ExtractWebPageTool,
    ExtractTextTool,
    PersonRetrievalTool,
    OrganizationRetrievalTool,
    ContentRetrievalTool,
    URLIngestTool,
    LocalIngestTool,
    WebScrapeTool,
    WebCrawlTool,
    WebSearchTool,
    WebMapTool,
    RedditIngestTool,
    NotionIngestTool,
    RSSIngestTool,
    MicrosoftEmailIngestTool,
    GoogleEmailIngestTool,
    GitHubIssueIngestTool,
    JiraIssueIngestTool,
    LinearIssueIngestTool,
    MicrosoftTeamsIngestTool,
    DiscordIngestTool,
    SlackIngestTool,
    CrewAIConverter,
    GriptapeConverter,
)



================================================
FILE: azure-pipelines.yml
================================================
trigger:
  branches:
    include:
      - main

name: $(Date:yyyyMMdd)$(Rev:rrr)

pool:
  vmImage: "ubuntu-latest"

steps:
  - task: UsePythonVersion@0
    inputs:
      versionSpec: "3.x"
      addToPath: true

  - script: |
      echo "##vso[task.setvariable variable=PACKAGE_VERSION]1.0.$(Build.BuildNumber)"
    displayName: "Set package version"

  - script: |
      python -m pip install --upgrade pip
      pip install setuptools wheel twine
      python setup.py sdist bdist_wheel
    displayName: "Install dependencies and build"

  - script: |
      twine upload dist/* -u __token__ -p $(PYPI_TOKEN) --skip-existing
    displayName: "Upload to PyPI"
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 Unstruk Data Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================



================================================
FILE: setup.py
================================================
import os
from setuptools import setup, find_packages

# Read the content of your README file
with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

version = os.getenv('PACKAGE_VERSION', '1.0.0')

setup(
    name='graphlit-tools',
    version=version,
    packages=find_packages(),
    install_requires=[
        'graphlit-client'        
    ],
    extras_require={
        "crewai": ["crewai"],  # Extras for CrewAI support
        "griptape": ["griptape"]  # Extras for Griptape support
    },
    python_requires='>=3.10',
    author='Unstruk Data Inc.',
    author_email='questions@graphlit.com',
    description='Graphlit Agent Tools',
    url='https://github.com/graphlit/graphlit-tools-python/',
    long_description=long_description,
    long_description_content_type="text/markdown",
)



================================================
FILE: .pylintrc
================================================
[MAIN]

# Analyse import fallback blocks. This can be used to support both Python 2 and
# 3 compatible code, which means that the block might have code that exists
# only in one or another interpreter, leading to false positives when analysed.
analyse-fallback-blocks=no

# Clear in-memory caches upon conclusion of linting. Useful if running pylint
# in a server-like mode.
clear-cache-post-run=no

# Load and enable all available extensions. Use --list-extensions to see a list
# all available extensions.
#enable-all-extensions=

# In error mode, messages with a category besides ERROR or FATAL are
# suppressed, and no reports are done by default. Error mode is compatible with
# disabling specific errors.
#errors-only=

# Always return a 0 (non-error) status code, even if lint errors are found.
# This is primarily useful in continuous integration scripts.
#exit-zero=

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code.
extension-pkg-allow-list=

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code. (This is an alternative name to extension-pkg-allow-list
# for backward compatibility.)
extension-pkg-whitelist=

# Return non-zero exit code if any of these messages/categories are detected,
# even if score is above --fail-under value. Syntax same as enable. Messages
# specified are enabled, while categories only check already-enabled messages.
fail-on=

# Specify a score threshold under which the program will exit with error.
fail-under=10

# Interpret the stdin as a python script, whose filename needs to be passed as
# the module_or_package argument.
#from-stdin=

# Files or directories to be skipped. They should be base names, not paths.
ignore=CVS

# Add files or directories matching the regular expressions patterns to the
# ignore-list. The regex matches against paths and can be in Posix or Windows
# format. Because '\\' represents the directory delimiter on Windows systems,
# it can't be used as an escape character.
ignore-paths=

# Files or directories matching the regular expression patterns are skipped.
# The regex matches against base names, not paths. The default value ignores
# Emacs file locks
ignore-patterns=^\.#

# List of module names for which member attributes should not be checked and
# will not be imported (useful for modules/projects where namespaces are
# manipulated during runtime and thus existing member attributes cannot be
# deduced by static analysis). It supports qualified module names, as well as
# Unix pattern matching.
ignored-modules=

# Python code to execute, usually for sys.path manipulation such as
# pygtk.require().
#init-hook=

# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
# number of processors available to use, and will cap the count on Windows to
# avoid hangs.
jobs=1

# Control the amount of potential inferred values when inferring a single
# object. This can help the performance when dealing with large functions or
# complex, nested conditions.
limit-inference-results=100

# List of plugins (as comma separated values of python module names) to load,
# usually to register additional checkers.
load-plugins=

# Pickle collected data for later comparisons.
persistent=yes

# Resolve imports to .pyi stubs if available. May reduce no-member messages and
# increase not-an-iterable messages.
prefer-stubs=no

# Minimum Python version to use for version dependent checks. Will default to
# the version used to run pylint.
py-version=3.10

# Discover python modules and packages in the file system subtree.
recursive=no

# Add paths to the list of the source roots. Supports globbing patterns. The
# source root is an absolute path or a path relative to the current working
# directory used to determine a package namespace for modules located under the
# source root.
source-roots=

# When enabled, pylint would attempt to guess common misconfiguration and emit
# user-friendly hints instead of false-positive error messages.
suggestion-mode=yes

# Allow loading of arbitrary C extensions. Extensions are imported into the
# active Python interpreter and may run arbitrary code.
unsafe-load-any-extension=no

# In verbose mode, extra non-checker-related info will be displayed.
#verbose=


[BASIC]

# Naming style matching correct argument names.
argument-naming-style=snake_case

# Regular expression matching correct argument names. Overrides argument-
# naming-style. If left empty, argument names will be checked with the set
# naming style.
#argument-rgx=

# Naming style matching correct attribute names.
attr-naming-style=snake_case

# Regular expression matching correct attribute names. Overrides attr-naming-
# style. If left empty, attribute names will be checked with the set naming
# style.
#attr-rgx=

# Bad variable names which should always be refused, separated by a comma.
bad-names=foo,
          bar,
          baz,
          toto,
          tutu,
          tata

# Bad variable names regexes, separated by a comma. If names match any regex,
# they will always be refused
bad-names-rgxs=

# Naming style matching correct class attribute names.
class-attribute-naming-style=any

# Regular expression matching correct class attribute names. Overrides class-
# attribute-naming-style. If left empty, class attribute names will be checked
# with the set naming style.
#class-attribute-rgx=

# Naming style matching correct class constant names.
class-const-naming-style=UPPER_CASE

# Regular expression matching correct class constant names. Overrides class-
# const-naming-style. If left empty, class constant names will be checked with
# the set naming style.
#class-const-rgx=

# Naming style matching correct class names.
class-naming-style=PascalCase

# Regular expression matching correct class names. Overrides class-naming-
# style. If left empty, class names will be checked with the set naming style.
#class-rgx=

# Naming style matching correct constant names.
const-naming-style=UPPER_CASE

# Regular expression matching correct constant names. Overrides const-naming-
# style. If left empty, constant names will be checked with the set naming
# style.
#const-rgx=

# Minimum line length for functions/classes that require docstrings, shorter
# ones are exempt.
docstring-min-length=-1

# Naming style matching correct function names.
function-naming-style=snake_case

# Regular expression matching correct function names. Overrides function-
# naming-style. If left empty, function names will be checked with the set
# naming style.
#function-rgx=

# Good variable names which should always be accepted, separated by a comma.
good-names=i,
           j,
           k,
           ex,
           Run,
           _

# Good variable names regexes, separated by a comma. If names match any regex,
# they will always be accepted
good-names-rgxs=

# Include a hint for the correct naming format with invalid-name.
include-naming-hint=no

# Naming style matching correct inline iteration names.
inlinevar-naming-style=any

# Regular expression matching correct inline iteration names. Overrides
# inlinevar-naming-style. If left empty, inline iteration names will be checked
# with the set naming style.
#inlinevar-rgx=

# Naming style matching correct method names.
method-naming-style=snake_case

# Regular expression matching correct method names. Overrides method-naming-
# style. If left empty, method names will be checked with the set naming style.
#method-rgx=

# Naming style matching correct module names.
module-naming-style=snake_case

# Regular expression matching correct module names. Overrides module-naming-
# style. If left empty, module names will be checked with the set naming style.
#module-rgx=

# Colon-delimited sets of names that determine each other's naming style when
# the name regexes allow several styles.
name-group=

# Regular expression which should only match function or class names that do
# not require a docstring.
no-docstring-rgx=^_

# List of decorators that produce properties, such as abc.abstractproperty. Add
# to this list to register other decorators that produce valid properties.
# These decorators are taken in consideration only for invalid-name.
property-classes=abc.abstractproperty

# Regular expression matching correct type alias names. If left empty, type
# alias names will be checked with the set naming style.
#typealias-rgx=

# Regular expression matching correct type variable names. If left empty, type
# variable names will be checked with the set naming style.
#typevar-rgx=

# Naming style matching correct variable names.
variable-naming-style=snake_case

# Regular expression matching correct variable names. Overrides variable-
# naming-style. If left empty, variable names will be checked with the set
# naming style.
#variable-rgx=


[CLASSES]

# Warn about protected attribute access inside special methods
check-protected-access-in-special-methods=no

# List of method names used to declare (i.e. assign) instance attributes.
defining-attr-methods=__init__,
                      __new__,
                      setUp,
                      asyncSetUp,
                      __post_init__

# List of member names, which should be excluded from the protected access
# warning.
exclude-protected=_asdict,_fields,_replace,_source,_make,os._exit

# List of valid names for the first argument in a class method.
valid-classmethod-first-arg=cls

# List of valid names for the first argument in a metaclass class method.
valid-metaclass-classmethod-first-arg=mcs


[DESIGN]

# List of regular expressions of class ancestor names to ignore when counting
# public methods (see R0903)
exclude-too-few-public-methods=

# List of qualified class names to ignore when counting class parents (see
# R0901)
ignored-parents=

# Maximum number of arguments for function / method.
max-args=8

# Maximum number of attributes for a class (see R0902).
max-attributes=7

# Maximum number of boolean expressions in an if statement (see R0916).
max-bool-expr=5

# Maximum number of branch for function / method body.
max-branches=12

# Maximum number of locals for function / method body.
max-locals=15

# Maximum number of parents for a class (see R0901).
max-parents=7

# Maximum number of public methods for a class (see R0904).
max-public-methods=20

# Maximum number of return / yield for function / method body.
max-returns=6

# Maximum number of statements in function / method body.
max-statements=50

# Minimum number of public methods for a class (see R0903).
min-public-methods=2


[EXCEPTIONS]

# Exceptions that will emit a warning when caught.
overgeneral-exceptions=builtins.BaseException,builtins.Exception


[FORMAT]

# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.
expected-line-ending-format=

# Regexp for a line that is allowed to be longer than the limit.
ignore-long-lines=^\s*(# )?<?https?://\S+>?$

# Number of spaces of indent required inside a hanging or continued line.
indent-after-paren=4

# String used as indentation unit. This is usually "    " (4 spaces) or "\t" (1
# tab).
indent-string='    '

# Maximum number of characters on a single line.
max-line-length=120

# Maximum number of lines in a module.
max-module-lines=1000

# Allow the body of a class to be on the same line as the declaration if body
# contains single statement.
single-line-class-stmt=no

# Allow the body of an if to be on the same line as the test if there is no
# else.
single-line-if-stmt=no


[IMPORTS]

# List of modules that can be imported at any level, not just the top level
# one.
allow-any-import-level=

# Allow explicit reexports by alias from a package __init__.
allow-reexport-from-package=no

# Allow wildcard imports from modules that define __all__.
allow-wildcard-with-all=no

# Deprecated modules which should not be used, separated by a comma.
deprecated-modules=

# Output a graph (.gv or any supported image format) of external dependencies
# to the given file (report RP0402 must not be disabled).
ext-import-graph=

# Output a graph (.gv or any supported image format) of all (i.e. internal and
# external) dependencies to the given file (report RP0402 must not be
# disabled).
import-graph=

# Output a graph (.gv or any supported image format) of internal dependencies
# to the given file (report RP0402 must not be disabled).
int-import-graph=

# Force import order to recognize a module as part of the standard
# compatibility libraries.
known-standard-library=

# Force import order to recognize a module as part of a third party library.
known-third-party=enchant

# Couples of modules and preferred modules, separated by a comma.
preferred-modules=


[LOGGING]

# The type of string formatting that logging methods do. `old` means using %
# formatting, `new` is for `{}` formatting.
logging-format-style=old

# Logging modules to check that the string format arguments are in logging
# function parameter format.
logging-modules=logging


[MESSAGES CONTROL]

# Only show warnings with the listed confidence levels. Leave empty to show
# all. Valid levels: HIGH, CONTROL_FLOW, INFERENCE, INFERENCE_FAILURE,
# UNDEFINED.
confidence=HIGH,
           CONTROL_FLOW,
           INFERENCE,
           INFERENCE_FAILURE,
           UNDEFINED

# Disable the message, report, category or checker with the given id(s). You
# can either give multiple identifiers separated by comma (,) or put this
# option multiple times (only on the command line, not in the configuration
# file where it should appear only once). You can also use "--disable=all" to
# disable everything first and then re-enable specific checks. For example, if
# you want to run only the similarities checker, you can use "--disable=all
# --enable=similarities". If you want to run only the classes checker, but have
# no Warning level messages displayed, use "--disable=all --enable=classes
# --disable=W".
disable=raw-checker-failed,
        bad-inline-option,
        locally-disabled,
        file-ignored,
        suppressed-message,
        useless-suppression,
        deprecated-pragma,
        use-symbolic-message-instead,
        use-implicit-booleaness-not-comparison-to-string,
        use-implicit-booleaness-not-comparison-to-zero,
        missing-module-docstring,  # C0114: Missing module docstring
        missing-class-docstring,   # C0115: Missing class docstring
        missing-function-docstring, # C0116: Missing function/method docstring        
        no-else-return,            # R1705: Unnecessary "else" after "return"
        logging-fstring-interpolation, # W1203: Use lazy % formatting in logging functions
        line-too-long,              # C0301: Line too long
        arguments-differ,          # W0221: Number of parameters/arguments differ in overridden methods
        duplicate-code,             # R0801: Duplicate code across multiple files
        invalid-name,
        too-many-nested-blocks,
        too-many-branches,
        too-many-positional-arguments,
        too-many-statements,
        too-few-public-methods,
        too-many-locals

# Enable the message, report, category or checker with the given id(s). You can
# either give multiple identifier separated by comma (,) or put this option
# multiple time (only on the command line, not in the configuration file where
# it should appear only once). See also the "--disable" option for examples.
enable=


[METHOD_ARGS]

# List of qualified names (i.e., library.method) which require a timeout
# parameter e.g. 'requests.api.get,requests.api.post'
timeout-methods=requests.api.delete,requests.api.get,requests.api.head,requests.api.options,requests.api.patch,requests.api.post,requests.api.put,requests.api.request


[MISCELLANEOUS]

# List of note tags to take in consideration, separated by a comma.
notes=FIXME,
      XXX,
      TODO

# Regular expression of note tags to take in consideration.
notes-rgx=


[REFACTORING]

# Maximum number of nested blocks for function / method body
max-nested-blocks=5

# Complete name of functions that never returns. When checking for
# inconsistent-return-statements if a never returning function is called then
# it will be considered as an explicit return statement and no message will be
# printed.
never-returning-functions=sys.exit,argparse.parse_error

# Let 'consider-using-join' be raised when the separator to join on would be
# non-empty (resulting in expected fixes of the type: ``"- " + " -
# ".join(items)``)
suggest-join-with-non-empty-separator=yes


[REPORTS]

# Python expression which should return a score less than or equal to 10. You
# have access to the variables 'fatal', 'error', 'warning', 'refactor',
# 'convention', and 'info' which contain the number of messages in each
# category, as well as 'statement' which is the total number of statements
# analyzed. This score is used by the global evaluation report (RP0004).
evaluation=max(0, 0 if fatal else 10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10))

# Template used to display messages. This is a python new-style format string
# used to format the message information. See doc for all details.
msg-template=

# Set the output format. Available formats are: text, parseable, colorized,
# json2 (improved json format), json (old json format) and msvs (visual
# studio). You can also give a reporter class, e.g.
# mypackage.mymodule.MyReporterClass.
#output-format=

# Tells whether to display a full report or only the messages.
reports=no

# Activate the evaluation score.
score=yes


[SIMILARITIES]

# Comments are removed from the similarity computation
ignore-comments=yes

# Docstrings are removed from the similarity computation
ignore-docstrings=yes

# Imports are removed from the similarity computation
ignore-imports=yes

# Signatures are removed from the similarity computation
ignore-signatures=yes

# Minimum lines number of a similarity.
min-similarity-lines=16


[SPELLING]

# Limits count of emitted suggestions for spelling mistakes.
max-spelling-suggestions=4

# Spelling dictionary name. No available dictionaries : You need to install
# both the python package and the system dependency for enchant to work.
spelling-dict=

# List of comma separated words that should be considered directives if they
# appear at the beginning of a comment and should not be checked.
spelling-ignore-comment-directives=fmt: on,fmt: off,noqa:,noqa,nosec,isort:skip,mypy:

# List of comma separated words that should not be checked.
spelling-ignore-words=

# A path to a file that contains the private dictionary; one word per line.
spelling-private-dict-file=

# Tells whether to store unknown words to the private dictionary (see the
# --spelling-private-dict-file option) instead of raising a message.
spelling-store-unknown-words=no


[STRING]

# This flag controls whether inconsistent-quotes generates a warning when the
# character used as a quote delimiter is used inconsistently within a module.
check-quote-consistency=no

# This flag controls whether the implicit-str-concat should generate a warning
# on implicit string concatenation in sequences defined over several lines.
check-str-concat-over-line-jumps=no


[TYPECHECK]

# List of decorators that produce context managers, such as
# contextlib.contextmanager. Add to this list to register other decorators that
# produce valid context managers.
contextmanager-decorators=contextlib.contextmanager

# List of members which are set dynamically and missed by pylint inference
# system, and so shouldn't trigger E1101 when accessed. Python regular
# expressions are accepted.
generated-members=

# Tells whether to warn about missing members when the owner of the attribute
# is inferred to be None.
ignore-none=yes

# This flag controls whether pylint should warn about no-member and similar
# checks whenever an opaque object is returned when inferring. The inference
# can return multiple potential results while evaluating a Python object, but
# some branches might not be evaluated, which results in partial inference. In
# that case, it might be useful to still emit no-member and other checks for
# the rest of the inferred objects.
ignore-on-opaque-inference=yes

# List of symbolic message names to ignore for Mixin members.
ignored-checks-for-mixins=no-member,
                          not-async-context-manager,
                          not-context-manager,
                          attribute-defined-outside-init

# List of class names for which member attributes should not be checked (useful
# for classes with dynamically set attributes). This supports the use of
# qualified names.
ignored-classes=optparse.Values,thread._local,_thread._local,argparse.Namespace

# Show a hint with possible names when a member name was not found. The aspect
# of finding the hint is based on edit distance.
missing-member-hint=yes

# The minimum edit distance a name should have in order to be considered a
# similar match for a missing member name.
missing-member-hint-distance=1

# The total number of similar names that should be taken in consideration when
# showing a hint for a missing member.
missing-member-max-choices=1

# Regex pattern to define which classes are considered mixins.
mixin-class-rgx=.*[Mm]ixin

# List of decorators that change the signature of a decorated function.
signature-mutators=


[VARIABLES]

# List of additional names supposed to be defined in builtins. Remember that
# you should avoid defining new builtins when possible.
additional-builtins=

# Tells whether unused global variables should be treated as a violation.
allow-global-unused-variables=yes

# List of names allowed to shadow builtins
allowed-redefined-builtins=

# List of strings which can identify a callback function by name. A callback
# name must start or end with one of those strings.
callbacks=cb_,
          _cb

# A regular expression matching the name of dummy variables (i.e. expected to
# not be used).
dummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_

# Argument names that match this expression will be ignored.
ignored-argument-names=_.*|^ignored_|^unused_

# Tells whether we should check for unused import in __init__ files.
init-import=no

# List of qualified module names which can have objects that can redefine
# builtins.
redefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io



================================================
FILE: graphlit_tools/__init__.py
================================================
from .base_tool import BaseTool
from .crewai_converter import CrewAIConverter
from .griptape_converter import GriptapeConverter
from .exceptions import ToolException
from .retrieval.content_retrieval_tool import ContentRetrievalTool
from .retrieval.person_retrieval_tool import PersonRetrievalTool
from .retrieval.organization_retrieval_tool import OrganizationRetrievalTool
from .extraction.extract_text_tool import ExtractTextTool
from .extraction.extract_url_tool import ExtractURLTool
from .extraction.extract_web_page_tool import ExtractWebPageTool
from .generation.prompt_tool import PromptTool, PromptToolInput
from .generation.describe_image_tool import DescribeImageTool
from .generation.describe_web_page_tool import DescribeWebPageTool
from .generation.generate_summary_tool import GenerateSummaryTool
from .generation.generate_bullets_tool import GenerateBulletsTool
from .generation.generate_headlines_tool import GenerateHeadlinesTool
from .generation.generate_social_media_posts_tool import GenerateSocialMediaPostsTool
from .generation.generate_questions_tool import GenerateQuestionsTool
from .generation.generate_keywords_tool import GenerateKeywordsTool
from .generation.generate_chapters_tool import GenerateChaptersTool
from .ingestion.url_ingest_tool import URLIngestTool
from .ingestion.local_ingest_tool import LocalIngestTool
from .ingestion.web_scrape_tool import WebScrapeTool
from .ingestion.web_crawl_tool import WebCrawlTool
from .ingestion.web_search_tool import WebSearchTool
from .ingestion.web_map_tool import WebMapTool
from .ingestion.reddit_ingest_tool import RedditIngestTool
from .ingestion.notion_ingest_tool import NotionIngestTool
from .ingestion.microsoft_email_ingest_tool import MicrosoftEmailIngestTool
from .ingestion.google_email_ingest_tool import GoogleEmailIngestTool
from .ingestion.github_issue_ingest_tool import GitHubIssueIngestTool
from .ingestion.jira_issue_ingest_tool import JiraIssueIngestTool
from .ingestion.linear_issue_ingest_tool import LinearIssueIngestTool
from .ingestion.microsoft_teams_ingest_tool import MicrosoftTeamsIngestTool
from .ingestion.discord_ingest_tool import DiscordIngestTool
from .ingestion.slack_ingest_tool import SlackIngestTool
from .ingestion.rss_ingest_tool import RSSIngestTool



================================================
FILE: graphlit_tools/base_tool.py
================================================
from abc import abstractmethod
from typing import Any, Type, Dict
from pydantic import BaseModel

class BaseTool(BaseModel):
    """
    Abstract base class for tools.
    
    Attributes:
        name (str): The name of the tool.
        description (str): A short description of the tool's functionality.
        args_schema (Type[BaseModel]): The schema for arguments expected by the tool.
    """
    name: str
    description: str
    args_schema: Type[BaseModel]

    def run(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> Any:
        """
        Public method to execute the tool. Delegates to the abstract _run method.

        Args:
            *args (Any): Positional arguments passed to the tool.
            **kwargs (Any): Keyword arguments passed to the tool.

        Returns:
            Any: The result of executing the tool.
        """
        return self._run(*args, **kwargs)

    @abstractmethod
    def _run(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> Any:
        """
        Abstract method to define the tool's behavior.

        Subclasses must implement this method to provide the actual logic.

        Args:
            *args (Any): Positional arguments passed to the tool.
            **kwargs (Any): Keyword arguments passed to the tool.

        Returns:
            Any: The result of executing the tool.
        """

    async def arun(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> Any:
        """
        Public async method to execute the tool. Delegates to the abstract _arun method.

        Args:
            *args (Any): Positional arguments passed to the tool.
            **kwargs (Any): Keyword arguments passed to the tool.

        Returns:
            Any: The result of executing the tool asynchronously.
        """
        return await self._arun(*args, **kwargs)

    @abstractmethod
    async def _arun(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> Any:
        """
        Abstract async method to define the tool's asynchronous behavior.

        Subclasses must implement this method to provide the actual async logic.

        Args:
            *args (Any): Positional arguments passed to the tool.
            **kwargs (Any): Keyword arguments passed to the tool.

        Returns:
            Any: The result of executing the tool asynchronously.
        """

    @property
    def json_schema(self) -> Dict[str, Any]:
        """Get the tool's JSON schema."""
        return self.args_schema.model_json_schema()

    def to_openai_tool(self) -> Dict[str, Any]:
        """
        Creates a tool definition compatible with OpenAI tool calling.

        Returns:
            dict: The tool definition for OpenAI.
        """
        return {
            "name": self.name,
            "description": self.description,
            "parameters": self.json_schema
        }



================================================
FILE: graphlit_tools/crewai_converter.py
================================================
from typing import Any, cast
from .base_tool import BaseTool

CrewAIBaseTool: Any = None

try:
    from crewai.tools import BaseTool as CrewAIBaseTool
except ImportError:
    CrewAIBaseTool = None

if CrewAIBaseTool:
    class CrewAIConverter(CrewAIBaseTool):
        """Tool to convert Graphlit tools into CrewAI tools."""

        graphlit_tool: Any

        def _run(
            self,
            *args: Any,
            **kwargs: Any,
        ) -> Any:
            tool = cast(BaseTool, self.graphlit_tool)

            return tool.run(*args, **kwargs)

        async def _arun(
            self,
            *args: Any,
            **kwargs: Any,
        ) -> Any:
            tool = cast(BaseTool, self.graphlit_tool)

            return await tool.arun(*args, **kwargs)

        @classmethod
        def from_tool(cls, tool: Any, **kwargs: Any) -> "CrewAIConverter":
            if not isinstance(tool, BaseTool):
                raise ValueError(f"Expected a Graphlit tool, got {type(tool)}")

            tool = cast(BaseTool, tool)

            if tool.args_schema is None:
                raise ValueError("Invalid arguments JSON schema.")

            return cls(
                name=tool.name,
                description=tool.description,
                args_schema=tool.args_schema,
                graphlit_tool=tool,
                **kwargs,
            )
else:
    class CrewAIConverter:
        """Fallback CrewAIConverter if crewai is not installed."""
        def __init__(self, *args, **kwargs):
            raise ImportError(
                "CrewAIConverter requires the crewai package. "
                "Install it using pip install graphlit-tools[crewai]."
            )



================================================
FILE: graphlit_tools/exceptions.py
================================================
class ToolException(Exception):
    """
    Custom exception class for tool-related errors.

    Args:
        error_message (str): A descriptive error message for the exception.
    """
    def __init__(self, error_message: str):
        super().__init__(error_message)
        self.error_message = error_message

    def __str__(self) -> str:
        """
        String representation of the exception.

        Returns:
            str: The error message.
        """
        return self.error_message



================================================
FILE: graphlit_tools/griptape_converter.py
================================================
from typing import Any, Dict, cast
from .base_tool import BaseTool

GriptapeBaseTool: Any = None

try:
    from schema import Schema
    from griptape.tools import BaseTool as GriptapeBaseTool
    from griptape.utils.decorators import activity
    from griptape.artifacts import TextArtifact
except ImportError:
    GriptapeBaseTool = None

if GriptapeBaseTool:
    class GriptapeConverter(GriptapeBaseTool):
        """Tool to convert Graphlit tools into Griptape tools."""

        graphlit_tool: BaseTool

        @classmethod
        def from_tool(cls, tool: Any, **kwargs: Any) -> "GriptapeConverter":
            if not isinstance(tool, BaseTool):
                raise ValueError(f"Expected a Graphlit tool, got {type(tool)}")

            graphlit_tool = cast(BaseTool, tool)

            if graphlit_tool.args_schema is None:
                raise ValueError("Invalid arguments JSON schema.")

            # Create an instance of GriptapeConverter
            instance = cls(name=graphlit_tool.name, **kwargs)
            instance.graphlit_tool = graphlit_tool

            # Define the generate method dynamically
            def generate(self, params: Dict[str, Any]) -> TextArtifact:
                return TextArtifact(str(self.graphlit_tool.run(**params)))

            # Convert the tool's schema
            tool_schema = Schema(graphlit_tool.json_schema)

            # Decorate the generate method
            decorated_generate = activity(
                config={
                    "description": graphlit_tool.description,
                    "schema": tool_schema,
                }
            )(generate)

            # Attach the dynamically created method to the instance
            setattr(instance, "generate", decorated_generate)

            return instance
else:
    class GriptapeConverter:
        """Fallback GriptapeConverter if griptape is not installed."""

        @classmethod
        def from_tool(cls, tool: Any, **kwargs: Any) -> "GriptapeConverter":
            raise ImportError(
                "GriptapeConverter requires the griptape package. "
                "Install it using pip install graphlit-tools[griptape]."
            )



================================================
FILE: graphlit_tools/helpers.py
================================================
import asyncio
from typing import Callable, Optional, List, Any, Coroutine
from graphlit_api import exceptions, input_types, enums
from .exceptions import ToolException

def format_person(person) -> List[str]:
    results = []

    results.append(f'**Person ID:** {person.id}')

    results.append(f'**Name:** {person.name}')

    if person.email is not None:
        results.append(f'**Email:** {person.email}')

    if person.uri is not None:
        results.append(f'**URI:** {person.uri}')

    if person.education is not None:
        results.append(f'**Education:** {person.education}')

    if person.occupation is not None:
        results.append(f'**Occupation:** {person.occupation}')

    results.append('\n')

    return results

def format_organization(organization) -> List[str]:
    results = []

    results.append(f'**Organization ID:** {organization.id}')

    results.append(f'**Name:** {organization.name}')

    if organization.email is not None:
        results.append(f'**Email:** {organization.email}')

    if organization.uri is not None:
        results.append(f'**URI:** {organization.uri}')

    results.append('\n')

    return results

def format_content(content, include_text: Optional[bool] = True) -> List[str]:
    results = []

    # Basic content details
    results.append(f"**Content ID:** {content.id}")

    if content.type == enums.ContentTypes.FILE:
        results.append(f"**File Type:** [{content.file_type}]")
        results.append(f"**File Name:** {content.file_name}")
    else:
        results.append(f"**Type:** [{content.type}]")
        if content.type not in [enums.ContentTypes.PAGE, enums.ContentTypes.EMAIL]:
            results.append(f"**Name:** {content.name}")

    # Optional metadata
    if content.uri:
        results.append(f"**URI:** {content.uri}")
    if content.creation_date:
        results.append(f"**Ingestion Date:** {content.creation_date}")
    if content.original_date:
        results.append(f"**Author Date:** {content.original_date}")

    # Issue details
    if content.issue:
        issue_attributes = [
            ("Title", content.issue.title),
            ("Identifier", content.issue.identifier),
            ("Type", content.issue.type),
            ("Project", content.issue.project),
            ("Team", content.issue.team),
            ("Status", content.issue.status),
            ("Priority", content.issue.priority),
        ]
        results.extend([f"**{label}:** {value}" for label, value in issue_attributes if value])

        if content.issue.labels:
            results.append(f"**Labels:** {', '.join(content.issue.labels)}")

    # Email details
    if content.email:
        email_attributes = [
            ("Subject", content.email.subject),
            ("Sensitivity", content.email.sensitivity.name if content.email.sensitivity else None),
            ("Priority", content.email.priority.name if content.email.priority else None),
            ("Importance", content.email.importance.name if content.email.importance else None),
            ("Labels", ', '.join(content.email.labels) if content.email.labels else None),
            ("To", ', '.join(f"{r.name} <{r.email}>" for r in content.email.to) if content.email.to else None),
            ("From", ', '.join(f"{r.name} <{r.email}>" for r in getattr(content.email, "from", []))),
            ("CC", ', '.join(f"{r.name} <{r.email}>" for r in content.email.cc) if content.email.cc else None),
            ("BCC", ', '.join(f"{r.name} <{r.email}>" for r in content.email.bcc) if content.email.bcc else None),
        ]
        results.extend([f"**{label}:** {value}" for label, value in email_attributes if value])

    # Document details
    if content.document:
        document_attributes = [
            ("Title", content.document.title),
            ("Author", content.document.author),
        ]
        results.extend([f"**{label}:** {value}" for label, value in document_attributes if value])

    # Audio details
    if content.audio:
        audio_attributes = [
            ("Title", content.audio.title),
            ("Host", content.audio.author),
            ("Episode", content.audio.episode),
            ("Series", content.audio.series),
        ]
        results.extend([f"**{label}:** {value}" for label, value in audio_attributes if value])

    # Image details
    if content.image:
        image_attributes = [
            ("Description", content.image.description),
            ("Software", content.image.software),
            ("Make", content.image.make),
            ("Model", content.image.model),
        ]
        results.extend([f"**{label}:** {value}" for label, value in image_attributes if value])

    # Links
    if content.links:
        if content.type in [enums.ContentTypes.PAGE]:
            results.extend([f"**{link.link_type} Link:** {link.uri}" for link in content.links[:100]])

    # Include text content if specified
    if include_text:
        if content.pages:
            for page in content.pages:
                if page.chunks:
                    results.append(f"**Page #{page.index + 1}:**")
                    results.extend([chunk.text for chunk in page.chunks])
                    results.append("\n---\n")

        if content.segments:
            for segment in content.segments:
                results.append(f"**Transcript Segment [{segment.start_time}-{segment.end_time}]:**")
                results.append(segment.text)
                results.append("\n---\n")

        if content.frames:
            for frame in content.frames:
                results.append(f"**Frame #{frame.index + 1}:**")
                results.append(frame.text)
                results.append("\n---\n")

        if not content.pages and not content.segments and not content.frames and content.markdown:
            results.append(content.markdown)
            results.append("\n")
    else:
        results.append("\n")

    return results

def run_async(coro_func: Callable[..., Coroutine[Any, Any, Any]], *args, **kwargs) -> Any:
    """
    Runs an async function synchronously, handling event loops properly.

    Args:
        coro_func: The asynchronous function to be run.
        *args: Positional arguments to pass to the async function.
        **kwargs: Keyword arguments to pass to the async function.

    Returns:
        The result of the async function execution.
    """
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If the loop is already running, ensure the coroutine runs within it
            return loop.run_until_complete(coro_func(*args, **kwargs))
        else:
            # If no loop is running, use asyncio.run
            return asyncio.run(coro_func(*args, **kwargs))
    except RuntimeError:
        # Handle case where the event loop is closed
        new_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(new_loop)
        return new_loop.run_until_complete(coro_func(*args, **kwargs))

async def is_feed_done(client, feed_id: str):
    response = await client.is_feed_done(feed_id)

    return response.is_feed_done.result if response.is_feed_done is not None else None

async def query_contents(client, feed_id: str, search: Optional[str] = None):
    try:
        response = await client.query_contents(
            filter=input_types.ContentFilter(
                search=search,
                searchType=enums.SearchTypes.HYBRID,
                feeds=[
                    input_types.EntityReferenceFilter(
                        id=feed_id
                    )
                ]
            )
        )

        return response.contents.results if response.contents is not None else None
    except exceptions.GraphQLClientError as e:
        print(str(e))
        return None

async def format_feed_contents(client, feed_id: str, search: Optional[str] = None):
    try:
        contents = await query_contents(client, feed_id, search)

        results = []

        for content in contents:
            results.extend(format_content(content))

        text = "\n".join(results)

        return text
    except exceptions.GraphQLClientError as e:
        print(str(e))
        raise ToolException(str(e)) from e



================================================
FILE: graphlit_tools/extraction/__init__.py
================================================



================================================
FILE: graphlit_tools/extraction/extract_text_tool.py
================================================
import logging
import json
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class ExtractTextInput(BaseModel):
    text: str = Field(description="Text to be extracted with LLM")
    model_schema: str = Field(description="Pydantic model JSON schema which describes the data which will be extracted. JSON schema needs be of type 'object' and include 'properties' and 'required' fields.")
    prompt: Optional[str] = Field(description="Text prompt which is provided to LLM to guide data extraction, optional.", default=None)

class ExtractTextTool(BaseTool):
    name: str = "Graphlit JSON text data extraction tool"
    description: str = """Extracts JSON data from text using LLM.
    Accepts text to be scraped, and JSON schema of Pydantic model to be extracted into. JSON schema needs be of type 'object' and include 'properties' and 'required' fields.
    Returns extracted JSON from text."""
    args_schema: Type[BaseModel] = ExtractTextInput

    graphlit: Graphlit = Field(None, exclude=True)

    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the ExtractTextTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            specification_id (Optional[str]): ID for the LLM specification to use. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, text: str, model_schema: str, prompt: Optional[str] = None) -> Optional[str]:
        default_name = "extract_pydantic_model"

        default_prompt = """
        Extract data using the tools provided.
        """

        try:
            response = await self.graphlit.client.extract_text(
                specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                tools=[input_types.ToolDefinitionInput(name=default_name, schema=model_schema)],
                prompt=default_prompt if prompt is None else prompt,
                text=text,
                correlation_id=self.correlation_id
            )

            if response.extract_text is None:
                raise ToolException('Failed to extract text.')

            extractions = response.extract_text

            json_array = json.loads('[' + ','.join(extraction.value for extraction in extractions) + ']')

            return json.dumps(json_array, indent=4)
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, text: str, model_schema: str, prompt: Optional[str] = None) -> Optional[str]:
        return helpers.run_async(self._arun, text, model_schema, prompt)



================================================
FILE: graphlit_tools/extraction/extract_url_tool.py
================================================
import logging
import json
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class ExtractURLInput(BaseModel):
    url: str = Field(description="URL of cloud-hosted file to be ingested into knowledge base")
    model_schema: str = Field(description="Pydantic model JSON schema which describes the data which will be extracted. JSON schema needs be of type 'object' and include 'properties' and 'required' fields.")
    prompt: Optional[str] = Field(description="Text prompt which is provided to LLM to guide data extraction, optional.", default=None)

class ExtractURLTool(BaseTool):
    name: str = "Graphlit JSON URL data extraction tool"
    description: str = """Extracts JSON data from ingested file using LLM.
    Accepts URL to be ingested, and JSON schema of Pydantic model to be extracted into. JSON schema needs be of type 'object' and include 'properties' and 'required' fields.
    Returns extracted JSON from file."""
    args_schema: Type[BaseModel] = ExtractURLInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, specification_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the ExtractURLTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting files. Defaults to None.
            specification_id (Optional[str]): ID for the LLM specification to use. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, url: str, model_schema: str, prompt: Optional[str] = None) -> Optional[str]:
        content_id = None

        try:
            response = await self.graphlit.client.ingest_uri(
                uri=url,
                workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                is_synchronous=True,
                correlation_id=self.correlation_id
            )

            content_id = response.ingest_uri.id if response.ingest_uri is not None else None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if content_id is None:
            raise ToolException('Invalid content identifier.')

        text = None

        try:
            response = await self.graphlit.client.get_content(
                id=content_id
            )

            if response.content is None:
                raise ToolException(f'Failed to get content [{content_id}].')

            logger.debug(f'ExtractURLTool: Retrieved content by ID [{content_id}].')

            results = helpers.format_content(response.content)

            text = "\n".join(results)
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if text is None:
            raise ToolException(f'Found no text to be extracted from content [{content_id}].')

        default_name = "extract_pydantic_model"

        default_prompt = """
        Extract data using the tools provided.
        """

        try:
            response = await self.graphlit.client.extract_text(
                specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                tools=[input_types.ToolDefinitionInput(name=default_name, schema=model_schema)],
                prompt=default_prompt if prompt is None else prompt,
                text=text,
                text_type=enums.TextTypes.MARKDOWN,
                correlation_id=self.correlation_id
            )

            if response.extract_text is None:
                raise ToolException('Failed to extract text.')

            extractions = response.extract_text

            json_array = json.loads('[' + ','.join(extraction.value for extraction in extractions) + ']')

            return json.dumps(json_array, indent=4)
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, url: str, model_schema: str, prompt: Optional[str] = None) -> Optional[str]:
        return helpers.run_async(self._arun, url, model_schema, prompt)



================================================
FILE: graphlit_tools/extraction/extract_web_page_tool.py
================================================
import logging
import json
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class ExtractWebPageInput(BaseModel):
    url: str = Field(description="URL of web page to be scraped and ingested into knowledge base")
    model_schema: str = Field(description="Pydantic model JSON schema which describes the data which will be extracted. JSON schema needs be of type 'object' and include 'properties' and 'required' fields.")
    prompt: Optional[str] = Field(description="Text prompt which is provided to LLM to guide data extraction, optional.", default=None)

class ExtractWebPageTool(BaseTool):
    name: str = "Graphlit JSON web page data extraction tool"
    description: str = """Extracts JSON data from ingested web page using LLM.
    Accepts URL to be scraped, and JSON schema of Pydantic model to be extracted into. JSON schema needs be of type 'object' and include 'properties' and 'required' fields.
    Returns extracted JSON from web page."""
    args_schema: Type[BaseModel] = ExtractWebPageInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, specification_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the ExtractWebPageTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting web pages. Defaults to None.
            specification_id (Optional[str]): ID for the LLM specification to use. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, url: str, model_schema: str, prompt: Optional[str] = None) -> Optional[str]:
        content_id = None

        try:
            response = await self.graphlit.client.ingest_uri(
                uri=url,
                workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                is_synchronous=True,
                correlation_id=self.correlation_id
            )

            content_id = response.ingest_uri.id if response.ingest_uri is not None else None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if content_id is None:
            raise ToolException('Invalid content identifier.')

        text = None

        try:
            response = await self.graphlit.client.get_content(
                id=content_id
            )

            if response.content is None:
                raise ToolException(f'Failed to get content [{content_id}].')

            logger.debug(f'ExtractWebPageTool: Retrieved content by ID [{content_id}].')

            results = helpers.format_content(response.content)

            text = "\n".join(results)
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if text is None:
            raise ToolException(f'Found no text to be extracted from content [{content_id}].')

        default_name = "extract_pydantic_model"

        default_prompt = """
        Extract data using the tools provided.
        """

        try:
            response = await self.graphlit.client.extract_text(
                specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                tools=[input_types.ToolDefinitionInput(name=default_name, schema=model_schema)],
                prompt=default_prompt if prompt is None else prompt,
                text=text,
                text_type=enums.TextTypes.MARKDOWN,
                correlation_id=self.correlation_id
            )

            if response.extract_text is None:
                raise ToolException('Failed to extract text.')

            extractions = response.extract_text

            json_array = json.loads('[' + ','.join(extraction.value for extraction in extractions) + ']')

            return json.dumps(json_array, indent=4)
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, url: str, model_schema: str, prompt: Optional[str] = None) -> Optional[str]:
        return helpers.run_async(self._arun, url, model_schema, prompt)



================================================
FILE: graphlit_tools/generation/__init__.py
================================================



================================================
FILE: graphlit_tools/generation/describe_image_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class DescribeImageInput(BaseModel):
    url: str = Field(description="URL for image to be described with vision LLM")
    prompt: str = Field(description="Text prompt which is provided to vision LLM for completion")

class DescribeImageTool(BaseTool):
    name: str = "Graphlit image description tool"
    description: str = """Accepts image URL as string.
    Prompts vision LLM and returns completion. Returns Markdown text from LLM completion."""
    args_schema: Type[BaseModel] = DescribeImageInput

    graphlit: Graphlit = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None,
                 correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the DescribeImageTool.

        Args:
            graphlit (Optional[Graphlit]): Instance for interacting with the Graphlit API.
                Defaults to a new Graphlit instance if not provided.
            specification_id (Optional[str]): ID for the LLM specification. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, prompt: str, url: str) -> str:
        try:
            response = await self.graphlit.client.describe_image(
                specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                prompt=prompt,
                uri=url,
                correlation_id=self.correlation_id
            )

            if response.describe_image is None or response.describe_image.message is None:
                raise ToolException('Failed to describe image.')

            message = response.describe_image.message

            return message.message
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, prompt: str, url: str) -> str:
        return helpers.run_async(self._arun, prompt, url)



================================================
FILE: graphlit_tools/generation/describe_web_page_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class DescribeWebPageInput(BaseModel):
    url: str = Field(description="URL of web page to screenshot and ingest into knowledge base")
    prompt: Optional[str] = Field(description="Text prompt which is provided to vision LLM for screenshot description, optional", default=None)

class DescribeWebPageTool(BaseTool):
    name: str = "Graphlit screenshot web page tool"
    description: str = """Screenshots web page from URL and describes web page with vision LLM.
    Returns Markdown description of screenshot and extracted Markdown text from image."""
    args_schema: Type[BaseModel] = DescribeWebPageInput

    graphlit: Graphlit = Field(None, exclude=True)

    specification_id: Optional[str] = Field(None, exclude=True)
    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the DescribeWebPageTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting files. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, url: str, prompt: Optional[str] = None) -> Optional[str]:
        content_id = None

        try:
            response = await self.graphlit.client.screenshot_page(
                uri=url,
                workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                is_synchronous=True,
                correlation_id=self.correlation_id
            )

            content_id = response.screenshot_page.id if response.screenshot_page is not None else None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if content_id is None:
            raise ToolException('Invalid content identifier.')

        content = None

        try:
            response = await self.graphlit.client.get_content(
                id=content_id
            )

            if response.content is None:
                raise ToolException(f'Failed to get content [{content_id}].')

            logger.debug(f'DescribeWebPageTool: Retrieved content by ID [{content_id}].')

            content = response.content
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if content.image_uri is None:
            raise ToolException(f'Invalid image URI for content [{content_id}].')

        # NOTE: if we've already analyzed the image, via workflow, return the image description
        if content.image.description is not None:
            return content.image.description

        default_prompt = """
        Conduct a thorough analysis of the screenshot, with a particular emphasis on the textual content and any included imagery. 
        Provide a detailed examination of the text, highlighting key points and dissecting technical terms, named entities, and data presentations that contribute to the understanding of the subject matter. 
        Discuss how the technical language and the named entities relate to the overarching topic and objectives of the webpage. 
        Also, describe how the visual elements, such as color schemes, imagery, and branding elements like logos and taglines, support the textual message and enhance the viewer's comprehension of the content. 
        Assess the readability and organization of the content, and evaluate how these aspects facilitate the visitor's navigation and learning experience. Refrain from delving into the specifics of the user interface design but focus on the communication effectiveness and coherence of visual and textual elements. 
        Finally, offer a comprehensive view of the website's ability to convey its message and fulfill its intended commercial, educational, or promotional role, considering the target audience's perspective and potential engagement with the content.

        Carefully examine the image for any text it contains and extract as Markdown text. 
        In cases where the image contains no extractable text or only text that is not useful for understanding, don't extract any text. 
        Focus on including text that contributes significantly to understanding the image, such as titles, headings, key phrases, important data points, or labels. 
        Exclude any text that is not relevant or does not add value to the comprehension of the image. 
        Ensure to transcribe the text completely, without truncating with ellipses.
        """

        # otherwise, describe the screenshot and return image description
        try:
            response = await self.graphlit.client.describe_image(
                specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                prompt=default_prompt if prompt is None else prompt,
                uri=content.image_uri,
                correlation_id=self.correlation_id
            )

            if response.describe_image is None or response.describe_image.message is None:
                raise ToolException('Failed to describe screenshot.')

            return response.describe_image.message
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, url: str, prompt: Optional[str] = None) -> Optional[str]:
        return helpers.run_async(self._arun, url, prompt)



================================================
FILE: graphlit_tools/generation/generate_bullets_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class GenerateBulletsInput(BaseModel):
    text: str = Field(description="Text to be summarized into bullet points")
    count: Optional[int] = Field(description="Number of bullet points to be generated, optional.", default=10)

class GenerateBulletsTool(BaseTool):
    name: str = "Graphlit bullet points generation tool"
    description: str = """Accepts text as string.
    Optionally accepts the count of bullet points to be generated.
    Returns bullet points as text."""
    args_schema: Type[BaseModel] = GenerateBulletsInput

    graphlit: Graphlit = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None,
                 correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the GenerateBulletsTool.

        Args:
            graphlit (Optional[Graphlit]): Instance for interacting with the Graphlit API.
                Defaults to a new Graphlit instance if not provided.
            specification_id (Optional[str]): ID for the LLM specification. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, text: str, count: Optional[int] = None) -> str:
        try:
            response = await self.graphlit.client.summarize_text(
                text=text,
                summarization=input_types.SummarizationStrategyInput(
                    type=enums.SummarizationTypes.BULLETS,
                    items=count if count is not None else 10,
                    specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            if response.summarize_text is None or response.summarize_text.items is None:
                raise ToolException('Failed to generate bullet points.')

            items = response.summarize_text.items

            item = items[0] if response.summarize_text.items is not None and len(response.summarize_text.items) > 0 else None

            return item.text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, text: str, count: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, text, count)



================================================
FILE: graphlit_tools/generation/generate_chapters_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class GenerateChaptersInput(BaseModel):
    text: str = Field(description="Transcript to be summarized into chapters. Assumes transcript contains time-stamped text.")

class GenerateChaptersTool(BaseTool):
    name: str = "Graphlit transcript chapters generation tool"
    description: str = """Accepts transcript as string.
    Returns chapters as text."""
    args_schema: Type[BaseModel] = GenerateChaptersInput

    graphlit: Graphlit = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None,
                 correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the GenerateChaptersTool.

        Args:
            graphlit (Optional[Graphlit]): Instance for interacting with the Graphlit API.
                Defaults to a new Graphlit instance if not provided.
            specification_id (Optional[str]): ID for the LLM specification. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, text: str) -> str:
        try:
            response = await self.graphlit.client.summarize_text(
                text=text,
                summarization=input_types.SummarizationStrategyInput(
                    type=enums.SummarizationTypes.CHAPTERS,
                    specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            if response.summarize_text is None or response.summarize_text.items is None:
                raise ToolException('Failed to generate chapters.')

            items = response.summarize_text.items

            item = items[0] if response.summarize_text.items is not None and len(response.summarize_text.items) > 0 else None

            return item.text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, text: str) -> str:
        return helpers.run_async(self._arun, text)



================================================
FILE: graphlit_tools/generation/generate_headlines_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class GenerateHeadlinesInput(BaseModel):
    text: str = Field(description="Text to be summarized into headlines")
    count: Optional[int] = Field(description="Number of headlines to be generated, optional", default=10)

class GenerateHeadlinesTool(BaseTool):
    name: str = "Graphlit headlines generation tool"
    description: str = """Accepts text as string.
    Optionally accepts the count of headlines to be generated.
    Returns headlines as text."""
    args_schema: Type[BaseModel] = GenerateHeadlinesInput

    graphlit: Graphlit = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None,
                 correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the GenerateHeadlinesTool.

        Args:
            graphlit (Optional[Graphlit]): Instance for interacting with the Graphlit API.
                Defaults to a new Graphlit instance if not provided.
            specification_id (Optional[str]): ID for the LLM specification. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, text: str, count: Optional[int] = None) -> str:
        try:
            response = await self.graphlit.client.summarize_text(
                text=text,
                summarization=input_types.SummarizationStrategyInput(
                    type=enums.SummarizationTypes.HEADLINES,
                    items=count if count is not None else 10,
                    specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            if response.summarize_text is None or response.summarize_text.items is None:
                raise ToolException('Failed to generate headlines.')

            items = response.summarize_text.items

            item = items[0] if response.summarize_text.items is not None and len(response.summarize_text.items) > 0 else None

            return item.text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, text: str, count: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, text, count)



================================================
FILE: graphlit_tools/generation/generate_keywords_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class GenerateKeywordsInput(BaseModel):
    text: str = Field(description="Text to be summarized into keywords")
    count: Optional[int] = Field(description="Number of keywords to be generated, optional", default=10)

class GenerateKeywordsTool(BaseTool):
    name: str = "Graphlit keywords generation tool"
    description: str = """Accepts text as string.
    Optionally accepts the count of keywords to be generated.
    Returns keywords as text."""
    args_schema: Type[BaseModel] = GenerateKeywordsInput

    graphlit: Graphlit = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None,
                 correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the GenerateKeywordsTool.

        Args:
            graphlit (Optional[Graphlit]): Instance for interacting with the Graphlit API.
                Defaults to a new Graphlit instance if not provided.
            specification_id (Optional[str]): ID for the LLM specification. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, text: str, count: Optional[int] = None) -> str:
        try:
            response = await self.graphlit.client.summarize_text(
                text=text,
                summarization=input_types.SummarizationStrategyInput(
                    type=enums.SummarizationTypes.HEADLINES,
                    items=count if count is not None else 10,
                    specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            if response.summarize_text is None or response.summarize_text.items is None:
                raise ToolException('Failed to generate keywords.')

            items = response.summarize_text.items

            item = items[0] if response.summarize_text.items is not None and len(response.summarize_text.items) > 0 else None

            return item.text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, text: str, count: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, text, count)



================================================
FILE: graphlit_tools/generation/generate_questions_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class GenerateQuestionsInput(BaseModel):
    text: str = Field(description="Text to be summarized into followup questions")
    count: Optional[int] = Field(description="Number of followup questions to be generated, optional", default=10)

class GenerateQuestionsTool(BaseTool):
    name: str = "Graphlit followup questions generation tool"
    description: str = """Accepts text as string.
    Optionally accepts the count of followup questions to be generated.
    Returns followup questions as text."""
    args_schema: Type[BaseModel] = GenerateQuestionsInput

    graphlit: Graphlit = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None,
                 correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the GenerateQuestionsTool.

        Args:
            graphlit (Optional[Graphlit]): Instance for interacting with the Graphlit API.
                Defaults to a new Graphlit instance if not provided.
            specification_id (Optional[str]): ID for the LLM specification. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, text: str, count: Optional[int] = None) -> str:
        try:
            response = await self.graphlit.client.summarize_text(
                text=text,
                summarization=input_types.SummarizationStrategyInput(
                    type=enums.SummarizationTypes.QUESTIONS,
                    items=count if count is not None else 10,
                    specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            if response.summarize_text is None or response.summarize_text.items is None:
                raise ToolException('Failed to generate followup questions.')

            items = response.summarize_text.items

            item = items[0] if response.summarize_text.items is not None and len(response.summarize_text.items) > 0 else None

            return item.text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, text: str, count: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, text, count)



================================================
FILE: graphlit_tools/generation/generate_social_media_posts_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class GenerateSocialMediaPostsInput(BaseModel):
    text: str = Field(description="Text to be summarized into social media posts")
    count: Optional[int] = Field(description="Number of social media posts to be generated, optional", default=10)

class GenerateSocialMediaPostsTool(BaseTool):
    name: str = "Graphlit social media posts generation tool"
    description: str = """Accepts text as string.
    Optionally accepts the count of social media posts to be generated.
    Returns social media posts as text."""
    args_schema: Type[BaseModel] = GenerateSocialMediaPostsInput

    graphlit: Graphlit = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None,
                 correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the GenerateSocialMediaPostsTool.

        Args:
            graphlit (Optional[Graphlit]): Instance for interacting with the Graphlit API.
                Defaults to a new Graphlit instance if not provided.
            specification_id (Optional[str]): ID for the LLM specification. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, text: str, count: Optional[int] = None) -> str:
        try:
            response = await self.graphlit.client.summarize_text(
                text=text,
                summarization=input_types.SummarizationStrategyInput(
                    type=enums.SummarizationTypes.POSTS,
                    items=count if count is not None else 10,
                    specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            if response.summarize_text is None or response.summarize_text.items is None:
                raise ToolException('Failed to generate social media posts.')

            items = response.summarize_text.items

            item = items[0] if response.summarize_text.items is not None and len(response.summarize_text.items) > 0 else None

            return item.text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, text: str, count: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, text, count)



================================================
FILE: graphlit_tools/generation/generate_summary_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class GenerateSummaryInput(BaseModel):
    text: str = Field(description="Text to be summarized")
    prompt: Optional[str] = Field(description="Text prompt which is provided to LLM for text summarization, optional.", default=None)

class GenerateSummaryTool(BaseTool):
    name: str = "Graphlit summary generation tool"
    description: str = """Accepts text as string.
    Optionally accepts text prompt to be provided to LLM for text summarization.
    Returns summary as text."""
    args_schema: Type[BaseModel] = GenerateSummaryInput

    graphlit: Graphlit = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    def __init__(self, graphlit: Optional[Graphlit] = None, specification_id: Optional[str] = None,
                 correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the GenerateSummaryTool.

        Args:
            graphlit (Optional[Graphlit]): Instance for interacting with the Graphlit API.
                Defaults to a new Graphlit instance if not provided.
            specification_id (Optional[str]): ID for the LLM specification. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.specification_id = specification_id
        self.correlation_id = correlation_id

    async def _arun(self, text: str, prompt: Optional[str] = None) -> str:
        try:
            response = await self.graphlit.client.summarize_text(
                text=text,
                summarization=input_types.SummarizationStrategyInput(
                    type=enums.SummarizationTypes.SUMMARY if prompt is None else enums.SummarizationTypes.CUSTOM,
                    prompt=prompt,
                    specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            if response.summarize_text is None or response.summarize_text.items is None:
                raise ToolException('Failed to generate summary.')

            items = response.summarize_text.items

            item = items[0] if response.summarize_text.items is not None and len(response.summarize_text.items) > 0 else None

            return item.text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, text: str, prompt: Optional[str] = None) -> str:
        return helpers.run_async(self._arun, text, prompt)



================================================
FILE: graphlit_tools/generation/prompt_tool.py
================================================
import asyncio
import logging
import json
from typing import Optional, Type, List, Callable

from graphlit import Graphlit
from graphlit_api import exceptions, input_types
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

"""
Example of tool callback function:

def callback(tool_name, **kwargs):
    if tool_name == 'add_numbers':
        return add_numbers(**kwargs)
    elif tool_name == 'concat_strings':
        return concat_strings(**kwargs)
    else:
        return None
"""

class PromptInput(BaseModel):
    prompt: str = Field(description="Text prompt which is provided to LLM for completion, via RAG pipeline")

class PromptToolInput(BaseModel):
    name: str = Field(description="Tool name.")
    description: Optional[str] = Field(description="Tool description.", default=None)
    parameters: dict = Field(description="JSON schema for tool parameters.")
    callback: Callable[..., Optional[str]] = Field(description="Function which gets called back upon tool call.")

class PromptTool(BaseTool):
    name: str = "Graphlit RAG prompt tool"
    description: str = """Accepts user prompt as string.
    Prompts LLM with relevant content and returns completion from RAG pipeline. Returns Markdown text from LLM completion.
    Uses vector embeddings and similarity search to retrieve relevant content from knowledge base.
    Can search through web pages, PDFs, audio transcripts, and other unstructured data."""
    args_schema: Type[BaseModel] = PromptInput

    graphlit: Graphlit = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    conversation_id: Optional[str] = Field(None, exclude=True)
    specification_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    tools: Optional[List[PromptToolInput]] = Field(None, exclude=True)

    def __init__(self, graphlit: Optional[Graphlit] = None, conversation_id: Optional[str] = None, specification_id: Optional[str] = None,
                 tools: Optional[List[PromptToolInput]] = None,
                 correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the PromptTool.

        Args:
            graphlit (Optional[Graphlit]): Instance for interacting with the Graphlit API.
                Defaults to a new Graphlit instance if not provided.
            conversation_id (Optional[str]): ID for the ongoing conversation. Defaults to None.
            specification_id (Optional[str]): ID for the LLM specification. Will update an existing conversation. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            tools (Optional[List[ToolInput]]): List of tools provided to LLM. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.conversation_id = conversation_id
        self.specification_id = specification_id
        self.tools = tools
        self.correlation_id = correlation_id

    async def _arun(self, prompt: str) -> str:
        try:
            response = await self.graphlit.client.prompt_conversation(
                id=self.conversation_id,
                specification=input_types.EntityReferenceInput(id=self.specification_id) if self.specification_id is not None else None,
                prompt=prompt,
                tools=[input_types.ToolDefinitionInput(name=tool.name, description=tool.description, schema=json.dumps(tool.parameters)) for tool in self.tools if tool.name is not None and tool.parameters is not None] if self.tools is not None else None,
                correlation_id=self.correlation_id
            )

            if response.prompt_conversation is None or response.prompt_conversation.conversation is None or response.prompt_conversation.message is None:
                raise ToolException('Failed to prompt conversation.')

            message = response.prompt_conversation.message

            if self.tools is not None and message.tool_calls is not None:
                responses = []

                for tool_call in message.tool_calls:
                    tool = next((x for x in self.tools if x.name == tool_call.name), None)

                    if tool is not None:
                        arguments = json.loads(tool_call.arguments)

                        if asyncio.iscoroutinefunction(tool.callback):
                            content = await tool.callback(**arguments)
                        else:
                            content = tool.callback(**arguments)

                        if content is not None:
                            responses.append(input_types.ConversationToolResponseInput(id=tool_call.id, content=content))

                if len(responses) > 0:
                    response = await self.graphlit.client.continue_conversation(
                        id=response.prompt_conversation.conversation.id,
                        responses=responses,
                        correlation_id=self.correlation_id
                    )

                    if response.continue_conversation is None or response.continue_conversation.message is None:
                        return None

                    return response.continue_conversation.message.message if response.continue_conversation.message is not None else None
                else:
                    return message.message
            else:
                return message.message
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e
        except Exception as e:
            logger.error(str(e))
            print(str(e))
            raise ToolException(str(e)) from e

    def _run(self, prompt: str) -> str:
        return helpers.run_async(self._arun, prompt)



================================================
FILE: graphlit_tools/ingestion/__init__.py
================================================



================================================
FILE: graphlit_tools/ingestion/discord_ingest_tool.py
================================================
import logging
import time
import os
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class DiscordIngestInput(BaseModel):
    channel_name: str = Field(description="Discord channel name")
    search: Optional[str] = Field(description="Text to search for within ingested messages", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of messages from Discord channel to be read", default=10)

class DiscordIngestTool(BaseTool):
    name: str = "Graphlit Discord ingest tool"
    description: str = """Ingests messages from Discord channel into knowledge base.
    Accepts Discord channel name.
    Optionally accepts search text for searching within the ingested messages. If search text was not provided, all ingested messages will be returned.
    Returns extracted Markdown text and metadata from messages."""
    args_schema: Type[BaseModel] = DiscordIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the DiscordIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting messages. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, channel_name: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        token = os.environ['DISCORD_BOT_TOKEN']

        if token is None:
            raise ToolException('Invalid Discord bot token. Need to assign DISCORD_BOT_TOKEN environment variable.')

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name='Discord',
                    type=enums.FeedTypes.DISCORD,
                    discord=input_types.DiscordFeedPropertiesInput(
                        type=enums.FeedListingTypes.PAST,
                        channel=channel_name,
                        token=token,
                        includeAttachments=True,
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, channel_name: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, channel_name, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/github_issue_ingest_tool.py
================================================
import logging
import time
import os
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class GitHubIssueIngestInput(BaseModel):
    repository_name: str = Field(description="GitHub repository name")
    repository_owner: str = Field(description="GitHub repository owner")
    search: Optional[str] = Field(description="Text to search for within ingested issues", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of issues from GitHub repository to be read", default=10)

class GitHubIssueIngestTool(BaseTool):
    name: str = "Graphlit GitHub Issue ingest tool"
    description: str = """Ingests issues from GitHub repository into knowledge base.
    Accepts GitHub repository owner and repository name.
    For example, for GitHub repository (https://github.com/openai/tiktoken), 'openai' is the repository owner, and 'tiktoken' is the repository name.
    Optionally accepts search text for searching within the ingested issues. If search text was not provided, all ingested issues will be returned.
    Returns extracted Markdown text and metadata from issues."""
    args_schema: Type[BaseModel] = GitHubIssueIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the GitHubIssueIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting issues. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, repository_name: str, repository_owner: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        personal_access_token = os.environ['GITHUB_PERSONAL_ACCESS_TOKEN']

        if personal_access_token is None:
            raise ToolException('Invalid GitHub personal access token. Need to assign GITHUB_PERSONAL_ACCESS_TOKEN environment variable.')

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name='GitHub Issue',
                    type=enums.FeedTypes.ISSUE,
                    issue=input_types.IssueFeedPropertiesInput(
                        type=enums.FeedServiceTypes.GIT_HUB_ISSUES,
                        github=input_types.GitHubIssuesFeedPropertiesInput(
                            repositoryName=repository_name,
                            repositoryOwner=repository_owner,
                            personalAccessToken=personal_access_token,
                        ),
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, repository_name: str, repository_owner: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, repository_name, repository_owner, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/google_email_ingest_tool.py
================================================
import logging
import time
import os
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class GoogleEmailIngestInput(BaseModel):
    search: Optional[str] = Field(description="Text to search for within ingested emails.", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of emails from Google Email account to be read.", default=10)

class GoogleEmailIngestTool(BaseTool):
    name: str = "Graphlit Google Email ingest tool"
    description: str = """Ingests emails from Google Email account into knowledge base.
    Optionally accepts search text for searching within the ingested emails. If search text was not provided, all ingested emails will be returned.
    Returns extracted Markdown text and metadata from emails."""
    args_schema: Type[BaseModel] = GoogleEmailIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the GmailIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting emails. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        refresh_token = os.environ['GOOGLE_EMAIL_REFRESH_TOKEN']

        if refresh_token is None:
            raise ToolException('Invalid Google Email refresh token. Need to assign GOOGLE_EMAIL_REFRESH_TOKEN environment variable.')

        client_id = os.environ['GOOGLE_EMAIL_CLIENT_ID']

        if client_id is None:
            raise ToolException('Invalid Google Email client identifier. Need to assign GOOGLE_EMAIL_CLIENT_ID environment variable.')

        client_secret = os.environ['GOOGLE_EMAIL_CLIENT_SECRET']

        if client_secret is None:
            raise ToolException('Invalid Google Email client secret. Need to assign GOOGLE_EMAIL_CLIENT_SECRET environment variable.')

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name='Google Email',
                    type=enums.FeedTypes.EMAIL,
                    email=input_types.EmailFeedPropertiesInput(
                        type=enums.FeedServiceTypes.GOOGLE_EMAIL,
                        google=input_types.GoogleEmailFeedPropertiesInput(
                            type=enums.EmailListingTypes.PAST,
                            refreshToken=refresh_token,
                            clientId=client_id,
                            clientSecret=client_secret,
                        ),
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, search: Optional[str] = None, read_limit: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/jira_issue_ingest_tool.py
================================================
import logging
import time
import os
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class JiraIssueIngestInput(BaseModel):
    url: str = Field(description="Atlassian Jira server URL")
    project: str = Field(description="Atlassian Jira project name")
    search: Optional[str] = Field(description="Text to search for within ingested issues", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of issues from Jira project to be read", default=10)

class JiraIssueIngestTool(BaseTool):
    name: str = "Graphlit Jira ingest tool"
    description: str = """Ingests issues from Atlassian Jira into knowledge base.
    Accepts Atlassian Jira server URL and project name.
    Optionally accepts search text for searching within the ingested issues. If search text was not provided, all ingested issues will be returned.
    Returns extracted Markdown text and metadata from issues."""
    args_schema: Type[BaseModel] = JiraIssueIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the JiraIssueIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting issues. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, url: str, project: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        email = os.environ['JIRA_EMAIL']

        if email is None:
            raise ToolException('Invalid Atlassian Jira email address. Need to assign JIRA_EMAIL environment variable.')

        token = os.environ['JIRA_TOKEN']

        if token is None:
            raise ToolException('Invalid Atlassian Jira token. Need to assign JIRA_TOKEN environment variable.')

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name='Jira',
                    type=enums.FeedTypes.ISSUE,
                    issue=input_types.IssueFeedPropertiesInput(
                        type=enums.FeedServiceTypes.ATLASSIAN_JIRA,
                        jira=input_types.AtlassianJiraFeedPropertiesInput(
                            uri=url,
                            project=project,
                            email=email,
                            token=token,
                        ),
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, url: str, project: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, url, project, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/linear_issue_ingest_tool.py
================================================
import logging
import time
import os
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class LinearIssueIngestInput(BaseModel):
    project: str = Field(description="Linear project name")
    search: Optional[str] = Field(description="Text to search for within ingested issues", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of issues from Linear project to be read", default=10)

class LinearIssueIngestTool(BaseTool):
    name: str = "Graphlit Linear ingest tool"
    description: str = """Ingests issues from Linear project into knowledge base.
    Accepts Linear project name.
    Optionally accepts search text for searching within the ingested issues. If search text was not provided, all ingested issues will be returned.
    Returns extracted Markdown text and metadata from issues."""
    args_schema: Type[BaseModel] = LinearIssueIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the LinearIssueIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting issues. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, project: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        key = os.environ['LINEAR_API_KEY']

        if key is None:
            raise ToolException('Invalid Linear API key. Need to assign LINEAR_API_KEY environment variable.')

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name='Linear',
                    type=enums.FeedTypes.ISSUE,
                    issue=input_types.IssueFeedPropertiesInput(
                        type=enums.FeedServiceTypes.LINEAR,
                        linear=input_types.LinearFeedPropertiesInput(
                            project=project,
                            key=key,
                        ),
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, uri: str, project: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, uri, project, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/local_ingest_tool.py
================================================
import logging
import os
import base64
import mimetypes
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class LocalIngestInput(BaseModel):
    file_path: str = Field(description="Path of local file to be ingested into knowledge base")

class LocalIngestTool(BaseTool):
    name: str = "Graphlit local file ingest tool"
    description: str = """Ingests content from local file.
    Returns extracted Markdown text and metadata from content.
    Can ingest individual Word documents, PDFs, audio recordings, videos, images, or any other unstructured data."""
    args_schema: Type[BaseModel] = LocalIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the LocalIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting files. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, file_path: str) -> Optional[str]:
        content_id = None

        try:
            file_name = os.path.basename(file_path)
            content_name, _ = os.path.splitext(file_name)

            mime_type = mimetypes.guess_type(file_name)[0]

            if mime_type is None:
                logger.error(f'Failed to infer MIME type from file [{file_name}].')
                raise ToolException(f'Failed to infer MIME type from file [{file_name}].')

            with open(file_path, "rb") as file:
                file_content = file.read()

            base64_content = base64.b64encode(file_content).decode('utf-8')

            response = await self.graphlit.client.ingest_encoded_file(content_name, base64_content, mime_type, is_synchronous=True)

            content_id = response.ingest_encoded_file.id if response.ingest_encoded_file is not None else None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if content_id is None:
            raise ToolException('Invalid content identifier.')

        try:
            response = await self.graphlit.client.get_content(
                id=content_id
            )

            if response.content is None:
                raise ToolException(f'Failed to get content [{content_id}].')

            logger.debug(f'LocalIngestTool: Retrieved content by ID [{content_id}].')

            results = helpers.format_content(response.content)

            text = "\n".join(results)

            return text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

    def _run(self, file_path: str) -> Optional[str]:
        return helpers.run_async(self._arun, file_path)



================================================
FILE: graphlit_tools/ingestion/microsoft_email_ingest_tool.py
================================================
import logging
import time
import os
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class MicrosoftEmailIngestInput(BaseModel):
    search: Optional[str] = Field(description="Text to search for within ingested email", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of emails from Microsoft Email account to be read", default=10)

class MicrosoftEmailIngestTool(BaseTool):
    name: str = "Graphlit Microsoft Email ingest tool"
    description: str = """Ingests emails from Microsoft Email account into knowledge base.
    Optionally accepts search text for searching within the ingested emails. If search text was not provided, all ingested emails will be returned.
    Returns extracted Markdown text and metadata from emails."""
    args_schema: Type[BaseModel] = MicrosoftEmailIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the MicrosoftEmailIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting emails. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        refresh_token = os.environ['MICROSOFT_EMAIL_REFRESH_TOKEN']

        if refresh_token is None:
            raise ToolException('Invalid Microsoft Email refresh token. Need to assign MICROSOFT_EMAIL_REFRESH_TOKEN environment variable.')

        client_id = os.environ['MICROSOFT_EMAIL_CLIENT_ID']

        if client_id is None:
            raise ToolException('Invalid Microsoft Email client identifier. Need to assign MICROSOFT_EMAIL_CLIENT_ID environment variable.')

        client_secret = os.environ['MICROSOFT_EMAIL_CLIENT_SECRET']

        if client_secret is None:
            raise ToolException('Invalid Microsoft Email client secret. Need to assign MICROSOFT_EMAIL_CLIENT_SECRET environment variable.')

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name='Microsoft Email',
                    type=enums.FeedTypes.EMAIL,
                    email=input_types.EmailFeedPropertiesInput(
                        type=enums.FeedServiceTypes.MICROSOFT_EMAIL,
                        microsoft=input_types.MicrosoftEmailFeedPropertiesInput(
                            type=enums.EmailListingTypes.PAST,
                            refreshToken=refresh_token,
                            clientId=client_id,
                            clientSecret=client_secret,
                        ),
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, search: Optional[str] = None, read_limit: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/microsoft_teams_ingest_tool.py
================================================
import logging
import time
import os
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class MicrosoftTeamsIngestInput(BaseModel):
    team_name: str = Field(description="Microsoft Teams team name")
    channel_name: str = Field(description="Microsoft Teams channel name")
    search: Optional[str] = Field(description="Text to search for within ingested messages", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of messages from Microsoft Teams channel to be read", default=10)

class MicrosoftTeamsIngestTool(BaseTool):
    name: str = "Graphlit Microsoft Teams ingest tool"
    description: str = """Ingests messages from Microsoft Teams channel into knowledge base.
    Optionally accepts search text for searching within the ingested messages. If search text was not provided, all ingested messages will be returned.
    Returns extracted Markdown text and metadata from messages."""
    args_schema: Type[BaseModel] = MicrosoftTeamsIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the MicrosoftTeamsIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting messages. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, team_name: Optional[str] = None, channel_name: Optional[str] = None, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        team_id = os.environ['MICROSOFT_TEAMS_TEAM_ID']
        channel_id = os.environ['MICROSOFT_TEAMS_CHANNEL_ID']

        refresh_token = os.environ['MICROSOFT_TEAMS_REFRESH_TOKEN']

        if refresh_token is None:
            raise ToolException('Invalid Microsoft Teams refresh token. Need to assign MICROSOFT_TEAMS_REFRESH_TOKEN environment variable.')

        client_id = os.environ['MICROSOFT_TEAMS_CLIENT_ID']

        if client_id is None:
            raise ToolException('Invalid Microsoft Teams client identifier. Need to assign MICROSOFT_TEAMS_CLIENT_ID environment variable.')

        client_secret = os.environ['MICROSOFT_TEAMS_CLIENT_SECRET']

        if client_secret is None:
            raise ToolException('Invalid Microsoft Teams client secret. Need to assign MICROSOFT_TEAMS_CLIENT_SECRET environment variable.')

        teams = None

        try:
            response = await self.graphlit.client.query_microsoft_teams_teams(
                properties=input_types.MicrosoftTeamsTeamsInput(
                    refreshToken=refresh_token,
                )
            )

            teams = response.microsoft_teams_teams.results if response.microsoft_teams_teams is not None else None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if len(teams) == 0:
            raise ToolException('No Microsoft Teams teams were found.')

        team = next(filter(lambda x: x['team_name'] == team_name, teams), None) if team_name is not None else teams[0]

        team_id = team.team_id if team is not None else team_id

        if team_id is None:
            raise ToolException('Invalid Microsoft Teams team identifier. Need to assign MICROSOFT_TEAMS_TEAM_ID environment variable.')

        channels = None

        try:
            response = await self.graphlit.client.query_microsoft_teams_channels(
                properties=input_types.MicrosoftTeamsChannelsInput(
                    refreshToken=refresh_token,
                ),
                team_id=team_id
            )

            channels = response.microsoft_teams_channels.results if response.microsoft_teams_channels is not None else None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if len(channels) == 0:
            raise ToolException('No Microsoft Teams channels were found.')

        channel = next(filter(lambda x: x['channel_name'] == channel_name, channels), None) if channel_name is not None else channels[0]

        channel_id = channel.channel_id if channel is not None else channel_id

        if channel_id is None:
            raise ToolException('Invalid Microsoft Teams channel identifier. Need to assign MICROSOFT_TEAMS_CHANNEL_ID environment variable.')

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name='Microsoft Teams',
                    type=enums.FeedTypes.MICROSOFT_TEAMS,
                    microsoftTeams=input_types.MicrosoftTeamsFeedPropertiesInput(
                        type=enums.FeedListingTypes.PAST,
                        teamId=team_id,
                        channelId=channel_id,
                        clientId=client_id,
                        clientSecret=client_secret,
                        refreshToken=refresh_token,
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, team_name: Optional[str] = None, channel_name: Optional[str] = None, search: Optional[str] = None, read_limit: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, team_name, channel_name, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/notion_ingest_tool.py
================================================
import logging
import time
import os
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class NotionIngestInput(BaseModel):
    search: Optional[str] = Field(description="Text to search for within ingested pages", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of pages from Notion database to be read", default=10)

class NotionIngestTool(BaseTool):
    name: str = "Graphlit Notion ingest tool"
    description: str = """Ingests pages from Notion database into knowledge base.
    Optionally accepts search text for searching within the ingested pages. If search text was not provided, all ingested pages will be returned.
    Returns extracted Markdown text and metadata from Notion pages."""
    args_schema: Type[BaseModel] = NotionIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the NotionIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting pages. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        token = os.environ['NOTION_API_KEY']

        if token is None:
            raise ToolException('Invalid Notion API key. Need to assign NOTION_API_KEY environment variable.')

        database_id = os.environ['NOTION_DATABASE_ID']

        if database_id is None:
            raise ToolException('Invalid Notion database identifier. Need to assign NOTION_DATABASE_ID environment variable.')

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name='Notion Feed',
                    type=enums.FeedTypes.NOTION,
                    notion=input_types.NotionFeedPropertiesInput(
                        type=enums.NotionTypes.DATABASE,
                        identifiers=[database_id],
                        token=token,
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/reddit_ingest_tool.py
================================================
import logging
import time
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class RedditIngestInput(BaseModel):
    subreddit_name: str = Field(description="Reddit subreddit name to be read and ingested into knowledge base")
    search: Optional[str] = Field(description="Text to search for within ingested posts", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of posts from Reddit subreddit to be read", default=10)

class RedditIngestTool(BaseTool):
    name: str = "Graphlit Reddit ingest tool"
    description: str = """Ingests posts from Reddit subreddit into knowledge base.
    Optionally accepts search text for searching within the ingested posts. If search text was not provided, all ingested posts will be returned.
    Returns extracted Markdown text and metadata from Reddit posts."""
    args_schema: Type[BaseModel] = RedditIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the RedditIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting posts. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, subreddit_name: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name=f'Reddit Feed [{subreddit_name}]',
                    type=enums.FeedTypes.REDDIT,
                    reddit=input_types.RedditFeedPropertiesInput(
                        subredditName=subreddit_name,
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, subreddit_name: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, subreddit_name, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/rss_ingest_tool.py
================================================
import logging
import time
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class RSSIngestInput(BaseModel):
    url: str = Field(description="RSS URL to be read and ingested into knowledge base")
    search: Optional[str] = Field(description="Text to search for within ingested posts", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of posts from RSS feed to be read", default=10)

class RSSIngestTool(BaseTool):
    name: str = "Graphlit RSS ingest tool"
    description: str = """Ingests posts from RSS feed into knowledge base.
    For podcast RSS feeds, audio will be transcribed and ingested into knowledge base.
    Optionally accepts search text for searching within the ingested posts. If search text was not provided, all ingested posts will be returned.
    Returns extracted Markdown text and metadata from RSS posts."""
    args_schema: Type[BaseModel] = RSSIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the RSSIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting posts. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, url: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name=f'RSS Feed [{url}]',
                    type=enums.FeedTypes.RSS,
                    rss=input_types.RSSFeedPropertiesInput(
                        uri=url,
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, url: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, url, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/slack_ingest_tool.py
================================================
import logging
import time
import os
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class SlackIngestInput(BaseModel):
    channel_name: str = Field(description="Slack channel name")
    search: Optional[str] = Field(description="Text to search for within ingested messages.", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of messages from Slack channel to be read.", default=10)

class SlackIngestTool(BaseTool):
    name: str = "Graphlit Slack ingest tool"
    description: str = """Ingests messages from Slack channel into knowledge base.
    Accepts Slack channel name.
    Optionally accepts search text for searching within the ingested messages. If search text was not provided, all ingested messages will be returned.
    Returns extracted Markdown text and metadata from messages."""
    args_schema: Type[BaseModel] = SlackIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the SlackIngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting messages. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, channel_name: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        token = os.environ['SLACK_BOT_TOKEN']

        if token is None:
            raise ToolException('Invalid Slack bot token. Need to assign SLACK_BOT_TOKEN environment variable.')

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name='Slack',
                    type=enums.FeedTypes.SLACK,
                    slack=input_types.SlackFeedPropertiesInput(
                        type=enums.FeedListingTypes.PAST,
                        channel=channel_name,
                        token=token,
                        includeAttachments=True,
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, channel_name: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> str:
        return helpers.run_async(self._arun, channel_name, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/url_ingest_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class URLIngestInput(BaseModel):
    url: str = Field(description="URL of cloud-hosted file to be ingested into knowledge base")

class URLIngestTool(BaseTool):
    name: str = "Graphlit URL ingest tool"
    description: str = """Ingests content from URL.
    Returns extracted Markdown text and metadata from content.
    Can ingest individual Word documents, PDFs, audio recordings, videos, images, or any other unstructured data."""
    args_schema: Type[BaseModel] = URLIngestInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the IngestTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting files. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, url: str) -> Optional[str]:
        content_id = None

        try:
            response = await self.graphlit.client.ingest_uri(
                uri=url,
                workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                is_synchronous=True,
                correlation_id=self.correlation_id
            )

            content_id = response.ingest_uri.id if response.ingest_uri is not None else None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if content_id is None:
            raise ToolException('Invalid content identifier.')

        try:
            response = await self.graphlit.client.get_content(
                id=content_id
            )

            if response.content is None:
                raise ToolException(f'Failed to get content [{content_id}].')

            logger.debug(f'URLIngestTool: Retrieved content by ID [{content_id}].')

            results = helpers.format_content(response.content)

            text = "\n".join(results)

            return text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

    def _run(self, url: str) -> Optional[str]:
        return helpers.run_async(self._arun, url)



================================================
FILE: graphlit_tools/ingestion/web_crawl_tool.py
================================================
import logging
import time
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class WebCrawlInput(BaseModel):
    url: str = Field(description="URL of web site to be crawled and ingested into knowledge base")
    search: Optional[str] = Field(description="Text to search for within ingested web pages", default=None)
    read_limit: Optional[int] = Field(description="Maximum number of web pages from web site to be crawled")

class WebCrawlTool(BaseTool):
    name: str = "Graphlit web crawl tool"
    description: str = """Crawls web pages from web site into knowledge base.
    Returns Markdown text and metadata extracted from web pages."""
    args_schema: Type[BaseModel] = WebCrawlInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the WebCrawlTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting web pages. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, url: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        feed_id = None

        try:
            response = await self.graphlit.client.create_feed(
                feed=input_types.FeedInput(
                    name=f'Web Feed [{url}]',
                    type=enums.FeedTypes.WEB,
                    web=input_types.WebFeedPropertiesInput(
                        uri=url,
                        readLimit=read_limit if read_limit is not None else 10
                    ),
                    workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                ),
                correlation_id=self.correlation_id
            )

            feed_id = response.create_feed.id if response.create_feed is not None else None

            if feed_id is None:
                raise ToolException('Invalid feed identifier.')

            logger.debug(f'Created feed [{feed_id}].')

            # Wait for feed to complete, since ingestion happens asychronously
            done = False
            time.sleep(5)

            while not done:
                done = await helpers.is_feed_done(self.graphlit.client, feed_id)

                if done is None:
                    break

                if not done:
                    time.sleep(5)

            logger.debug(f'Completed feed [{feed_id}].')
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        return await helpers.format_feed_contents(self.graphlit.client, feed_id, search)

    def _run(self, url: str, search: Optional[str] = None, read_limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, url, search, read_limit)



================================================
FILE: graphlit_tools/ingestion/web_map_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class WebMapInput(BaseModel):
    url: str = Field(description="URL of the web page to be mapped")

class WebMapTool(BaseTool):
    name: str = "Graphlit web map tool"
    description: str = """Accepts web page URL as string.
    Enumerates the web pages at or beneath the provided URL using web sitemap.
    Returns list of mapped URIs from web site."""
    args_schema: Type[BaseModel] = WebMapInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, correlation_id: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.correlation_id = correlation_id

    async def _arun(self, url: str) -> Optional[str]:
        try:
            response = await self.graphlit.client.map_web(
                uri=url,
                correlation_id=self.correlation_id
            )

            results = response.map_web.results if response.map_web is not None else None

            if results is not None:
                logger.debug(f'Completed web map, found [{len(results)}] results.')

                return '\n'.join(result for result in results) if results is not None else None
            else:
                return None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

    def _run(self, url: str) -> Optional[str]:
        return helpers.run_async(self._arun, url)



================================================
FILE: graphlit_tools/ingestion/web_scrape_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class WebScrapeInput(BaseModel):
    url: str = Field(description="URL of web page to be scraped and ingested into knowledge base")

class WebScrapeTool(BaseTool):
    name: str = "Graphlit web scrape tool"
    description: str = """Scrapes web page into knowledge base.
    Returns Markdown text and metadata extracted from web page."""
    args_schema: Type[BaseModel] = WebScrapeInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, workflow_id: Optional[str] = None, correlation_id: Optional[str] = None, **kwargs):
        """
        Initializes the WebScrapeTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            workflow_id (Optional[str]): ID for the workflow to use when ingesting web pages. Defaults to None.
            correlation_id (Optional[str]): Correlation ID for tracking requests. Defaults to None.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.workflow_id = workflow_id
        self.correlation_id = correlation_id

    async def _arun(self, url: str) -> Optional[str]:
        content_id = None

        try:
            response = await self.graphlit.client.ingest_uri(
                uri=url,
                workflow=input_types.EntityReferenceInput(id=self.workflow_id) if self.workflow_id is not None else None,
                is_synchronous=True,
                correlation_id=self.correlation_id
            )

            content_id = response.ingest_uri.id if response.ingest_uri is not None else None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

        if content_id is None:
            raise ToolException('Invalid content identifier.')

        try:
            response = await self.graphlit.client.get_content(
                id=content_id
            )

            if response.content is None:
                raise ToolException(f'Failed to get content [{content_id}].')

            logger.debug(f'WebScrapeTool: Retrieved content by ID [{content_id}].')

            results = helpers.format_content(response.content)

            text = "\n".join(results)

            return text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

    def _run(self, url: str) -> Optional[str]:
        return helpers.run_async(self._arun, url)



================================================
FILE: graphlit_tools/ingestion/web_search_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class WebSearchInput(BaseModel):
    search: str = Field(description="Text to search for within web pages across the Internet")
    search_limit: Optional[int] = Field(description="Maximum number of web pages to be returned from web search", default=10)

class WebSearchTool(BaseTool):
    name: str = "Graphlit web search tool"
    description: str = """Accepts search query as string.
    Performs web search based on search query. Format the search query as what would be entered into a Google search.
    Returns URL, title and relevant Markdown text from resulting web pages."""
    args_schema: Type[BaseModel] = WebSearchInput

    graphlit: Graphlit = Field(None, exclude=True)

    workflow_id: Optional[str] = Field(None, exclude=True)
    correlation_id: Optional[str] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, correlation_id: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.correlation_id = correlation_id

    async def _arun(self, search: str, search_limit: Optional[int] = None) -> Optional[str]:
        try:
            response = await self.graphlit.client.search_web(
                service=enums.SearchServiceTypes.TAVILY,
                text=search,
                limit=search_limit,
                correlation_id=self.correlation_id
            )

            results = response.search_web.results if response.search_web is not None else None

            if results is not None:
                logger.debug(f'Completed web search, found [{len(results)}] results.')

                return '\n\n'.join(f'URL: {result.uri}\nTitle: {result.title}\n\n{result.text}' for result in results) if results is not None else None
            else:
                return None
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

    def _run(self, search: str, search_limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, search, search_limit)



================================================
FILE: graphlit_tools/retrieval/__init__.py
================================================



================================================
FILE: graphlit_tools/retrieval/content_retrieval_tool.py
================================================
import logging
from typing import Optional, Type, List

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class ContentRetrievalInput(BaseModel):
    search: str = Field(description="Text to search for within the knowledge base")
    types: Optional[List[enums.ContentTypes]] = Field(description="List of content types (i.e. FILE, PAGE, EMAIL, ISSUE, MESSAGE) to be returned from knowledge base, optional.", default=None)
    limit: Optional[int] = Field(description="Number of contents to return from search query, optional.", default=None)

class ContentRetrievalTool(BaseTool):
    name: str = "Graphlit content retrieval tool"
    description: str = """Accepts search text as string.
    Optionally accepts a list of content types (i.e. FILE, PAGE, EMAIL, ISSUE, MESSAGE) for filtering the result set.
    Retrieves contents based on similarity search from knowledge base.
    Returns extracted Markdown text and metadata from contents relevant to the search text.
    Can search through web pages, PDFs, audio transcripts, Slack messages, emails, or any unstructured data ingested into the knowledge base."""
    args_schema: Type[BaseModel] = ContentRetrievalInput

    graphlit: Graphlit = Field(None, exclude=True)
    search_type: Optional[enums.SearchTypes] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, search_type: Optional[enums.SearchTypes] = None, **kwargs):
        """
        Initializes the ContentRetrievalTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            search_type (Optional[SearchTypes]): An optional enum specifying the type of search to use: VECTOR, HYBRID or KEYWORD.
                If not provided, vector search will be used.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.search_type = search_type

    async def _arun(self, search: str, types: Optional[List[enums.ContentTypes]] = None, limit: Optional[int] = None) -> Optional[str]:
        try:
            response = await self.graphlit.client.query_contents(
                filter=input_types.ContentFilter(
                    types=types,
                    search=search,
                    searchType=self.search_type if self.search_type is not None else enums.SearchTypes.HYBRID,
                    limit=limit if limit is not None else 10 # NOTE: default to 10 relevant contents
                )
            )

            if response.contents is None or response.contents.results is None:
                raise ToolException('Failed to retrieve contents.')

            logger.debug(f'ContentRetrievalTool: Retrieved [{len(response.contents.results)}] content(s) given search text [{search}].')

            results = []

            for content in response.contents.results:
                results.extend(helpers.format_content(content))

            text = "\n".join(results)

            return text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

    def _run(self, search: str, types: Optional[List[enums.ContentTypes]] = None, limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, search, types, limit)



================================================
FILE: graphlit_tools/retrieval/organization_retrieval_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class OrganizationRetrievalInput(BaseModel):
    search: str = Field(description="Text to search for within the knowledge base")
    limit: Optional[int] = Field(description="Number of organizations to return from search query, optional.", default=10)

class OrganizationRetrievalTool(BaseTool):
    name: str = "Graphlit organization retrieval tool"
    description: str = """Accepts search text as string.
    Retrieves organizations based on similarity search from knowledge base.
    Returns metadata from organizations relevant to the search text."""
    args_schema: Type[BaseModel] = OrganizationRetrievalInput

    graphlit: Graphlit = Field(None, exclude=True)
    search_type: Optional[enums.SearchTypes] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, search_type: Optional[enums.SearchTypes] = None, **kwargs):
        """
        Initializes the OrganizationRetrievalTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            search_type (Optional[SearchTypes]): An optional enum specifying the type of search to use: VECTOR, HYBRID or KEYWORD.
                If not provided, vector search will be used.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.search_type = search_type

    async def _arun(self, search: str = None, limit: Optional[int] = None) -> Optional[str]:
        try:
            response = await self.graphlit.client.query_organizations(
                filter=input_types.OrganizationFilter(
                    search=search,
                    searchType=self.search_type if self.search_type is not None else enums.SearchTypes.HYBRID,
                    limit=limit if limit is not None else 10 # NOTE: default to 10 relevant organizations
                )
            )

            if response.organizations is None or response.organizations.results is None:
                return None

            logger.debug(f'OrganizationRetrievalTool: Retrieved [{len(response.organizations.results)}] organization(s) given search text [{search}].')

            results = []

            for organization in response.organizations.results:
                results.extend(helpers.format_organization(organization))

            text = "\n".join(results)

            return text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

    def _run(self, search: str = None, limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, search, limit)



================================================
FILE: graphlit_tools/retrieval/person_retrieval_tool.py
================================================
import logging
from typing import Optional, Type

from graphlit import Graphlit
from graphlit_api import exceptions, input_types, enums
from pydantic import BaseModel, Field

from ..base_tool import BaseTool
from ..exceptions import ToolException
from .. import helpers

logger = logging.getLogger(__name__)

class PersonRetrievalInput(BaseModel):
    search: str = Field(description="Text to search for within the knowledge base")
    email: Optional[str] = Field(description="Email of person to retrieve.", default=None)
    limit: Optional[int] = Field(description="Number of persons to return from search query, optional.", default=10)

class PersonRetrievalTool(BaseTool):
    name: str = "Graphlit person retrieval tool"
    description: str = """Accepts search text as string. Optionally, accepts person email as string.
    Retrieves persons based on similarity search from knowledge base.
    Returns metadata from persons relevant to the search text."""
    args_schema: Type[BaseModel] = PersonRetrievalInput

    graphlit: Graphlit = Field(None, exclude=True)
    search_type: Optional[enums.SearchTypes] = Field(None, exclude=True)

    model_config = {
        "arbitrary_types_allowed": True
    }

    def __init__(self, graphlit: Optional[Graphlit] = None, search_type: Optional[enums.SearchTypes] = None, **kwargs):
        """
        Initializes the PersonRetrievalTool.

        Args:
            graphlit (Optional[Graphlit]): An optional Graphlit instance to interact with the Graphlit API.
                If not provided, a new Graphlit instance will be created.
            search_type (Optional[SearchTypes]): An optional enum specifying the type of search to use: VECTOR, HYBRID or KEYWORD.
                If not provided, vector search will be used.
            **kwargs: Additional keyword arguments for the BaseTool superclass.
        """
        super().__init__(**kwargs)
        self.graphlit = graphlit or Graphlit()
        self.search_type = search_type

    async def _arun(self, search: str = None, email: Optional[str] = None, limit: Optional[int] = None) -> Optional[str]:
        try:
            response = await self.graphlit.client.query_persons(
                filter=input_types.PersonFilter(
                    search=search,
                    email=email,
                    searchType=self.search_type if self.search_type is not None else enums.SearchTypes.HYBRID,
                    limit=limit if limit is not None else 10 # NOTE: default to 10 relevant persons
                )
            )

            if response.persons is None or response.persons.results is None:
                return None

            logger.debug(f'PersonRetrievalTool: Retrieved [{len(response.persons.results)}] person(s) given search text [{search}].')

            results = []

            for person in response.persons.results:
                results.extend(helpers.format_person(person))

            text = "\n".join(results)

            return text
        except exceptions.GraphQLClientError as e:
            logger.error(str(e))
            raise ToolException(str(e)) from e

    def _run(self, search: str = None, email: Optional[str] = None, limit: Optional[int] = None) -> Optional[str]:
        return helpers.run_async(self._arun, search, email, limit)


