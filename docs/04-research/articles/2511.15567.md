# CUA as Judge: Agent-Native UI Design

**Source:** [Computer-Use Agents as Judges for Generative User Interface](https://arxiv.org/html/2511.15567)

## Synthesis
This paper proposes a radical shift in how we design User Interfaces (UIs) for **Computer-Use Agents (CUAs)**. Currently, agents try to use UIs designed for humans (with animations, hover states, complex layouts). The authors argue that we should let the **Agent** judge the UI.

The framework involves two roles:
1.  **Coder (Designer):** Generates a UI based on requirements.
2.  **CUA (Judge):** Tries to perform tasks on that UI. It provides two types of feedback:
    *   **Task Solvability:** "I literally cannot do this because the button doesn't exist."
    *   **Navigation Feedback:** "I failed because I had to scroll too much" or "I couldn't distinguish the button from the text."

The authors introduce **AUI-Gym**, a benchmark where agents redesign UIs to be "Agent-Native". The results show that agents prefer **high contrast, simple layouts, explicit buttons (vs hover), and minimal scrolling**.

## Core Strategic Ideas
1.  **Agent-Native Interfaces:** Instead of making agents "human-like", make the environment "agent-friendly". For CodeSwarm, this means our agents shouldn't just read "human" logs. We should expose **structured, agent-native debug interfaces**.
2.  **The CUA Dashboard:** A visual summary of an agent's run. Instead of a video, it's a single high-res image showing the "Key Interactive Regions" cropped and tiled. This is a very efficient way for a "Manager Agent" to review the work of a "Worker Agent" without re-reading the whole log.
3.  **Feedback-Driven Refinement:** The Coder agent improves the code *specifically* to address the failures of the Tester agent. "The Tester couldn't find the Submit button, so I will make it red and put it at the top."

## Integration Plan (Agno + SurrealDB + Gemini 3)
We can apply "Agent-Native Design" to the internal tools the agents use:

1.  **Agent-Friendly Tools:**
    *   Instead of `ls -la` (human readable), create `list_files_json` (agent readable).
    *   Instead of `git diff` (human readable), create `git_diff_structured` (returning a JSON with added/removed lines).
    *   Store these tools in `codeswarm/tools/agent_native/`.

2.  **Dashboard for Meta-Review:**
    *   When an agent fails a task, generate a "Dashboard" summary.
    *   Query `SurrealDB` for the agent's step trace.
    *   Use `Agno` to generate a markdown summary of *only* the steps where errors occurred.
    *   Pass this summary to the **Manager Agent** to decide on a retry strategy.

3.  **Self-Correction Loop:**
    *   Implement the "Coder-Judge" loop in `codeswarm/patterns/self_correction.py`.
    *   `Coder` writes code.
    *   `Judge` runs it. If it fails, `Judge` explains *why* in a structured format (e.g., "RuntimeError on line 5").
    *   `Coder` receives the structured error (not just the log) and fixes it.

4.  **Gemini 3 Vision:**
    *   If we ever add GUI testing to CodeSwarm, we can use Gemini 3 to "look" at the screenshots of the app we are building and critique them: "This button is too small for a mouse click action."
