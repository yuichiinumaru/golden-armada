#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…ç®€åŒ–LLMå“åº”æ—¥å¿—è®°å½•å™¨
ä¸“æ³¨äºè®°å½•LLMå›å¤çš„æ ¸å¿ƒå†…å®¹ï¼Œé…ç½®ç®€å•æ˜“ç”¨
"""

import json
import os
import yaml
from datetime import datetime
from pathlib import Path
from typing import Dict, Any


class SimpleLLMLogger:
    """è¶…ç®€åŒ–çš„LLMå“åº”æ—¥å¿—è®°å½•å™¨"""

    def __init__(self, config_path: str = "mcp_agent.config.yaml"):
        """
        åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.llm_config = self.config.get("llm_logger", {})

        # å¦‚æœç¦ç”¨åˆ™ç›´æ¥è¿”å›
        if not self.llm_config.get("enabled", True):
            self.enabled = False
            return

        self.enabled = True
        self._setup_logger()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "llm_logger": {
                "enabled": True,
                "output_format": "json",
                "log_level": "basic",
                "log_directory": "logs/llm_responses",
                "filename_pattern": "llm_responses_{timestamp}.jsonl",
                "include_models": ["claude-sonnet-4", "gpt-4", "o3-mini"],
                "min_response_length": 50,
            }
        }

    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        log_dir = self.llm_config.get("log_directory", "logs/llm_responses")

        # åˆ›å»ºæ—¥å¿—ç›®å½•
        Path(log_dir).mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename_pattern = self.llm_config.get(
            "filename_pattern", "llm_responses_{timestamp}.jsonl"
        )
        self.log_file = os.path.join(
            log_dir, filename_pattern.format(timestamp=timestamp)
        )

        print(f"ğŸ“ LLMå“åº”æ—¥å¿—: {self.log_file}")

    def log_response(self, content: str, model: str = "", agent: str = "", **kwargs):
        """
        è®°å½•LLMå“åº” - ç®€åŒ–ç‰ˆæœ¬

        Args:
            content: LLMå“åº”å†…å®¹
            model: æ¨¡å‹åç§°
            agent: Agentåç§°
            **kwargs: å…¶ä»–å¯é€‰ä¿¡æ¯
        """
        if not self.enabled:
            return

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è®°å½•
        if not self._should_log(content, model):
            return

        # æ„å»ºæ—¥å¿—è®°å½•
        log_entry = self._build_entry(content, model, agent, kwargs)

        # å†™å…¥æ—¥å¿—
        self._write_log(log_entry)

        # æ§åˆ¶å°æ˜¾ç¤º
        self._console_log(content, model, agent)

    def _should_log(self, content: str, model: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è®°å½•"""
        # æ£€æŸ¥é•¿åº¦
        min_length = self.llm_config.get("min_response_length", 50)
        if len(content) < min_length:
            return False

        # æ£€æŸ¥æ¨¡å‹
        include_models = self.llm_config.get("include_models", [])
        if include_models and not any(m in model for m in include_models):
            return False

        return True

    def _build_entry(self, content: str, model: str, agent: str, extra: Dict) -> Dict:
        """æ„å»ºæ—¥å¿—æ¡ç›®"""
        log_level = self.llm_config.get("log_level", "basic")

        if log_level == "basic":
            # åŸºç¡€çº§åˆ«ï¼šåªè®°å½•æ ¸å¿ƒå†…å®¹
            return {
                "timestamp": datetime.now().isoformat(),
                "content": content,
                "model": model,
            }
        else:
            # è¯¦ç»†çº§åˆ«ï¼šåŒ…å«æ›´å¤šä¿¡æ¯
            entry = {
                "timestamp": datetime.now().isoformat(),
                "content": content,
                "model": model,
                "agent": agent,
            }
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            if "token_usage" in extra:
                entry["tokens"] = extra["token_usage"]
            if "session_id" in extra:
                entry["session"] = extra["session_id"]
            return entry

    def _write_log(self, entry: Dict):
        """å†™å…¥æ—¥å¿—æ–‡ä»¶"""
        output_format = self.llm_config.get("output_format", "json")

        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                if output_format == "json":
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                elif output_format == "text":
                    timestamp = entry.get("timestamp", "")
                    model = entry.get("model", "")
                    content = entry.get("content", "")
                    f.write(f"[{timestamp}] {model}: {content}\n\n")
                elif output_format == "markdown":
                    timestamp = entry.get("timestamp", "")
                    model = entry.get("model", "")
                    content = entry.get("content", "")
                    f.write(f"**{timestamp}** | {model}\n\n{content}\n\n---\n\n")
        except Exception as e:
            print(f"âš ï¸ å†™å…¥æ—¥å¿—å¤±è´¥: {e}")

    def _console_log(self, content: str, model: str, agent: str):
        """æ§åˆ¶å°ç®€è¦æ˜¾ç¤º"""
        preview = content[:80] + "..." if len(content) > 80 else content
        print(f"ğŸ¤– {model} ({agent}): {preview}")


# å…¨å±€å®ä¾‹
_global_logger = None


def get_llm_logger() -> SimpleLLMLogger:
    """è·å–å…¨å±€LLMæ—¥å¿—è®°å½•å™¨å®ä¾‹"""
    global _global_logger
    if _global_logger is None:
        _global_logger = SimpleLLMLogger()
    return _global_logger


def log_llm_response(content: str, model: str = "", agent: str = "", **kwargs):
    """ä¾¿æ·å‡½æ•°ï¼šè®°å½•LLMå“åº”"""
    logger = get_llm_logger()
    logger.log_response(content, model, agent, **kwargs)


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # æµ‹è¯•æ—¥å¿—è®°å½•
    log_llm_response(
        content="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•çš„LLMå“åº”å†…å®¹ï¼Œç”¨äºéªŒè¯ç®€åŒ–æ—¥å¿—è®°å½•å™¨çš„åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚",
        model="claude-sonnet-4-20250514",
        agent="TestAgent",
    )

    print("âœ… ç®€åŒ–LLMæ—¥å¿—æµ‹è¯•å®Œæˆ")
