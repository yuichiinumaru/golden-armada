# Arxiv Analysis: AutoTool: Efficient Tool Selection for Large Language Model Agents

**ID:** 2511.14650
**Date:** 2025-11-18
**Link:** https://arxiv.org/html/2511.14650

## 1. Executive Summary
AutoTool presents a graph-based framework to reduce the high inference cost of LLM tool selection. It exploits **"Tool Usage Inertia"**â€”the empirical finding that tool sequences are highly predictable (low conditional entropy). By constructing a **Tool Inertia Graph (TIG)** from execution history, AutoTool can predict the next tool and its parameters without invoking the LLM, reducing inference costs by ~30% while maintaining performance.

## 2. Detailed Key Concepts

### 2.1. Tool Inertia Graph (TIG)
A dynamic graph structure $G_t = (V_t, E_t, W_t)$ where:
-   **Tool Nodes:** Represent tools (e.g., `read_file`).
-   **Tool Sequence Edges:** Directed edges $(v_i, v_j)$ representing sequential calls (e.g., `read` -> `write`).
-   **Parameter Nodes/Edges:** Track data flow. If `tool_A` output is used as `tool_B` input, a dependency edge is created.

### 2.2. Inertia Sensing (Algorithm 1)
Instead of asking the LLM "What next?", AutoTool calculates a **CIPS** (Comprehensive Inertia Potential Score) for candidate tools:
$$ \text{CIPS} = (1-\alpha) \cdot \text{Score}_{\text{freq}} + \alpha \cdot \text{Score}_{\text{ctx}} $$
-   **Freq:** Historical probability from the graph.
-   **Ctx:** Semantic similarity (embedding-based) between current thought and tool description.
-   If CIPS > Threshold, the tool is selected *without* LLM inference.

### 2.3. Hierarchical Parameter Filling (Algorithm 2)
Once a tool is selected via inertia, parameters are filled using a hierarchy:
1.  **Dependency Backtracking:** traversing TIG parameter edges (e.g., use filename from previous `ls` output).
2.  **Environment Context:** Matching state variables (e.g., current directory).
3.  **Heuristic:** Only if above fail, fall back to LLM or heuristics.

## 3. Gap Analysis

| Feature/Concept | Current CodeSwarm | Gap |
| :--- | :--- | :--- |
| **Tool Prediction** | 100% LLM inference | **Inefficient:** We ask Gemini for every single step, even obvious ones like `read_file` after `ls`. |
| **Graph Memory** | `codeswarm_events.jsonl` (flat log) | **High:** We log history but don't build a structured *transition graph* from it. |
| **Parameter Flow** | LLM copy-paste | **Medium:** LLM manually extracts params. We don't automatically pipe output A to input B. |

## 4. Implementation Plan (Agno + SurrealDB + Gemini)

### Phase 1: Build the TIG in SurrealDB
- **Schema:**
    - `tool_nodes`: `{ name: string }`
    - `sequence_edges`: `{ from: tool_node, to: tool_node, weight: float, success_count: int }`
- **Ingestion:** Write a script to parse existing `codeswarm_events.jsonl` and populate this graph in SurrealDB.

### Phase 2: Inertia Sensing Module
- **Action:** Before calling the DevAgent LLM, query SurrealDB:
    - "Given the last tool was X, what are the outgoing edges?"
    - If a destination Y has high weight (> 80% probability), suggest it.
- **Integration:** Inject this into the prompt: "Suggestion: The last tool was X. Usually you use Y next. If you want to use Y, just output 'NEXT'." (Or skip LLM entirely if confidence is extremely high, but prompt injection is safer first).

### Phase 3: Auto-Parameter Filling
- **Action:** If the LLM selects a tool but leaves params empty (or if we skip LLM), try to fill params from the *previous* tool's output using regex/logic.
- **Example:** `ls` output -> `read_file` filename.
