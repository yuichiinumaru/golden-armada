#!/usr/bin/env python3
"""
Smart PDF Downloader MCP Tool

A standardized MCP tool using FastMCP for intelligent file downloading and document conversion.
Supports natural language instructions for downloading files from URLs, moving local files,
and automatic conversion to Markdown format with image extraction.

Features:
- Natural language instruction parsing
- URL and local path extraction
- Automatic document conversion (PDF, DOCX, PPTX, HTML, etc.)
- Image extraction and preservation
- Multi-format support with fallback options
"""

import os
import re
import aiohttp
import aiofiles
import shutil
import sys
import io
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse, unquote
from datetime import datetime

from mcp.server import FastMCP

# Docling imports for document conversion
try:
    from docling.document_converter import DocumentConverter
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions
    from docling.document_converter import PdfFormatOption

    DOCLING_AVAILABLE = True
except ImportError:
    DOCLING_AVAILABLE = False
    print(
        "Warning: docling package not available. Document conversion will be disabled."
    )

# Fallback PDF text extraction
try:
    import PyPDF2

    PYPDF2_AVAILABLE = True
except ImportError:
    PYPDF2_AVAILABLE = False
    print(
        "Warning: PyPDF2 package not available. Fallback PDF extraction will be disabled."
    )

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8
if sys.stdout.encoding != "utf-8":
    try:
        if hasattr(sys.stdout, "reconfigure"):
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        else:
            sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding="utf-8")
            sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding="utf-8")
    except Exception as e:
        print(f"Warning: Could not set UTF-8 encoding: {e}")

# åˆ›å»º FastMCP å®ä¾‹
mcp = FastMCP("smart-pdf-downloader")


# è¾…åŠ©å‡½æ•°
def format_success_message(action: str, details: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–æˆåŠŸæ¶ˆæ¯"""
    return f"âœ… {action}\n" + "\n".join(f"   {k}: {v}" for k, v in details.items())


def format_error_message(action: str, error: str) -> str:
    """æ ¼å¼åŒ–é”™è¯¯æ¶ˆæ¯"""
    return f"âŒ {action}\n   Error: {error}"


def format_warning_message(action: str, warning: str) -> str:
    """æ ¼å¼åŒ–è­¦å‘Šæ¶ˆæ¯"""
    return f"âš ï¸ {action}\n   Warning: {warning}"


async def perform_document_conversion(
    file_path: str, extract_images: bool = True
) -> Optional[str]:
    """
    æ‰§è¡Œæ–‡æ¡£è½¬æ¢çš„å…±ç”¨é€»è¾‘

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        extract_images: æ˜¯å¦æå–å›¾ç‰‡

    Returns:
        è½¬æ¢ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æœ‰è½¬æ¢åˆ™è¿”å›None
    """
    if not file_path:
        return None

    conversion_msg = ""

    # é¦–å…ˆå°è¯•ä½¿ç”¨ç®€å•çš„PDFè½¬æ¢å™¨ï¼ˆå¯¹äºPDFæ–‡ä»¶ï¼‰
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®é™…ä¸ºPDFï¼ˆæ— è®ºæ‰©å±•åå¦‚ä½•ï¼‰
    is_pdf_file = False
    if PYPDF2_AVAILABLE:
        try:
            with open(file_path, "rb") as f:
                header = f.read(8)
                is_pdf_file = header.startswith(b"%PDF")
        except Exception:
            is_pdf_file = file_path.lower().endswith(".pdf")

    if is_pdf_file and PYPDF2_AVAILABLE:
        try:
            simple_converter = SimplePdfConverter()
            conversion_result = simple_converter.convert_pdf_to_markdown(file_path)
            if conversion_result["success"]:
                conversion_msg = "\n   [INFO] PDF converted to Markdown (PyPDF2)"
                conversion_msg += (
                    f"\n   Markdown file: {conversion_result['output_file']}"
                )
                conversion_msg += (
                    f"\n   Conversion time: {conversion_result['duration']:.2f} seconds"
                )
                conversion_msg += (
                    f"\n   Pages extracted: {conversion_result['pages_extracted']}"
                )

            else:
                conversion_msg = f"\n   [WARNING] PDF conversion failed: {conversion_result['error']}"
        except Exception as conv_error:
            conversion_msg = f"\n   [WARNING] PDF conversion error: {str(conv_error)}"

    # å¦‚æœç®€å•è½¬æ¢å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨doclingï¼ˆæ”¯æŒå›¾ç‰‡æå–ï¼‰
    # if not conversion_success and DOCLING_AVAILABLE:
    #     try:
    #         converter = DoclingConverter()
    #         if converter.is_supported_format(file_path):
    #             conversion_result = converter.convert_to_markdown(
    #                 file_path, extract_images=extract_images
    #             )
    #             if conversion_result["success"]:
    #                 conversion_msg = (
    #                     "\n   [INFO] Document converted to Markdown (docling)"
    #                 )
    #                 conversion_msg += (
    #                     f"\n   Markdown file: {conversion_result['output_file']}"
    #                 )
    #                 conversion_msg += f"\n   Conversion time: {conversion_result['duration']:.2f} seconds"
    #                 if conversion_result.get("images_extracted", 0) > 0:
    #                     conversion_msg += f"\n   Images extracted: {conversion_result['images_extracted']}"
    #                     images_dir = os.path.join(
    #                         os.path.dirname(conversion_result["output_file"]), "images"
    #                     )
    #                     conversion_msg += f"\n   Images saved to: {images_dir}"
    #             else:
    #                 conversion_msg = f"\n   [WARNING] Docling conversion failed: {conversion_result['error']}"
    #     except Exception as conv_error:
    #         conversion_msg = (
    #             f"\n   [WARNING] Docling conversion error: {str(conv_error)}"
    #         )

    return conversion_msg if conversion_msg else None


def format_file_operation_result(
    operation: str,
    source: str,
    destination: str,
    result: Dict[str, Any],
    conversion_msg: Optional[str] = None,
) -> str:
    """
    æ ¼å¼åŒ–æ–‡ä»¶æ“ä½œç»“æœçš„å…±ç”¨é€»è¾‘

    Args:
        operation: æ“ä½œç±»å‹ ("download", "copy", æˆ– "move")
        source: æºæ–‡ä»¶/URL
        destination: ç›®æ ‡è·¯å¾„
        result: æ“ä½œç»“æœå­—å…¸
        conversion_msg: è½¬æ¢æ¶ˆæ¯

    Returns:
        æ ¼å¼åŒ–çš„ç»“æœæ¶ˆæ¯
    """
    if result["success"]:
        size_mb = result["size"] / (1024 * 1024)

        # å¤„ç†ä¸åŒæ“ä½œç±»å‹çš„åŠ¨è¯å½¢å¼
        if operation == "copy":
            operation_verb = "copied"
        elif operation == "download":
            operation_verb = "downloaded"
        else:  # move
            operation_verb = "moved"

        msg = f"[SUCCESS] Successfully {operation_verb}: {source}\n"

        if operation == "download":
            msg += f"   File: {destination}\n"
            msg += f"   Size: {size_mb:.2f} MB\n"
            msg += f"   Time: {result['duration']:.2f} seconds\n"
            speed_mb = result.get("speed", 0) / (1024 * 1024)
            msg += f"   Speed: {speed_mb:.2f} MB/s"
        else:  # copy or move
            msg += f"   To: {destination}\n"
            msg += f"   Size: {size_mb:.2f} MB\n"
            msg += f"   Time: {result['duration']:.2f} seconds"
            if operation == "copy":
                msg += "\n   Note: Original file preserved"

        if conversion_msg:
            msg += conversion_msg

        return msg
    else:
        return f"[ERROR] Failed to {operation}: {source}\n   Error: {result.get('error', 'Unknown error')}"


class LocalPathExtractor:
    """æœ¬åœ°è·¯å¾„æå–å™¨"""

    @staticmethod
    def is_local_path(path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„"""
        path = path.strip("\"'")

        # æ£€æŸ¥æ˜¯å¦ä¸ºURL
        if re.match(r"^https?://", path, re.IGNORECASE) or re.match(
            r"^ftp://", path, re.IGNORECASE
        ):
            return False

        # è·¯å¾„æŒ‡ç¤ºç¬¦
        path_indicators = [os.path.sep, "/", "\\", "~", ".", ".."]
        has_extension = bool(os.path.splitext(path)[1])

        if any(indicator in path for indicator in path_indicators) or has_extension:
            expanded_path = os.path.expanduser(path)
            return os.path.exists(expanded_path) or any(
                indicator in path for indicator in path_indicators
            )

        return False

    @staticmethod
    def extract_local_paths(text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æœ¬åœ°æ–‡ä»¶è·¯å¾„"""
        patterns = [
            r'"([^"]+)"',
            r"'([^']+)'",
            r"(?:^|\s)((?:[~./\\]|[A-Za-z]:)?(?:[^/\\\s]+[/\\])*[^/\\\s]+\.[A-Za-z0-9]+)(?:\s|$)",
            r"(?:^|\s)((?:~|\.{1,2})?/[^\s]+)(?:\s|$)",
            r"(?:^|\s)([A-Za-z]:[/\\][^\s]+)(?:\s|$)",
            r"(?:^|\s)(\.{1,2}[/\\][^\s]+)(?:\s|$)",
        ]

        local_paths = []
        potential_paths = []

        for pattern in patterns:
            matches = re.findall(pattern, text, re.MULTILINE)
            potential_paths.extend(matches)

        for path in potential_paths:
            path = path.strip()
            if path and LocalPathExtractor.is_local_path(path):
                expanded_path = os.path.expanduser(path)
                if expanded_path not in local_paths:
                    local_paths.append(expanded_path)

        return local_paths


class URLExtractor:
    """URLæå–å™¨"""

    URL_PATTERNS = [
        r"https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/(?:[-\w._~!$&\'()*+,;=:@]|%[\da-fA-F]{2})*)*(?:\?(?:[-\w._~!$&\'()*+,;=:@/?]|%[\da-fA-F]{2})*)?(?:#(?:[-\w._~!$&\'()*+,;=:@/?]|%[\da-fA-F]{2})*)?",
        r"ftp://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/(?:[-\w._~!$&\'()*+,;=:@]|%[\da-fA-F]{2})*)*",
        r"(?<!\S)(?:www\.)?[-\w]+(?:\.[-\w]+)+/(?:[-\w._~!$&\'()*+,;=:@/]|%[\da-fA-F]{2})+",
    ]

    @staticmethod
    def convert_arxiv_url(url: str) -> str:
        """å°†arXivç½‘é¡µé“¾æ¥è½¬æ¢ä¸ºPDFä¸‹è½½é“¾æ¥"""
        # åŒ¹é…arXivè®ºæ–‡IDçš„æ­£åˆ™è¡¨è¾¾å¼
        arxiv_pattern = r"arxiv\.org/abs/(\d+\.\d+)(?:v\d+)?"
        match = re.search(arxiv_pattern, url, re.IGNORECASE)
        if match:
            paper_id = match.group(1)
            return f"https://arxiv.org/pdf/{paper_id}.pdf"
        return url

    @classmethod
    def extract_urls(cls, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–URL"""
        urls = []

        # é¦–å…ˆå¤„ç†ç‰¹æ®Šæƒ…å†µï¼š@å¼€å¤´çš„URL
        at_url_pattern = r"@(https?://[^\s]+)"
        at_matches = re.findall(at_url_pattern, text, re.IGNORECASE)
        for match in at_matches:
            # å¤„ç†arXivé“¾æ¥
            url = cls.convert_arxiv_url(match.rstrip("/"))
            urls.append(url)

        # ç„¶åä½¿ç”¨åŸæœ‰çš„æ­£åˆ™æ¨¡å¼
        for pattern in cls.URL_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # å¤„ç†å¯èƒ½ç¼ºå°‘åè®®çš„URL
                if not match.startswith(("http://", "https://", "ftp://")):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ www å¼€å¤´
                    if match.startswith("www."):
                        match = "https://" + match
                    else:
                        # å…¶ä»–æƒ…å†µä¹Ÿæ·»åŠ  https
                        match = "https://" + match

                # å¤„ç†arXivé“¾æ¥
                url = cls.convert_arxiv_url(match.rstrip("/"))
                urls.append(url)

        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_urls = []
        for url in urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)

        return unique_urls

    @staticmethod
    def infer_filename_from_url(url: str) -> str:
        """ä»URLæ¨æ–­æ–‡ä»¶å"""
        parsed = urlparse(url)
        path = unquote(parsed.path)

        # ä»è·¯å¾„ä¸­æå–æ–‡ä»¶å
        filename = os.path.basename(path)

        # ç‰¹æ®Šå¤„ç†ï¼šarxiv PDFé“¾æ¥
        if "arxiv.org" in parsed.netloc and "/pdf/" in path:
            if filename:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰åˆé€‚çš„æ–‡ä»¶æ‰©å±•å
                if not filename.lower().endswith((".pdf", ".doc", ".docx", ".txt")):
                    filename = f"{filename}.pdf"
            else:
                path_parts = [p for p in path.split("/") if p]
                if path_parts and path_parts[-1]:
                    filename = f"{path_parts[-1]}.pdf"
                else:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"arxiv_paper_{timestamp}.pdf"

        # å¦‚æœæ²¡æœ‰æ–‡ä»¶åæˆ–æ²¡æœ‰æ‰©å±•åï¼Œç”Ÿæˆä¸€ä¸ª
        elif not filename or "." not in filename:
            # å°è¯•ä»URLç”Ÿæˆæœ‰æ„ä¹‰çš„æ–‡ä»¶å
            domain = parsed.netloc.replace("www.", "").replace(".", "_")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # å°è¯•æ ¹æ®è·¯å¾„æ¨æ–­æ–‡ä»¶ç±»å‹
            if not path or path == "/":
                filename = f"{domain}_{timestamp}.html"
            else:
                # ä½¿ç”¨è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†
                path_parts = [p for p in path.split("/") if p]
                if path_parts:
                    filename = f"{path_parts[-1]}_{timestamp}"
                else:
                    filename = f"{domain}_{timestamp}"

                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰©å±•åï¼Œæ ¹æ®è·¯å¾„æ¨æ–­
                if "." not in filename:
                    # æ ¹æ®è·¯å¾„ä¸­çš„å…³é”®è¯æ¨æ–­æ–‡ä»¶ç±»å‹
                    if "/pdf/" in path.lower() or path.lower().endswith("pdf"):
                        filename += ".pdf"
                    elif any(
                        ext in path.lower() for ext in ["/doc/", "/word/", ".docx"]
                    ):
                        filename += ".docx"
                    elif any(
                        ext in path.lower()
                        for ext in ["/ppt/", "/powerpoint/", ".pptx"]
                    ):
                        filename += ".pptx"
                    elif any(ext in path.lower() for ext in ["/csv/", ".csv"]):
                        filename += ".csv"
                    elif any(ext in path.lower() for ext in ["/zip/", ".zip"]):
                        filename += ".zip"
                    else:
                        filename += ".html"

        return filename


class PathExtractor:
    """è·¯å¾„æå–å™¨"""

    @staticmethod
    def extract_target_path(text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå–ç›®æ ‡è·¯å¾„"""
        patterns = [
            r'(?:save|download|store|put|place|write|copy|move)\s+(?:to|into|in|at)\s+["\']?([^\s"\']+)["\']?',
            r'(?:to|into|in|at)\s+(?:folder|directory|dir|path|location)\s*["\']?([^\s"\']+)["\']?',
            r'(?:destination|target|output)\s*(?:is|:)?\s*["\']?([^\s"\']+)["\']?',
            r'(?:ä¿å­˜|ä¸‹è½½|å­˜å‚¨|æ”¾åˆ°|å†™å…¥|å¤åˆ¶|ç§»åŠ¨)(?:åˆ°|è‡³|å»)\s*["\']?([^\s"\']+)["\']?',
            r'(?:åˆ°|åœ¨|è‡³)\s*["\']?([^\s"\']+)["\']?\s*(?:æ–‡ä»¶å¤¹|ç›®å½•|è·¯å¾„|ä½ç½®)',
        ]

        filter_words = {
            "here",
            "there",
            "current",
            "local",
            "this",
            "that",
            "è¿™é‡Œ",
            "é‚£é‡Œ",
            "å½“å‰",
            "æœ¬åœ°",
            "è¿™ä¸ª",
            "é‚£ä¸ª",
        }

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                path = match.group(1).strip("ã€‚ï¼Œ,.ã€")
                if path and path.lower() not in filter_words:
                    return path

        return None


class SimplePdfConverter:
    """ç®€å•çš„PDFè½¬æ¢å™¨ï¼Œä½¿ç”¨PyPDF2æå–æ–‡æœ¬"""

    def convert_pdf_to_markdown(
        self, input_file: str, output_file: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨PyPDF2å°†PDFè½¬æ¢ä¸ºMarkdownæ ¼å¼

        Args:
            input_file: è¾“å…¥PDFæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºMarkdownæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            è½¬æ¢ç»“æœå­—å…¸
        """
        if not PYPDF2_AVAILABLE:
            return {"success": False, "error": "PyPDF2 package is not available"}

        try:
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(input_file):
                return {
                    "success": False,
                    "error": f"Input file not found: {input_file}",
                }

            # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆ
            if not output_file:
                base_name = os.path.splitext(input_file)[0]
                output_file = f"{base_name}.md"

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_file)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)

            # æ‰§è¡Œè½¬æ¢
            start_time = datetime.now()

            # è¯»å–PDFæ–‡ä»¶
            with open(input_file, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_content = []

                # æå–æ¯é¡µæ–‡æœ¬
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    text = page.extract_text()
                    if text.strip():
                        text_content.append(f"## Page {page_num}\n\n{text.strip()}\n\n")

            # ç”ŸæˆMarkdownå†…å®¹
            markdown_content = f"# Extracted from {os.path.basename(input_file)}\n\n"
            markdown_content += f"*Total pages: {len(pdf_reader.pages)}*\n\n"
            markdown_content += "---\n\n"
            markdown_content += "".join(text_content)

            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            # è®¡ç®—è½¬æ¢æ—¶é—´
            duration = (datetime.now() - start_time).total_seconds()

            # è·å–æ–‡ä»¶å¤§å°
            input_size = os.path.getsize(input_file)
            output_size = os.path.getsize(output_file)

            return {
                "success": True,
                "input_file": input_file,
                "output_file": output_file,
                "input_size": input_size,
                "output_size": output_size,
                "duration": duration,
                "markdown_content": markdown_content,
                "pages_extracted": len(pdf_reader.pages),
            }

        except Exception as e:
            return {
                "success": False,
                "input_file": input_file,
                "error": f"Conversion failed: {str(e)}",
            }


class DoclingConverter:
    """æ–‡æ¡£è½¬æ¢å™¨ï¼Œä½¿ç”¨doclingå°†æ–‡æ¡£è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œæ”¯æŒå›¾ç‰‡æå–"""

    def __init__(self):
        if not DOCLING_AVAILABLE:
            raise ImportError(
                "docling package is not available. Please install it first."
            )

        # é…ç½®PDFå¤„ç†é€‰é¡¹
        pdf_pipeline_options = PdfPipelineOptions()
        pdf_pipeline_options.do_ocr = False  # æš‚æ—¶ç¦ç”¨OCRä»¥é¿å…è®¤è¯é—®é¢˜
        pdf_pipeline_options.do_table_structure = False  # æš‚æ—¶ç¦ç”¨è¡¨æ ¼ç»“æ„è¯†åˆ«

        # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨ï¼ˆä½¿ç”¨åŸºç¡€æ¨¡å¼ï¼‰
        try:
            self.converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=pdf_pipeline_options
                    )
                }
            )
        except Exception:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ›´ç®€å•çš„é…ç½®
            self.converter = DocumentConverter()

    def is_supported_format(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒè½¬æ¢"""
        if not DOCLING_AVAILABLE:
            return False

        supported_extensions = {".pdf", ".docx", ".pptx", ".html", ".md", ".txt"}
        file_extension = os.path.splitext(file_path)[1].lower()
        return file_extension in supported_extensions

    def is_url(self, path: str) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºURL"""
        try:
            result = urlparse(path)
            return result.scheme in ("http", "https")
        except Exception:
            return False

    def extract_images(self, doc, output_dir: str) -> Dict[str, str]:
        """
        æå–æ–‡æ¡£ä¸­çš„å›¾ç‰‡å¹¶ä¿å­˜åˆ°æœ¬åœ°

        Args:
            doc: doclingæ–‡æ¡£å¯¹è±¡
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            å›¾ç‰‡IDåˆ°æœ¬åœ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        """
        images_dir = os.path.join(output_dir, "images")
        os.makedirs(images_dir, exist_ok=True)
        image_map = {}  # doclingå›¾ç‰‡id -> æœ¬åœ°æ–‡ä»¶å

        try:
            # è·å–æ–‡æ¡£ä¸­çš„å›¾ç‰‡
            images = getattr(doc, "images", [])

            for idx, img in enumerate(images):
                try:
                    # è·å–å›¾ç‰‡æ ¼å¼ï¼Œé»˜è®¤ä¸ºpng
                    ext = getattr(img, "format", None) or "png"
                    if ext.lower() not in ["png", "jpg", "jpeg", "gif", "bmp", "webp"]:
                        ext = "png"

                    # ç”Ÿæˆæ–‡ä»¶å
                    filename = f"image_{idx+1}.{ext}"
                    filepath = os.path.join(images_dir, filename)

                    # ä¿å­˜å›¾ç‰‡æ•°æ®
                    img_data = getattr(img, "data", None)
                    if img_data:
                        with open(filepath, "wb") as f:
                            f.write(img_data)

                        # è®¡ç®—ç›¸å¯¹è·¯å¾„
                        rel_path = os.path.relpath(filepath, output_dir)
                        img_id = getattr(img, "id", str(idx + 1))
                        image_map[img_id] = rel_path

                except Exception as img_error:
                    print(f"Warning: Failed to extract image {idx+1}: {img_error}")
                    continue

        except Exception as e:
            print(f"Warning: Failed to extract images: {e}")

        return image_map

    def process_markdown_with_images(
        self, markdown_content: str, image_map: Dict[str, str]
    ) -> str:
        """
        å¤„ç†Markdownå†…å®¹ï¼Œæ›¿æ¢å›¾ç‰‡å ä½ç¬¦ä¸ºå®é™…çš„å›¾ç‰‡è·¯å¾„

        Args:
            markdown_content: åŸå§‹Markdownå†…å®¹
            image_map: å›¾ç‰‡IDåˆ°æœ¬åœ°è·¯å¾„çš„æ˜ å°„

        Returns:
            å¤„ç†åçš„Markdownå†…å®¹
        """

        def replace_img(match):
            img_id = match.group(1)
            if img_id in image_map:
                return f"![Image]({image_map[img_id]})"
            else:
                return match.group(0)

        # æ›¿æ¢doclingçš„å›¾ç‰‡å ä½ç¬¦
        processed_content = re.sub(
            r"!\[Image\]\(docling://image/([^)]+)\)", replace_img, markdown_content
        )

        return processed_content

    def convert_to_markdown(
        self,
        input_file: str,
        output_file: Optional[str] = None,
        extract_images: bool = True,
    ) -> Dict[str, Any]:
        """
        å°†æ–‡æ¡£è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œæ”¯æŒå›¾ç‰‡æå–

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„æˆ–URL
            output_file: è¾“å‡ºMarkdownæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            extract_images: æ˜¯å¦æå–å›¾ç‰‡ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            è½¬æ¢ç»“æœå­—å…¸
        """
        if not DOCLING_AVAILABLE:
            return {"success": False, "error": "docling package is not available"}

        try:
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶ï¼ˆå¦‚æœä¸æ˜¯URLï¼‰
            if not self.is_url(input_file):
                if not os.path.exists(input_file):
                    return {
                        "success": False,
                        "error": f"Input file not found: {input_file}",
                    }

                # æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
                if not self.is_supported_format(input_file):
                    return {
                        "success": False,
                        "error": f"Unsupported file format: {os.path.splitext(input_file)[1]}",
                    }
            else:
                # å¯¹äºURLï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„æ ¼å¼
                if not input_file.lower().endswith(
                    (".pdf", ".docx", ".pptx", ".html", ".md", ".txt")
                ):
                    return {
                        "success": False,
                        "error": f"Unsupported URL format: {input_file}",
                    }

            # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆ
            if not output_file:
                if self.is_url(input_file):
                    # ä»URLç”Ÿæˆæ–‡ä»¶å
                    filename = URLExtractor.infer_filename_from_url(input_file)
                    base_name = os.path.splitext(filename)[0]
                else:
                    base_name = os.path.splitext(input_file)[0]
                output_file = f"{base_name}.md"

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_file) or "."
            os.makedirs(output_dir, exist_ok=True)

            # æ‰§è¡Œè½¬æ¢
            start_time = datetime.now()
            result = self.converter.convert(input_file)
            doc = result.document

            # æå–å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            image_map = {}
            images_extracted = 0
            if extract_images:
                image_map = self.extract_images(doc, output_dir)
                images_extracted = len(image_map)

            # è·å–Markdownå†…å®¹
            markdown_content = doc.export_to_markdown()

            # å¤„ç†å›¾ç‰‡å ä½ç¬¦
            if extract_images and image_map:
                markdown_content = self.process_markdown_with_images(
                    markdown_content, image_map
                )

            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            # è®¡ç®—è½¬æ¢æ—¶é—´
            duration = (datetime.now() - start_time).total_seconds()

            # è·å–æ–‡ä»¶å¤§å°
            if self.is_url(input_file):
                input_size = 0  # URLæ— æ³•ç›´æ¥è·å–å¤§å°
            else:
                input_size = os.path.getsize(input_file)
            output_size = os.path.getsize(output_file)

            return {
                "success": True,
                "input_file": input_file,
                "output_file": output_file,
                "input_size": input_size,
                "output_size": output_size,
                "duration": duration,
                "markdown_content": markdown_content,
                "images_extracted": images_extracted,
                "image_map": image_map,
            }

        except Exception as e:
            return {
                "success": False,
                "input_file": input_file,
                "error": f"Conversion failed: {str(e)}",
            }


async def check_url_accessible(url: str) -> Dict[str, Any]:
    """æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®"""
    try:
        timeout = aiohttp.ClientTimeout(total=10)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.head(url, allow_redirects=True) as response:
                return {
                    "accessible": response.status < 400,
                    "status": response.status,
                    "content_type": response.headers.get("Content-Type", ""),
                    "content_length": response.headers.get("Content-Length", 0),
                }
    except Exception:
        return {
            "accessible": False,
            "status": 0,
            "content_type": "",
            "content_length": 0,
        }


async def download_file(url: str, destination: str) -> Dict[str, Any]:
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    start_time = datetime.now()
    chunk_size = 8192

    try:
        timeout = aiohttp.ClientTimeout(total=300)  # 5åˆ†é’Ÿè¶…æ—¶
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url) as response:
                # æ£€æŸ¥å“åº”çŠ¶æ€
                response.raise_for_status()

                # è·å–æ–‡ä»¶ä¿¡æ¯
                content_type = response.headers.get(
                    "Content-Type", "application/octet-stream"
                )

                # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                parent_dir = os.path.dirname(destination)
                if parent_dir:
                    os.makedirs(parent_dir, exist_ok=True)

                # ä¸‹è½½æ–‡ä»¶
                downloaded = 0
                async with aiofiles.open(destination, "wb") as file:
                    async for chunk in response.content.iter_chunked(chunk_size):
                        await file.write(chunk)
                        downloaded += len(chunk)

                # è®¡ç®—ä¸‹è½½æ—¶é—´
                duration = (datetime.now() - start_time).total_seconds()

                return {
                    "success": True,
                    "url": url,
                    "destination": destination,
                    "size": downloaded,
                    "content_type": content_type,
                    "duration": duration,
                    "speed": downloaded / duration if duration > 0 else 0,
                }

    except aiohttp.ClientError as e:
        return {
            "success": False,
            "url": url,
            "destination": destination,
            "error": f"Network error: {str(e)}",
        }
    except Exception as e:
        return {
            "success": False,
            "url": url,
            "destination": destination,
            "error": f"Download error: {str(e)}",
        }


async def move_local_file(source_path: str, destination: str) -> Dict[str, Any]:
    """å¤åˆ¶æœ¬åœ°æ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼‰"""
    start_time = datetime.now()

    try:
        # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(source_path):
            return {
                "success": False,
                "source": source_path,
                "destination": destination,
                "error": f"Source file not found: {source_path}",
            }

        # è·å–æºæ–‡ä»¶ä¿¡æ¯
        source_size = os.path.getsize(source_path)

        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        parent_dir = os.path.dirname(destination)
        if parent_dir:
            os.makedirs(parent_dir, exist_ok=True)

        # æ‰§è¡Œå¤åˆ¶æ“ä½œï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰
        shutil.copy2(source_path, destination)

        # è®¡ç®—æ“ä½œæ—¶é—´
        duration = (datetime.now() - start_time).total_seconds()

        return {
            "success": True,
            "source": source_path,
            "destination": destination,
            "size": source_size,
            "duration": duration,
            "operation": "copy",  # æ”¹ä¸ºcopy
        }

    except Exception as e:
        return {
            "success": False,
            "source": source_path,
            "destination": destination,
            "error": f"Copy error: {str(e)}",
        }


@mcp.tool()
async def download_files(instruction: str) -> str:
    """
    Download files from URLs or move local files mentioned in natural language instructions.

    Args:
        instruction: Natural language instruction containing URLs/local paths and optional destination paths

    Returns:
        Status message about the download/move operations

    Examples:
        - "Download https://example.com/file.pdf to documents folder"
        - "Move /home/user/file.pdf to documents folder"
        - "Please get https://raw.githubusercontent.com/user/repo/main/data.csv and save it to ~/downloads"
        - "ç§»åŠ¨ ~/Desktop/report.docx åˆ° /tmp/documents/"
        - "Download www.example.com/report.xlsx"
    """
    urls = URLExtractor.extract_urls(instruction)
    local_paths = LocalPathExtractor.extract_local_paths(instruction)

    if not urls and not local_paths:
        return format_error_message(
            "Failed to parse instruction",
            "No downloadable URLs or movable local files found",
        )

    target_path = PathExtractor.extract_target_path(instruction)

    # å¤„ç†æ–‡ä»¶
    results = []

    # å¤„ç†URLä¸‹è½½
    for url in urls:
        try:
            # æ¨æ–­æ–‡ä»¶å
            filename = URLExtractor.infer_filename_from_url(url)

            # æ„å»ºå®Œæ•´çš„ç›®æ ‡è·¯å¾„
            if target_path:
                # å¤„ç†è·¯å¾„
                if target_path.startswith("~"):
                    target_path = os.path.expanduser(target_path)

                # ç¡®ä¿ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚æœä¸æ˜¯ç»å¯¹è·¯å¾„ï¼‰
                if not os.path.isabs(target_path):
                    target_path = os.path.normpath(target_path)

                # åˆ¤æ–­æ˜¯æ–‡ä»¶è·¯å¾„è¿˜æ˜¯ç›®å½•è·¯å¾„
                if os.path.splitext(target_path)[1]:  # æœ‰æ‰©å±•åï¼Œæ˜¯æ–‡ä»¶
                    destination = target_path
                else:  # æ˜¯ç›®å½•
                    destination = os.path.join(target_path, filename)
            else:
                # é»˜è®¤ä¸‹è½½åˆ°å½“å‰ç›®å½•
                destination = filename

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(destination):
                results.append(
                    f"[WARNING] Skipped {url}: File already exists at {destination}"
                )
                continue

            # å…ˆæ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®
            check_result = await check_url_accessible(url)
            if not check_result["accessible"]:
                results.append(
                    f"[ERROR] Failed to access {url}: HTTP {check_result['status'] or 'Connection failed'}"
                )
                continue

            # æ‰§è¡Œä¸‹è½½
            result = await download_file(url, destination)

            # æ‰§è¡Œè½¬æ¢ï¼ˆå¦‚æœæˆåŠŸä¸‹è½½ï¼‰
            conversion_msg = None
            if result["success"]:
                conversion_msg = await perform_document_conversion(
                    destination, extract_images=True
                )

            # æ ¼å¼åŒ–ç»“æœ
            msg = format_file_operation_result(
                "download", url, destination, result, conversion_msg
            )

        except Exception as e:
            msg = f"[ERROR] Failed to download: {url}\n"
            msg += f"   Error: {str(e)}"

        results.append(msg)

    # å¤„ç†æœ¬åœ°æ–‡ä»¶ç§»åŠ¨
    for local_path in local_paths:
        try:
            # è·å–æ–‡ä»¶å
            filename = os.path.basename(local_path)

            # æ„å»ºå®Œæ•´çš„ç›®æ ‡è·¯å¾„
            if target_path:
                # å¤„ç†è·¯å¾„
                if target_path.startswith("~"):
                    target_path = os.path.expanduser(target_path)

                # ç¡®ä¿ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚æœä¸æ˜¯ç»å¯¹è·¯å¾„ï¼‰
                if not os.path.isabs(target_path):
                    target_path = os.path.normpath(target_path)

                # åˆ¤æ–­æ˜¯æ–‡ä»¶è·¯å¾„è¿˜æ˜¯ç›®å½•è·¯å¾„
                if os.path.splitext(target_path)[1]:  # æœ‰æ‰©å±•åï¼Œæ˜¯æ–‡ä»¶
                    destination = target_path
                else:  # æ˜¯ç›®å½•
                    destination = os.path.join(target_path, filename)
            else:
                # é»˜è®¤ç§»åŠ¨åˆ°å½“å‰ç›®å½•
                destination = filename

            # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(destination):
                results.append(
                    f"[WARNING] Skipped {local_path}: File already exists at {destination}"
                )
                continue

            # æ‰§è¡Œå¤åˆ¶ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼‰
            result = await move_local_file(local_path, destination)

            # æ‰§è¡Œè½¬æ¢ï¼ˆå¦‚æœæˆåŠŸå¤åˆ¶ï¼‰
            conversion_msg = None
            if result["success"]:
                conversion_msg = await perform_document_conversion(
                    destination, extract_images=True
                )

            # æ ¼å¼åŒ–ç»“æœ
            msg = format_file_operation_result(
                "copy", local_path, destination, result, conversion_msg
            )

        except Exception as e:
            msg = f"[ERROR] Failed to copy: {local_path}\n"
            msg += f"   Error: {str(e)}"

        results.append(msg)

    return "\n\n".join(results)


@mcp.tool()
async def parse_download_urls(text: str) -> str:
    """
    Extract URLs, local paths and target paths from text without downloading or moving.

    Args:
        text: Text containing URLs, local paths and optional destination paths

    Returns:
        Parsed URLs, local paths and target path information
    """
    urls = URLExtractor.extract_urls(text)
    local_paths = LocalPathExtractor.extract_local_paths(text)
    target_path = PathExtractor.extract_target_path(text)

    content = "ğŸ“‹ Parsed file operation information:\n\n"

    if urls:
        content += f"ğŸ”— URLs found ({len(urls)}):\n"
        for i, url in enumerate(urls, 1):
            filename = URLExtractor.infer_filename_from_url(url)
            content += f"  {i}. {url}\n     ğŸ“„ Filename: {filename}\n"
    else:
        content += "ğŸ”— No URLs found\n"

    if local_paths:
        content += f"\nğŸ“ Local files found ({len(local_paths)}):\n"
        for i, path in enumerate(local_paths, 1):
            exists = os.path.exists(path)
            content += f"  {i}. {path}\n"
            content += f"     âœ… Exists: {'Yes' if exists else 'No'}\n"
            if exists:
                size_mb = os.path.getsize(path) / (1024 * 1024)
                content += f"     ğŸ“Š Size: {size_mb:.2f} MB\n"
    else:
        content += "\nğŸ“ No local files found\n"

    if target_path:
        content += f"\nğŸ¯ Target path: {target_path}"
        if target_path.startswith("~"):
            content += f"\n   (Expanded: {os.path.expanduser(target_path)})"
    else:
        content += "\nğŸ¯ Target path: Not specified (will use current directory)"

    return content


@mcp.tool()
async def download_file_to(
    url: str, destination: Optional[str] = None, filename: Optional[str] = None
) -> str:
    """
    Download a specific file with detailed options.

    Args:
        url: URL to download from
        destination: Target directory or full file path (optional)
        filename: Specific filename to use (optional, ignored if destination is a full file path)

    Returns:
        Status message about the download operation
    """
    # ç¡®å®šæ–‡ä»¶å

    url = URLExtractor.extract_urls(url)[0]

    if not filename:
        filename = URLExtractor.infer_filename_from_url(url)

    if not filename:
        filename = URLExtractor.infer_filename_from_url(url)
    else:
        name_source, extension_source = os.path.splitext(
            os.path.basename(URLExtractor.infer_filename_from_url(url))
        )
        name_destination, extension_destination = os.path.splitext(
            os.path.basename(filename)
        )
        if extension_source:
            filename = name_destination + extension_source
        else:
            filename = name_destination + extension_destination

    # ç¡®å®šå®Œæ•´è·¯å¾„
    if destination:
        # å±•å¼€ç”¨æˆ·ç›®å½•
        if destination.startswith("~"):
            destination = os.path.expanduser(destination)

        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´æ–‡ä»¶è·¯å¾„
        if os.path.splitext(destination)[1]:  # æœ‰æ‰©å±•å
            target_path = destination
        else:  # æ˜¯ç›®å½•
            target_path = os.path.join(destination, filename)
    else:
        target_path = filename

    # ç¡®ä¿ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚æœä¸æ˜¯ç»å¯¹è·¯å¾„ï¼‰
    if not os.path.isabs(target_path):
        target_path = os.path.normpath(target_path)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(target_path):
        return format_error_message(
            "Download aborted", f"File already exists at {target_path}"
        )

    # å…ˆæ£€æŸ¥URL
    check_result = await check_url_accessible(url)
    if not check_result["accessible"]:
        return format_error_message(
            "Cannot access URL",
            f"{url} (HTTP {check_result['status'] or 'Connection failed'})",
        )

    # æ˜¾ç¤ºä¸‹è½½ä¿¡æ¯
    size_mb = (
        int(check_result["content_length"]) / (1024 * 1024)
        if check_result["content_length"]
        else 0
    )
    msg = "[INFO] Downloading file:\n"
    msg += f"   URL: {url}\n"
    msg += f"   Target: {target_path}\n"
    if size_mb > 0:
        msg += f"   Expected size: {size_mb:.2f} MB\n"
    msg += "\n"

    # æ‰§è¡Œä¸‹è½½
    result = await download_file(url, target_path)

    # æ‰§è¡Œè½¬æ¢ï¼ˆå¦‚æœæˆåŠŸä¸‹è½½ï¼‰
    conversion_msg = None
    if result["success"]:
        conversion_msg = await perform_document_conversion(
            target_path, extract_images=True
        )

        # æ·»åŠ ä¸‹è½½ä¿¡æ¯å‰ç¼€
        actual_size_mb = result["size"] / (1024 * 1024)
        speed_mb = result["speed"] / (1024 * 1024)
        info_msg = "[SUCCESS] Download completed!\n"
        info_msg += f"   Saved to: {target_path}\n"
        info_msg += f"   Size: {actual_size_mb:.2f} MB\n"
        info_msg += f"   Duration: {result['duration']:.2f} seconds\n"
        info_msg += f"   Speed: {speed_mb:.2f} MB/s\n"
        info_msg += f"   Type: {result['content_type']}"

        if conversion_msg:
            info_msg += conversion_msg

        return msg + info_msg
    else:
        return msg + f"[ERROR] Download failed!\n   Error: {result['error']}"


@mcp.tool()
async def move_file_to(
    source: str, destination: Optional[str] = None, filename: Optional[str] = None
) -> str:
    """
    Copy a local file to a new location (preserves original file).

    Note: Despite the name "move_file_to", this tool COPIES the file to preserve the original.
    This prevents data loss during file processing workflows.

    Args:
        source: Source file path to copy
        destination: Target directory or full file path (optional)
        filename: Specific filename to use (optional, ignored if destination is a full file path)

    Returns:
        Status message about the copy operation
    """
    # å±•å¼€æºè·¯å¾„
    if source.startswith("~"):
        source = os.path.expanduser(source)

    # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(source):
        return format_error_message("Copy aborted", f"Source file not found: {source}")

    # ç¡®å®šæ–‡ä»¶å
    if not filename:
        filename = os.path.basename(source)
    else:
        name_source, extension_source = os.path.splitext(os.path.basename(source))
        name_destination, extension_destination = os.path.splitext(
            os.path.basename(filename)
        )
        if extension_source:
            filename = name_destination + extension_source
        else:
            filename = name_destination + extension_destination

    # ç¡®å®šå®Œæ•´è·¯å¾„
    if destination:
        # å±•å¼€ç”¨æˆ·ç›®å½•
        if destination.startswith("~"):
            destination = os.path.expanduser(destination)

        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´æ–‡ä»¶è·¯å¾„
        if os.path.splitext(destination)[1]:  # æœ‰æ‰©å±•å
            target_path = destination
        else:  # æ˜¯ç›®å½•
            target_path = os.path.join(destination, filename)

    else:
        target_path = filename

    # ç¡®ä¿ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚æœä¸æ˜¯ç»å¯¹è·¯å¾„ï¼‰
    if not os.path.isabs(target_path):
        target_path = os.path.normpath(target_path)

    # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(target_path):
        return f"[ERROR] Target file already exists: {target_path}"

    # æ˜¾ç¤ºå¤åˆ¶ä¿¡æ¯
    source_size_mb = os.path.getsize(source) / (1024 * 1024)
    msg = "[INFO] Copying file (original preserved):\n"
    msg += f"   Source: {source}\n"
    msg += f"   Target: {target_path}\n"
    msg += f"   Size: {source_size_mb:.2f} MB\n"
    msg += "\n"

    # æ‰§è¡Œå¤åˆ¶ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼‰
    result = await move_local_file(source, target_path)

    # æ‰§è¡Œè½¬æ¢ï¼ˆå¦‚æœæˆåŠŸå¤åˆ¶ï¼‰
    conversion_msg = None
    if result["success"]:
        conversion_msg = await perform_document_conversion(
            target_path, extract_images=True
        )

        # æ·»åŠ å¤åˆ¶ä¿¡æ¯å‰ç¼€
        info_msg = "[SUCCESS] File copied successfully (original preserved)!\n"
        info_msg += f"   From: {source}\n"
        info_msg += f"   To: {target_path}\n"
        info_msg += f"   Duration: {result['duration']:.2f} seconds"

        if conversion_msg:
            info_msg += conversion_msg

        return msg + info_msg
    else:
        return msg + f"[ERROR] Copy failed!\n   Error: {result['error']}"


# @mcp.tool()
# async def convert_document_to_markdown(
#     file_path: str, output_path: Optional[str] = None, extract_images: bool = True
# ) -> str:
#     """
#     Convert a document to Markdown format with image extraction support.

#     Supports both local files and URLs. Uses docling for advanced conversion with image extraction,
#     or falls back to PyPDF2 for simple PDF text extraction.

#     Args:
#         file_path: Path to the input document file or URL (supports PDF, DOCX, PPTX, HTML, TXT, MD)
#         output_path: Path for the output Markdown file (optional, auto-generated if not provided)
#         extract_images: Whether to extract images from the document (default: True)

#     Returns:
#         Status message about the conversion operation with preview of converted content

#     Examples:
#         - "convert_document_to_markdown('paper.pdf')"
#         - "convert_document_to_markdown('https://example.com/doc.pdf', 'output.md')"
#         - "convert_document_to_markdown('presentation.pptx', extract_images=False)"
#     """
#     # æ£€æŸ¥æ˜¯å¦ä¸ºURL
#     is_url_input = False
#     try:
#         parsed = urlparse(file_path)
#         is_url_input = parsed.scheme in ("http", "https")
#     except Exception:
#         is_url_input = False

#     # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœä¸æ˜¯URLï¼‰
#     if not is_url_input and not os.path.exists(file_path):
#         return f"[ERROR] Input file not found: {file_path}"

#     # æ£€æŸ¥æ˜¯å¦æ˜¯PDFæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ç®€å•è½¬æ¢å™¨ï¼ˆä»…å¯¹æœ¬åœ°æ–‡ä»¶ï¼‰
#     if (
#         not is_url_input
#         and file_path.lower().endswith(".pdf")
#         and PYPDF2_AVAILABLE
#         and not extract_images
#     ):
#         try:
#             simple_converter = SimplePdfConverter()
#             result = simple_converter.convert_pdf_to_markdown(file_path, output_path)
#         except Exception as e:
#             return f"[ERROR] PDF conversion error: {str(e)}"
#     elif DOCLING_AVAILABLE:
#         try:
#             converter = DoclingConverter()

#             # æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
#             if not is_url_input and not converter.is_supported_format(file_path):
#                 supported_formats = [".pdf", ".docx", ".pptx", ".html", ".md", ".txt"]
#                 return f"[ERROR] Unsupported file format. Supported formats: {', '.join(supported_formats)}"
#             elif is_url_input and not file_path.lower().endswith(
#                 (".pdf", ".docx", ".pptx", ".html", ".md", ".txt")
#             ):
#                 return f"[ERROR] Unsupported URL format: {file_path}"

#             # æ‰§è¡Œè½¬æ¢ï¼ˆæ”¯æŒå›¾ç‰‡æå–ï¼‰
#             result = converter.convert_to_markdown(
#                 file_path, output_path, extract_images
#             )
#         except Exception as e:
#             return f"[ERROR] Docling conversion error: {str(e)}"
#     else:
#         return (
#             "[ERROR] No conversion tools available. Please install docling or PyPDF2."
#         )

#     if result["success"]:
#         msg = "[SUCCESS] Document converted successfully!\n"
#         msg += f"   Input: {result['input_file']}\n"
#         msg += f"   Output file: {result['output_file']}\n"
#         msg += f"   Conversion time: {result['duration']:.2f} seconds\n"

#         if result["input_size"] > 0:
#             msg += f"   Original size: {result['input_size'] / 1024:.1f} KB\n"
#         msg += f"   Markdown size: {result['output_size'] / 1024:.1f} KB\n"

#         # æ˜¾ç¤ºå›¾ç‰‡æå–ä¿¡æ¯
#         if extract_images and "images_extracted" in result:
#             images_count = result["images_extracted"]
#             if images_count > 0:
#                 msg += f"   Images extracted: {images_count}\n"
#                 msg += f"   Images saved to: {os.path.join(os.path.dirname(result['output_file']), 'images')}\n"
#             else:
#                 msg += "   No images found in document\n"

#         # æ˜¾ç¤ºMarkdownå†…å®¹çš„å‰å‡ è¡Œä½œä¸ºé¢„è§ˆ
#         content_lines = result["markdown_content"].split("\n")
#         preview_lines = content_lines[:5]
#         if len(content_lines) > 5:
#             preview_lines.append("...")

#         msg += "\n[PREVIEW] First few lines of converted Markdown:\n"
#         for line in preview_lines:
#             msg += f"   {line}\n"
#     else:
#         msg = "[ERROR] Conversion failed!\n"
#         msg += f"   Error: {result['error']}"

#     return msg


if __name__ == "__main__":
    print("ğŸ“„ Smart PDF Downloader MCP Tool")
    print("ğŸ“ Starting server with FastMCP...")

    if DOCLING_AVAILABLE:
        print("âœ… Document conversion to Markdown is ENABLED (docling available)")
    else:
        print("âŒ Document conversion to Markdown is DISABLED (docling not available)")
        print("   Install docling to enable: pip install docling")

    print("\nAvailable tools:")
    print(
        "  â€¢ download_files - Download files or move local files from natural language"
    )
    print("  â€¢ parse_download_urls - Extract URLs, local paths and destination paths")
    print("  â€¢ download_file_to - Download a specific file with options")
    print("  â€¢ move_file_to - Move a specific local file with options")
    print("  â€¢ convert_document_to_markdown - Convert documents to Markdown format")

    if DOCLING_AVAILABLE:
        print("\nSupported formats: PDF, DOCX, PPTX, HTML, TXT, MD")
        print("Features: Image extraction, Layout preservation, Automatic conversion")

    print("")

    # è¿è¡ŒæœåŠ¡å™¨
    mcp.run()
