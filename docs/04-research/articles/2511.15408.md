# NAMeGEn: Agent-based Multi-Objective Optimization

**Source:** [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/html/2511.15408)

## Synthesis
This paper addresses the challenge of **Creative Natural Language Generation (CNLG)** where agents must satisfy conflicting, fine-grained **Explicit User-specified Objectives (EUOs)** (e.g., "must be culturally significant", "must match these parents' expectations") while maintaining **Implicit Interpretive Objectives (IIOs)** (e.g., factuality, clarity, completeness).

The authors propose **NAMeGEn**, a multi-agent framework consisting of three specialized agents:
1.  **Multi-Objective Manager (MOM):** Analyzes the task, extracts key information, and performs *verified retrieval*. It actively rewrites queries to match the style of the knowledge base and verifies retrieved content before passing it downstream.
2.  **Multi-Objective Generator (MOG):** Generates content based on the "Hybrid Information Set" (User input + Retrieved Knowledge + Explicit requirements).
3.  **Multi-Objective Evaluator (MOE):** Scores the output against both EUOs and IIOs.

A key innovation is the **Dynamic Iterative Hybrid Multi-Objective Optimization (DI-HMOO)** algorithm. Instead of static acceptance criteria, it uses **dynamic thresholds** that start high (strict quality control) and decay logarithmically over iterations. This ensures the system strives for perfection but guarantees convergence to a "best possible" solution within a time limit, preventing infinite optimization loops.

## Core Strategic Ideas
1.  **Dynamic Thresholding for Convergence:** Autonomous agents often get stuck in loops trying to fix minor linting errors or edge cases. NAMeGEn's formula for dynamic thresholds ($\psi = \frac{\delta}{\alpha \cdot \log(j + t_w)}$) allows strict initial standards that relax over time, ensuring the agent eventually outputs a result.
2.  **Explicit vs. Implicit Objective Separation:** Explicit goals are user-defined (Project Specs). Implicit goals are system-defined (Security, Performance, Style). Treating them as separate vectors allows the Evaluator to weight them differently (e.g., Implicit Security might never relax, while Explicit Style might).
3.  **Verified Retrieval (RAG-Verification):** The MOM agent doesn't just retrieve chunks; it passes them to the Evaluator *before* generation. If the Evaluator deems the chunks irrelevant, the MOM rewrites the query. This prevents "Garbage-In-Garbage-Out" in RAG.

## Integration Plan (Agno + SurrealDB + Gemini 3)
We can adapt the NAMeGEn architecture for a **"Quality-Assured Coding Pipeline"**:

1.  **Architecture (`codeswarm/patterns/namegen_loop.py`):**
    *   **Manager Agent (MOM):** Uses `Agno` to parse the `feature_request`. It queries `SurrealDB` for existing code context.
    *   **Pre-Generation Verification:** An `Evaluator` checks if the context retrieved from `SurrealDB` is actually relevant to the request. If not, MOM refines the search query.
    *   **Generator Agent (MOG):** Uses `Gemini 3` to write code.
    *   **Evaluator Agent (MOE):** Runs tests and linters. It produces two scores: `S_explicit` (Does it pass user tests?) and `S_implicit` (Is the code clean/secure?).

2.  **Implementation of Dynamic Thresholds:**
    *   Define a helper class `DynamicThreshold` in `codeswarm/utils/math.py`.
    *   In the main loop, check: `if score < threshold(iteration): retry() else: submit()`.

3.  **SurrealDB Schema:**
    *   Store optimization traces: `CREATE interaction SET scores = [...], threshold_snapshot = [...]`. This allows analyzing how often agents "settle" for lower quality vs. achieving high quality.

4.  **Prompt Engineering:**
    *   Use the paper's strategy of decomposing "User Input" into `I_desc` (Detailed Description) and `I_reqs` (Explanatory Requirements) before generation.
