# Arxiv Analysis: Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities

**ID:** 2511.14631
**Date:** 2025-11-18
**Link:** https://arxiv.org/html/2511.14631

## 1. Executive Summary
This paper, from the **cmbagent** team, introduces a VLM-driven feedback loop for autonomous scientific discovery. It treats **plots as verifiable checkpoints**. A **VLM-as-a-judge** evaluates generated figures against dynamically created domain-specific rubrics. If a plot fails the rubric, a **Plot Debugger** agent fixes the code. This approach recovers from errors where text-only agents fail (pass@1 improves from 0.2 to 0.8).

## 2. Detailed Key Concepts

### 2.1. VLM-as-a-Judge Workflow
1.  **Generation:** Engineering agents generate code and produce a plot (checkpoint).
2.  **Rubric Generation:** A text LLM (GPT-4o) generates a *dynamic rubric* based on the scientific goal (e.g., "Peak should be at x=200").
3.  **Visual Evaluation:** A VLM (Plot Judge) compares the plot image against the rubric.
4.  **Verdict:** "Continue" or "Retry".
5.  **Debugging:** If "Retry", the **Plot Debugger** agent receives the VLM analysis and the code, and proposes specific fixes (line numbers or diffs).

### 2.2. Specialized Agents
-   **Plot Judge:** Evaluates "Scientific Accuracy" (dynamic) and "Visual Clarity" (static).
-   **Plot Debugger:** Traces visual errors back to code root causes.
-   **Experiment Proposer:** (In Discovery Mode) Proposes new experiments if the VLM detects anomalies (e.g., "Unexpected dip in spectrum").

### 2.3. Discovery vs. Correction
-   **Correction Mode:** Fix errors until rubric is met.
-   **Discovery Mode:** Treat deviations as "Signals" to investigate. The VLM flags anomalies, triggering an "Explore" branch where new hypotheses are tested.

## 3. Gap Analysis

| Feature/Concept | Current CodeSwarm | Gap |
| :--- | :--- | :--- |
| **Visual Verification** | None | **Critical:** We blindly trust code output. We don't check if the *result* (UI, plot) looks right. |
| **Artifact Checkpoints** | Code Review only | **High:** We don't treat generated files (images, PDFs) as checkpoints for verification. |
| **Dynamic Rubrics** | Fixed instructions | **Medium:** We don't generate specific verification criteria *per task*. |

## 4. Implementation Plan (Agno + SurrealDB + Gemini)

### Phase 1: Visual Artifact Generation
- **Action:** Ensure DevAgent tasks that involve UI or Data Viz produce an image artifact (Screenshot via Playwright or `.png` from Matplotlib).
- **Storage:** Store artifact path in SurrealDB `task_outputs`.

### Phase 2: Visual Revisor Agent (VLM Judge)
- **Action:** Create `VisualRevisorAgent`.
- **Logic:**
    1.  **Rubric Gen:** Ask Gemini (Text): "Given task X, what visual features must be present in the screenshot/plot? Output a checklist."
    2.  **Visual Check:** Ask Gemini (Vision): "Look at this image. Check it against this checklist. Output PASS/FAIL and reasoning."
- **Integration:** If FAIL, pass the visual reasoning back to DevAgent as feedback.

### Phase 3: The "Plot Debugger" Pattern
- **Action:** Create a specialized prompts for `DevAgent` when fixing visual bugs.
- **Prompt:** "The Visual Judge found these issues: [Issues]. Here is the code. Fix the code to address these visual discrepancies."

### Phase 4: Prompts from Appendix B
- **Adaptation:** Adapt the specific prompts from the paper (e.g., "You are a plot judge...", "You are a plot debugging expert...") for CodeSwarm's context (e.g., "You are a UI judge...").
