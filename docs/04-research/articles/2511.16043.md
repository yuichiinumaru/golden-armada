# Agent0: Zero-Data Self-Evolution

**Source:** [Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning](https://arxiv.org/html/2511.16043)

## Synthesis
This paper introduces **Agent0**, a framework for training LLM agents without *any* human data. It relies on a co-evolutionary game between two agents:
1.  **Curriculum Agent:** Generates tasks. Its goal is to create tasks that are *challenging but solvable* (at the frontier of the Executor's capability). It is rewarded based on the Executor's **Uncertainty** (high disagreement in self-consistency samples) and **Tool Use** (forcing the executor to use tools).
2.  **Executor Agent:** Solves tasks. It uses **Reinforcement Learning (RL)** with a "Stop-and-Go" rollout: it thinks, pauses to execute a tool (Python), gets the result, and continues.

The key innovation is the **Virtuous Cycle**:
*   Executor gets better at using tools -> Solves harder problems.
*   Curriculum Agent sees this -> Must generate *even harder* problems (multi-step, tool-heavy) to maintain high uncertainty rewards.
*   Result: Qwen3-8B improved by 18% on Math and 24% on General Reasoning, surpassing models trained on human data.

## Core Strategic Ideas
1.  **Curriculum as an Agent:** Instead of a static dataset, use an agent to generate the training data. This agent's goal is explicitly to find the "weak spots" of the solver agent.
2.  **Uncertainty as Reward:** A task is "good" for training if the solver is uncertain (e.g., 50/50 pass rate). If the solver always passes (too easy) or always fails (too hard), the task is useless.
3.  **Tool-Centric Evolution:** The paper proves that merely giving a model tools isn't enough. You must *force* it to use them by generating tasks that *cannot* be solved without tools (e.g., calculation heavy, simulation based).

## Integration Plan (Agno + SurrealDB + Gemini 3)
We can implement a "Self-Training" loop for our CodeSwarm agents:

1.  **The Gym:**
    *   Create a `codeswarm/gym` module.
    *   **Trainer Agent:** Generates "Bug Reports" or "Feature Requests".
    *   **Solver Agent:** Tries to write code to satisfy them.
    *   **Judge Agent (Gemini 3):** Runs the code/tests and awards a score.

2.  **SurrealDB for Experience Replay:**
    *   Store every `{task, solution, score}` tuple in `SurrealDB`.
    *   Use this data to fine-tune prompts or (eventually) fine-tune a small specialized model.

3.  **Ambiguity-Dynamic Policy (Simplified):**
    *   We don't need full RL optimization yet. We can use a simpler heuristic:
    *   If the Solver Agent solves a task easily (100% success on first try), the Trainer Agent is prompted: "That was too easy. Generate a harder version of this task."
    *   If the Solver fails completely, the Trainer is prompted: "That was too hard. Simplify the task."
