{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "587f29a5",
   "metadata": {},
   "source": [
    "## Better RAG with HyDE: Hypothetical Document Embeddings\n",
    "\n",
    "## [Checkout the detailed tutorial on my Newsletter](https://www.rohan-paul.com/p/better-rag-with-hyde-hypothetical)\n",
    "\n",
    "#### ü§ñ Basic Concept\n",
    "\n",
    "HyDE encodes queries into vector form by first generating a hypothetical text via an instruction-based LLM. That text is then fed into a contrastive encoder for embedding. This indirect approach captures semantic relevance without needing labeled data.\n",
    "\n",
    "![](assets/2025-03-30-22-04-44.png)\n",
    "\n",
    "The key core concept here is externalization of configuration‚Äîthat is, separating environment-specific settings (like your API endpoint) from your application logic. In our refactored code, instead of hardcoding the URL, we retrieve it from an environment variable (or build it dynamically), which makes the code more flexible, secure, and easier to maintain.\n",
    "\n",
    "#### ‚öôÔ∏è Two-Step Mechanism\n",
    "\n",
    "1. **Hypothetical Document Generation**  \n",
    "An LLM receives a query and responds with an artificial passage that attempts to address the query. The output may contain inaccurate details, but it still encodes topic-related patterns.\n",
    "\n",
    "2. **Contrastive Embedding**  \n",
    "A contrastive encoder transforms the generated text into a dense vector. This encoding filters out fabricated elements and focuses on the core meaning. Final retrieval is done by matching this vector against the real corpus vectors.\n",
    "\n",
    "#### üö© Practical Advantages\n",
    "\n",
    "- Straightforward to deploy: No explicit relevance training data is needed.  \n",
    "- Flexible: Works across different domains and languages with minimal adjustments.  \n",
    "- Solid performance: Often rivals or surpasses unsupervised approaches (like Contriever alone) and can approach fine-tuned retrievers.\n",
    "\n",
    "#### üåü Key Highlights\n",
    "\n",
    "‚Üí Uses an LLM to propose a synthetic passage as a starting point.  \n",
    "‚Üí Contrastive embeddings focus on essential details, discarding hallucinated content.  \n",
    "‚Üí Matches synthetic vectors to real documents, bridging query and corpus.  \n",
    "‚Üí No domain-specific labeled data required.  \n",
    "‚Üí Outperforms many zero-shot dense retrieval baselines.\n",
    "\n",
    "#### üß≤ Five Hooking Summaries\n",
    "\n",
    "1. \"It generates a fake document to guide retrieval in a real corpus.\"  \n",
    "2. \"Queries gain a synthetic helper document for sharper search in dense vectors.\"  \n",
    "3. \"A two-step trick that eliminates complex labels but keeps retrieval strong.\"  \n",
    "4. \"Hypothesis text plus contrastive encoding equals simplified, label-free document search.\"  \n",
    "5. \"Instructions feed a made-up passage, and vector matching does the rest.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 1) LIBRARY IMPORTS\n",
    "# -------------------------------------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) OPENAI CLIENT SETUP\n",
    "# -------------------------------------------------------\n",
    "# Instantiating the OpenAI client using an endpoint and an API key from environment variables.\n",
    "import os\n",
    "\n",
    "# Make sure to set the environment variable externally (e.g., in your OS or a .env file)\n",
    "openai_base_url = os.getenv(\"OPENAI_BASE_URL\")  # e.g., \"https://api.studio.nebius.com/v1/\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "api_client = OpenAI(\n",
    "    base_url=openai_base_url,\n",
    "    api_key=openai_api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dabe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 3) DOCUMENT READING AND SEGMENTING\n",
    "# -------------------------------------------------------\n",
    "def pdf_to_text(pdf_file):\n",
    "    \"\"\"\n",
    "    Reads a PDF file and extracts textual content page by page.\n",
    "\n",
    "    Args:\n",
    "        pdf_file (str): Path to the PDF.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: Each item holds 'text' and 'metadata'.\n",
    "    \"\"\"\n",
    "    print(f\"Parsing PDF file: {pdf_file}\")\n",
    "    doc = fitz.open(pdf_file)\n",
    "    contents = []\n",
    "\n",
    "    for page_index in range(len(doc)):\n",
    "        page_content = doc[page_index].get_text()\n",
    "        if len(page_content.strip()) > 50:\n",
    "            contents.append({\n",
    "                \"text\": page_content,\n",
    "                \"metadata\": {\n",
    "                    \"source\": pdf_file,\n",
    "                    \"page_number\": page_index + 1\n",
    "                }\n",
    "            })\n",
    "\n",
    "    print(f\"Pages extracted with sufficient content: {len(contents)}\")\n",
    "    return contents\n",
    "\n",
    "def segment_text(raw_text, seg_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Splits text into sections of fixed size, with optional overlap.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): Entire text to be split.\n",
    "        seg_size (int): Character length of each segment.\n",
    "        overlap (int): Overlapping characters between segments.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: Segments with 'text' and 'metadata'.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    step = seg_size - overlap\n",
    "\n",
    "    for start_idx in range(0, len(raw_text), step):\n",
    "        segment = raw_text[start_idx:start_idx + seg_size]\n",
    "        if segment:\n",
    "            segments.append({\n",
    "                \"text\": segment,\n",
    "                \"metadata\": {\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": start_idx + len(segment)\n",
    "                }\n",
    "            })\n",
    "\n",
    "    print(f\"Total segments created: {len(segments)}\")\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b0d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) VECTOR STORAGE CLASS\n",
    "# -------------------------------------------------------\n",
    "class VectorStorage:\n",
    "    \"\"\"\n",
    "    Stores text segments and corresponding vector embeddings.\n",
    "    Provides similarity searches based on cosine similarity.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.embeds = []\n",
    "        self.text_blobs = []\n",
    "        self.meta_info = []\n",
    "\n",
    "    def store(self, text_piece, vector_embedding, meta=None):\n",
    "        \"\"\"\n",
    "        Inserts a new record in the vector store.\n",
    "\n",
    "        Args:\n",
    "            text_piece (str): Content chunk.\n",
    "            vector_embedding (list of float): Embedding for chunk.\n",
    "            meta (dict): Additional metadata to store.\n",
    "        \"\"\"\n",
    "        self.embeds.append(np.array(vector_embedding))\n",
    "        self.text_blobs.append(text_piece)\n",
    "        self.meta_info.append(meta if meta else {})\n",
    "\n",
    "    def find_similar(self, query_vector, top_k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Retrieves top_k similar chunks based on cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_vector (list of float): Embedding vector of query.\n",
    "            top_k (int): Number of similar results.\n",
    "            filter_func (callable): Optional function to filter results.\n",
    "\n",
    "        Returns:\n",
    "            list of dict: The top_k results, sorted by similarity.\n",
    "        \"\"\"\n",
    "        if not self.embeds:\n",
    "            return []\n",
    "\n",
    "        query_vec = np.array(query_vector)\n",
    "        sims = []\n",
    "\n",
    "        for idx, db_vec in enumerate(self.embeds):\n",
    "            if filter_func and not filter_func(self.meta_info[idx]):\n",
    "                continue\n",
    "            sim_val = np.dot(query_vec, db_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(db_vec))\n",
    "            sims.append((idx, sim_val))\n",
    "\n",
    "        sims.sort(key=lambda tup: tup[1], reverse=True)\n",
    "        top_matches = []\n",
    "        for i in range(min(top_k, len(sims))):\n",
    "            found_idx, score_val = sims[i]\n",
    "            top_matches.append({\n",
    "                \"text\": self.text_blobs[found_idx],\n",
    "                \"metadata\": self.meta_info[found_idx],\n",
    "                \"similarity\": float(score_val)\n",
    "            })\n",
    "\n",
    "        return top_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a29b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) EMBEDDING FUNCTIONS\n",
    "# -------------------------------------------------------\n",
    "def create_vector(text_list, model_name=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    Generates vector embeddings for a batch of text inputs.\n",
    "\n",
    "    Args:\n",
    "        text_list (list of str): List of textual data to embed.\n",
    "        model_name (str): Model used for embedding.\n",
    "\n",
    "    Returns:\n",
    "        list of list of float: Embedding vectors.\n",
    "    \"\"\"\n",
    "    if not text_list:\n",
    "        return []\n",
    "\n",
    "    batch_limit = 100\n",
    "    all_vecs = []\n",
    "\n",
    "    # Splitting into smaller groups to avoid hitting API limits\n",
    "    for start_index in range(0, len(text_list), batch_limit):\n",
    "        subset = text_list[start_index:start_index + batch_limit]\n",
    "        resp = api_client.embeddings.create(\n",
    "            model=model_name,\n",
    "            input=subset\n",
    "        )\n",
    "        sub_embeddings = [r.embedding for r in resp.data]\n",
    "        all_vecs.extend(sub_embeddings)\n",
    "\n",
    "    return all_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8152df6",
   "metadata": {},
   "source": [
    "## 6) Building Vector Storage from a PDF\n",
    "\n",
    "Reads the PDF into a list of pages (via pdf_to_text).\n",
    "\n",
    "Splits each page into overlapping chunks (segment_text).\n",
    "\n",
    "Invokes create_vector to embed each chunk.\n",
    "\n",
    "Stores them in an instance of VectorStorage.\n",
    "\n",
    "This function effectively converts the PDF into an indexed and searchable structure for RAG queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) BUILD THE VECTOR STORAGE FROM A PDF\n",
    "# -------------------------------------------------------\n",
    "def build_vector_storage(pdf_file, seg_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Constructs a vector store from a PDF by extracting text, segmenting,\n",
    "    and embedding each segment.\n",
    "\n",
    "    Args:\n",
    "        pdf_file (str): Path to PDF.\n",
    "        seg_size (int): Segment size in characters.\n",
    "        overlap (int): Overlap in characters.\n",
    "\n",
    "    Returns:\n",
    "        VectorStorage: The completed vector store.\n",
    "    \"\"\"\n",
    "    pages_data = pdf_to_text(pdf_file)\n",
    "    all_sections = []\n",
    "\n",
    "    for page_item in pages_data:\n",
    "        sub_parts = segment_text(page_item[\"text\"], seg_size, overlap)\n",
    "        for part in sub_parts:\n",
    "            # Update metadata with the page's info\n",
    "            part[\"metadata\"].update(page_item[\"metadata\"])\n",
    "        all_sections.extend(sub_parts)\n",
    "\n",
    "    print(\"Computing embeddings for all segments...\")\n",
    "    text_inputs = [p[\"text\"] for p in all_sections]\n",
    "    text_embeddings = create_vector(text_inputs)\n",
    "\n",
    "    vs = VectorStorage()\n",
    "    for idx, fragment in enumerate(all_sections):\n",
    "        vs.store(\n",
    "            text_piece=fragment[\"text\"],\n",
    "            vector_embedding=text_embeddings[idx],\n",
    "            meta=fragment[\"metadata\"]\n",
    "        )\n",
    "\n",
    "    print(f\"Vector storage completed with {len(all_sections)} total segments.\")\n",
    "    return vs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc81280",
   "metadata": {},
   "source": [
    "## 7) Crafting a Hypothetical Document\n",
    "\n",
    "* Sends a system prompt and user prompt to the LLM.\n",
    "\n",
    "* The system instructs the model to generate a single, cohesive document that might answer the question thoroughly.\n",
    "\n",
    "* The user prompt clarifies the question.\n",
    "\n",
    "* The outcome is a ‚Äúhypothetical‚Äù text that might represent a perfect or near-perfect response to the user‚Äôs question. We don‚Äôt store or surface this text to the user directly; instead, we embed it and use it to search in our knowledge base.\n",
    "\n",
    "## HyDE‚Äôs Core Concept\n",
    "\n",
    "The fundamental idea behind HyDE (Hypothetical Document Embedding) is that instead of directly embedding the user‚Äôs short (and sometimes ambiguous) query, you first generate a hypothetical response to that query‚Äîan imagined, in-depth document that could answer the query in the best possible way‚Äîthen embed that document instead of the user‚Äôs original query. By embedding a richer, more detailed piece of text, you often capture deeper semantics that can lead to more accurate retrieval from your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 7) CREATE HYPOTHETICAL DOCUMENT (HyDE)\n",
    "# -------------------------------------------------------\n",
    "def craft_hypothetical_answer(user_query, approx_len=1000):\n",
    "    \"\"\"\n",
    "    Uses an LLM to produce a hypothetical document that could\n",
    "    potentially answer the user's query in depth.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query.\n",
    "        approx_len (int): Desired approximate length of the generated doc.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated hypothetical answer.\n",
    "    \"\"\"\n",
    "    system_context = (\n",
    "        f\"You are tasked with writing an authoritative, detailed piece of text. \"\n",
    "        f\"Given a user question, produce a thorough and informative response of about {approx_len} characters. \"\n",
    "        f\"Provide facts, examples, and clarity. Do not mention it's hypothetical.\"\n",
    "    )\n",
    "    user_request = f\"Question: {user_query}\\n\\nWrite a comprehensive document that fully addresses this question.\"\n",
    "\n",
    "    response_msg = api_client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_context},\n",
    "            {\"role\": \"user\", \"content\": user_request}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    return response_msg.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c8267",
   "metadata": {},
   "source": [
    "## 8) The HyDE RAG Process\n",
    "\n",
    "* Generate a hypothetical answer to the user‚Äôs query via craft_hypothetical_answer.\n",
    "\n",
    "* Embed that hypothetical document using create_vector.\n",
    "\n",
    "* Retrieve top num_chunks from the vector store with the new embedding.\n",
    "\n",
    "* (Optional) Compose a final answer, using the retrieved segments.\n",
    "\n",
    "**Why does this help?**\n",
    "\n",
    "A query might be only a few words. Generating a richer ‚Äúpotential answer‚Äù can produce a more comprehensive embedding that captures the semantics the user truly wants. This bridging can improve retrieval for short or ambiguous queries.\n",
    "\n",
    "An instruction-based LLM creates a fake passage for each query q. Imagine you ask, ‚ÄúHow to store solar energy cheaply?‚Äù The LLM responds with a quick, possibly inaccurate paragraph about solar energy storage. This text represents a hypothetical document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58883727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 8) RAG WITH HYPOTHETICAL EMBEDDING (HyDE)\n",
    "# -------------------------------------------------------\n",
    "def execute_hyde_rag(query_text, vec_storage, num_chunks=5, gen_final=True):\n",
    "    \"\"\"\n",
    "    Performs RAG with HyDE. Generates a hypothetical document,\n",
    "    embeds it, retrieves similar text segments, and optionally\n",
    "    composes a final answer.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): User's query.\n",
    "        vec_storage (VectorStorage): Populated vector store.\n",
    "        num_chunks (int): Number of chunks to fetch.\n",
    "        gen_final (bool): Whether to generate the final answer.\n",
    "\n",
    "    Returns:\n",
    "        dict: Includes the hypothetical doc, retrieved segments,\n",
    "              and optionally the final composed response.\n",
    "    \"\"\"\n",
    "    print(f\"\\n** HyDE RAG Query: {query_text} **\\n\")\n",
    "    print(\"1) Creating hypothetical document...\")\n",
    "    hypothetical_text = craft_hypothetical_answer(query_text)\n",
    "    print(f\"Hypothetical document length: {len(hypothetical_text)} characters\")\n",
    "\n",
    "    print(\"2) Embedding the hypothetical document...\")\n",
    "    hypo_embed = create_vector([hypothetical_text])[0]\n",
    "\n",
    "    print(f\"3) Retrieving top {num_chunks} matching segments...\")\n",
    "    retrieved = vec_storage.find_similar(hypo_embed, top_k=num_chunks)\n",
    "\n",
    "    outcomes = {\n",
    "        \"user_query\": query_text,\n",
    "        \"hypo_document\": hypothetical_text,\n",
    "        \"retrieved_fragments\": retrieved\n",
    "    }\n",
    "\n",
    "    if gen_final:\n",
    "        print(\"4) Composing final answer from retrieved segments...\")\n",
    "        final_ans = compose_answer(query_text, retrieved)\n",
    "        outcomes[\"final_answer\"] = final_ans\n",
    "\n",
    "    return outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51721b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 9) COMPOSING THE FINAL ANSWER\n",
    "# -------------------------------------------------------\n",
    "def compose_answer(query_text, relevant_chunks):\n",
    "    \"\"\"\n",
    "    Forms a concluding response from relevant chunks.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The original user query.\n",
    "        relevant_chunks (list of dict): Top retrieved segments.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated final answer.\n",
    "    \"\"\"\n",
    "    combined_context = \"\\n\\n\".join([f[\"text\"] for f in relevant_chunks])\n",
    "\n",
    "    response_msg = api_client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful AI assistant. \"\n",
    "                    \"Use the context below to answer the user's question.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context:\\n{combined_context}\\n\\nQuestion: {query_text}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    return response_msg.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83554c2a",
   "metadata": {},
   "source": [
    "## 10) Standard RAG\n",
    "\n",
    "Directly embeds the user query.\n",
    "\n",
    "Retrieves the top num_chunks.\n",
    "\n",
    "Optionally constructs a final answer from those retrieved pieces.\n",
    "\n",
    "This is a baseline approach, often good but sometimes not as robust as HyDE for short or vague prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e7c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 10) STANDARD RAG (DIRECT QUERY EMBEDDING)\n",
    "# -------------------------------------------------------\n",
    "def execute_standard_rag(query_text, vec_storage, num_chunks=5, gen_final=True):\n",
    "    \"\"\"\n",
    "    Carries out standard RAG: directly embed the user's query,\n",
    "    retrieve similar segments, and optionally produce a final answer.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): User's query.\n",
    "        vec_storage (VectorStorage): Populated vector store.\n",
    "        num_chunks (int): Number of chunks to retrieve.\n",
    "        gen_final (bool): Whether to generate the final answer.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the query, retrieved chunks, and optionally the answer.\n",
    "    \"\"\"\n",
    "    print(f\"\\n** Standard RAG Query: {query_text} **\\n\")\n",
    "    print(\"1) Generating query embedding...\")\n",
    "    direct_embed = create_vector([query_text])[0]\n",
    "\n",
    "    print(f\"2) Retrieving top {num_chunks} segments...\")\n",
    "    retrieved = vec_storage.find_similar(direct_embed, top_k=num_chunks)\n",
    "\n",
    "    result_payload = {\n",
    "        \"user_query\": query_text,\n",
    "        \"retrieved_fragments\": retrieved\n",
    "    }\n",
    "\n",
    "    if gen_final:\n",
    "        print(\"3) Composing final answer from retrieved segments...\")\n",
    "        final = compose_answer(query_text, retrieved)\n",
    "        result_payload[\"final_answer\"] = final\n",
    "\n",
    "    return result_payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 11) COMPARISON AND EVALUATION\n",
    "# -------------------------------------------------------\n",
    "def assess_strategies(query_text, vec_storage, reference_solution=None):\n",
    "    \"\"\"\n",
    "    Evaluates the outcomes of both HyDE-based RAG and\n",
    "    Standard RAG for a particular query.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The query to evaluate.\n",
    "        vec_storage (VectorStorage): Vector store containing chunk data.\n",
    "        reference_solution (str): Optional known correct answer.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains responses from both approaches and a comparison.\n",
    "    \"\"\"\n",
    "    # HyDE approach\n",
    "    hyde_out = execute_hyde_rag(query_text, vec_storage)\n",
    "    hyde_reply = hyde_out.get(\"final_answer\", \"\")\n",
    "\n",
    "    # Direct RAG approach\n",
    "    standard_out = execute_standard_rag(query_text, vec_storage)\n",
    "    standard_reply = standard_out.get(\"final_answer\", \"\")\n",
    "\n",
    "    # Evaluate differences\n",
    "    comp = evaluate_outcomes(query_text, hyde_reply, standard_reply, reference_solution)\n",
    "\n",
    "    return {\n",
    "        \"query\": query_text,\n",
    "        \"hyde_response\": hyde_reply,\n",
    "        \"hyde_hypothetical_doc\": hyde_out[\"hypo_document\"],\n",
    "        \"standard_response\": standard_reply,\n",
    "        \"reference\": reference_solution,\n",
    "        \"comparison\": comp\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_outcomes(query_text, hyde_ans, standard_ans, ref_ans=None):\n",
    "    \"\"\"\n",
    "    Uses LLM to compare two responses (HyDE vs. Standard RAG)\n",
    "    and optionally references a correct solution.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The original user query.\n",
    "        hyde_ans (str): HyDE's final answer.\n",
    "        standard_ans (str): Standard RAG final answer.\n",
    "        ref_ans (str, optional): A known correct answer for reference.\n",
    "\n",
    "    Returns:\n",
    "        str: Summarized comparison text from the LLM.\n",
    "    \"\"\"\n",
    "    sys_instructions = (\n",
    "        \"You are an evaluator comparing two approaches. \"\n",
    "        \"Discuss accuracy, relevance, completeness, and clarity.\"\n",
    "    )\n",
    "\n",
    "    user_instructions = (\n",
    "        f\"Query: {query_text}\\n\\n\"\n",
    "        f\"HyDE-based response:\\n{hyde_ans}\\n\\n\"\n",
    "        f\"Standard RAG response:\\n{standard_ans}\"\n",
    "    )\n",
    "\n",
    "    if ref_ans:\n",
    "        user_instructions += f\"\\n\\nReference Answer:\\n{ref_ans}\"\n",
    "\n",
    "    user_instructions += (\n",
    "        \"\\n\\nPlease analyze which approach better addresses the question and why.\"\n",
    "    )\n",
    "\n",
    "    eval_msg = api_client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_instructions}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return eval_msg.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 12) RUNNING A BATCH EVALUATION\n",
    "# -------------------------------------------------------\n",
    "def batch_evaluation(pdf_file, queries, correct_answers=None, seg_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Processes a PDF file into a vector store, then runs multiple queries\n",
    "    against both HyDE-based RAG and Standard RAG, collecting comparisons.\n",
    "\n",
    "    Args:\n",
    "        pdf_file (str): Path to the PDF.\n",
    "        queries (list of str): List of user queries.\n",
    "        correct_answers (list of str): Reference solutions for each query.\n",
    "        seg_size (int): Segment size for chunking.\n",
    "        overlap (int): Overlap among segments.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the list of comparison results and a summary.\n",
    "    \"\"\"\n",
    "    vs = build_vector_storage(pdf_file, seg_size, overlap)\n",
    "\n",
    "    results_list = []\n",
    "    for idx, item_query in enumerate(queries):\n",
    "        print(f\"\\n=== EVALUATING QUERY {idx+1}/{len(queries)}: {item_query} ===\")\n",
    "        reference_text = None\n",
    "        if correct_answers and idx < len(correct_answers):\n",
    "            reference_text = correct_answers[idx]\n",
    "\n",
    "        outcome = assess_strategies(item_query, vs, reference_text)\n",
    "        results_list.append(outcome)\n",
    "\n",
    "    overall_summary = aggregate_results(results_list)\n",
    "    return {\n",
    "        \"comparisons\": results_list,\n",
    "        \"summary\": overall_summary\n",
    "    }\n",
    "\n",
    "def aggregate_results(comparison_outcomes):\n",
    "    \"\"\"\n",
    "    Summarizes the overall performance across all queries.\n",
    "\n",
    "    Args:\n",
    "        comparison_outcomes (list of dict): Results from assess_strategies.\n",
    "\n",
    "    Returns:\n",
    "        str: A compiled summary from the LLM.\n",
    "    \"\"\"\n",
    "    sys_prompt = (\n",
    "        \"You are an expert in comparing retrieval approaches for QA. \"\n",
    "        \"Based on multiple queries, provide an overall assessment comparing \"\n",
    "        \"HyDE RAG with standard RAG. Discuss where each approach excels and any recommendations.\"\n",
    "    )\n",
    "\n",
    "    short_summaries = \"\"\n",
    "    for idx, result_item in enumerate(comparison_outcomes):\n",
    "        short_summaries += f\"Query #{idx+1}: {result_item['query']}\\nComparison snippet: {result_item['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Below are partial comparisons of {len(comparison_outcomes)} queries. \"\n",
    "        f\"Please form a comprehensive analysis:\\n\\n{short_summaries}\"\n",
    "    )\n",
    "\n",
    "    response_msg = api_client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response_msg.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 13) VISUALIZING RESULTS\n",
    "# -------------------------------------------------------\n",
    "def draw_comparison(query_txt, hyde_data, standard_data):\n",
    "    \"\"\"\n",
    "    Provides a simple visualization comparing the query,\n",
    "    hypothetical document, and retrieved chunks from both approaches.\n",
    "\n",
    "    Args:\n",
    "        query_txt (str): The user's query string.\n",
    "        hyde_data (dict): Output from execute_hyde_rag.\n",
    "        standard_data (dict): Output from execute_standard_rag.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # 1) Plot the user query\n",
    "    axes[0].text(0.5, 0.5, f\"Query:\\n\\n{query_txt}\",\n",
    "                 ha='center', va='center', fontsize=11, wrap=True)\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "    # 2) Show the hypothetical document\n",
    "    hypo_doc = hyde_data[\"hypo_document\"]\n",
    "    doc_excerpt = hypo_doc if len(hypo_doc) <= 500 else hypo_doc[:500] + \"...\"\n",
    "    axes[1].text(0.5, 0.5, f\"Hypothetical Doc:\\n\\n{doc_excerpt}\",\n",
    "                 ha='center', va='center', fontsize=10, wrap=True)\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "    # 3) Compare retrieved chunks\n",
    "    hyde_chx = [c[\"text\"][:100] + \"...\" for c in hyde_data[\"retrieved_fragments\"]]\n",
    "    std_chx = [c[\"text\"][:100] + \"...\" for c in standard_data[\"retrieved_fragments\"]]\n",
    "\n",
    "    compare_text = \"HyDE RAG retrieved:\\n\\n\"\n",
    "    for i, chunky in enumerate(hyde_chx):\n",
    "        compare_text += f\"{i+1}) {chunky}\\n\\n\"\n",
    "\n",
    "    compare_text += \"\\nStandard RAG retrieved:\\n\\n\"\n",
    "    for i, chunky in enumerate(std_chx):\n",
    "        compare_text += f\"{i+1}) {chunky}\\n\\n\"\n",
    "\n",
    "    axes[2].text(0.5, 0.5, compare_text, ha='center', va='center',\n",
    "                 fontsize=8, wrap=True)\n",
    "    axes[2].set_axis_off()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 14) USAGE EXAMPLE\n",
    "# -------------------------------------------------------\n",
    "# Below is an example usage scenario that:\n",
    "# 1) Processes a sample PDF into the vector store\n",
    "# 2) Asks a query using HyDE\n",
    "# 3) Asks a query using standard RAG\n",
    "# 4) Compares & visualizes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Provide a path to a PDF file\n",
    "    sample_pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "    # Build vector store\n",
    "    store = build_vector_storage(sample_pdf_path)\n",
    "\n",
    "    # Example user query\n",
    "    example_query = \"What are the main ethical considerations in artificial intelligence development?\"\n",
    "\n",
    "    # HyDE-based retrieval\n",
    "    hyde_outcome = execute_hyde_rag(example_query, store)\n",
    "    print(\"\\n-- HyDE-based Answer --\")\n",
    "    print(hyde_outcome.get(\"final_answer\", \"\"))\n",
    "\n",
    "    # Standard RAG retrieval\n",
    "    standard_outcome = execute_standard_rag(example_query, store)\n",
    "    print(\"\\n-- Standard RAG Answer --\")\n",
    "    print(standard_outcome.get(\"final_answer\", \"\"))\n",
    "\n",
    "    # Visualization\n",
    "    draw_comparison(example_query, hyde_outcome, standard_outcome)\n",
    "\n",
    "    # Additional queries for a batch evaluation\n",
    "    queries_to_test = [\n",
    "        \"How does neural network architecture impact AI performance?\"\n",
    "    ]\n",
    "    # Optional reference answers\n",
    "    references_list = [\n",
    "        \"Neural network architecture affects performance by influencing model capacity, generalization, and efficiency. Variations in layer depth, width, and connections optimize tasks like vision or language.\"\n",
    "    ]\n",
    "\n",
    "    # Execute a comprehensive run\n",
    "    evaluation_data = batch_evaluation(\n",
    "        pdf_file=sample_pdf_path,\n",
    "        queries=queries_to_test,\n",
    "        correct_answers=references_list\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Overall Summary ===\")\n",
    "    print(evaluation_data[\"summary\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
