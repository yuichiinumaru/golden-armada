# Arxiv Analysis: Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents (Rogue One)

**ID:** 2511.15074
**Date:** 2025-11-19
**Link:** https://arxiv.org/abs/2511.15074

## 1. Executive Summary
This paper presents **Rogue One**, a multi-agent framework for **Automated Feature Extraction (AutoFE)** in machine learning. Unlike monolithic approaches, it uses three specialized agents (**Scientist, Extractor, Tester**) to iteratively discover, code, and validate features. It introduces a **"flooding-pruning"** strategy to balance exploration (generating many candidates) and exploitation (selecting the best), achieving SOTA results on 28 datasets.

## 2. Detailed Key Concepts

### 2.1. The Agent Trio
-   **Scientist Agent:** Uses RAG to retrieve external domain knowledge and hypothesizes new features (e.g., "For myocardial data, try extracting this specific biomarker").
-   **Extractor Agent:** Translates hypotheses into executable Python/Pandas code.
-   **Tester Agent:** Runs the code, trains a model, and provides **qualitative** and **quantitative** feedback (not just "Accuracy: 0.8", but "This feature is highly correlated with target but sparse").

### 2.2. Flooding-Pruning Strategy
-   **Flooding:** Generate a diverse set of potential features based on broad hypotheses.
-   **Pruning:** Rigorously test and select only the most predictive ones, discarding noise. This prevents the "feature explosion" problem common in AutoFE.

## 3. Gap Analysis

| Feature/Concept | Current CodeSwarm | Gap |
| :--- | :--- | :--- |
| **Domain-Specific RAG** | Knowledge Agent (Codebase only) | **High:** Our Knowledge Agent searches the *codebase*. Rogue One's Scientist searches *external domain literature* to inform coding. |
| **Exploration/Exploitation** | Linear execution | **Medium:** We don't have a "Try 10 things, keep 1" loop. We usually try 1 thing and fix it. |
| **Qualitative Testing** | Revisor (Code quality) | **Medium:** Revisor checks if code *runs* and looks good. It doesn't check if the *result* (model performance) is good. |

## 4. Implementation Plan (Agno + SurrealDB + Gemini)

### Phase 1: The "Scientist" Role (External RAG)
-   **Action:** Upgrade `KnowledgeAgent` to support external search (Google Search / ArXiv) for "Domain Knowledge".
-   **Prompt:** "Search for best practices/algorithms for [User Task]. Summarize them for the DevAgent."

### Phase 2: Parallel Experimentation (Flooding)
-   **Action:** Allow AdminAgent to spawn *multiple* parallel DevAgents for the same task with different strategies ("Try Approach A", "Try Approach B").
-   **Evaluation:** A "Judge" agent compares the results (e.g., benchmark scores, code complexity) and picks the winner (Pruning).

### Phase 3: Qualitative Feedback Loop
-   **Action:** Ensure the Tester/Revisor provides semantic feedback ("This solution is O(n^2), too slow") rather than just pass/fail.
