# Arxiv Analysis: Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration

**ID:** 2511.15351
**Date:** 2025-11-19
**Link:** https://arxiv.org/html/2511.15351

## 1. Executive Summary
Octopus introduces a **capability-centric** framework for multimodal agents. Instead of treating tool use as a flat list of API calls, it organizes reasoning into six distinct capabilities: **Perception, Augmentation, Spatial Understanding, Logical Programming, Transformation, and Generation**. The agent follows a structured loop: internal thought -> capability selection -> tool selection -> execution. This "Capability-First" approach significantly outperforms monolithic MLLMs on complex visual reasoning tasks.

## 2. Detailed Key Concepts

### 2.1. The Six Capabilities
1.  **Fine-grained Perception:** Extracting text (OCR), locating objects.
2.  **Visual Augmentation:** Drawing bounding boxes or arrows on the image to "externalize" reasoning.
3.  **Spatial Understanding:** Computing distances, areas, and geometric relations.
4.  **Logical Programming:** Writing code to solve symbolic math or logic puzzles.
5.  **Visual Transformation:** Cropping, resizing, or segmenting the image to focus on details.
6.  **Visual Generation:** Creating sketches or diagrams to aid visualization.

### 2.2. The Reasoning Loop
Algorithm 1 describes the loop:
1.  Initialize state with Image + Question.
2.  Loop:
    -   Generate **Thought** (`<think>...`).
    -   Select **Capability** (`<cap>...`).
    -   Select **Tool** from that capability (`<toolcall>...`).
    -   Execute and update state.
3.  Output final answer.

## 3. Gap Analysis

| Feature/Concept | Current CodeSwarm | Gap |
| :--- | :--- | :--- |
| **Capability Grouping** | Flat Tool List | **Medium:** DevAgent has access to all tools at once. Grouping them by "Phase" (Coding, Testing, Debugging) might improve focus. |
| **Visual Augmentation** | None | **Low:** (Unless we do UI tasks). Agents can't "draw" on screenshots to debug UI layouts. |
| **Explicit Capability Step** | Implicit | **Medium:** Agents jump straight to tools. Forcing a "Capability Selection" step (e.g., "I need to *Search* now") improves reasoning stability. |

## 4. Implementation Plan (Agno + SurrealDB + Gemini)

### Phase 1: Tool Categorization
-   **Action:** Tag every tool in `codeswarm/tools.py` with a capability (e.g., `File Ops`, `Git Ops`, `Analysis`, `Testing`).
-   **Usage:** In the system prompt, present tools grouped by these categories.

### Phase 2: Capability-First Prompting
-   **Action:** Update DevAgent prompt:
    -   "Step 1: Decide which Capability you need (Coding, Testing, Searching)."
    -   "Step 2: Choose a tool from that category."
-   **Benefit:** Reduces the search space for the LLM and prevents "tool hallucination".

### Phase 3: Visual Debugging (Future)
-   **Action:** If debugging a frontend, allow the agent to use Python (PIL/OpenCV) to draw bounding boxes on the screenshot around the element it *thinks* is broken, then ask the VLM (Gemini) to confirm.
