# Arxiv Analysis: Attacking Autonomous Driving Agents with Adversarial Machine Learning

**ID:** 2511.14876
**Date:** 2025-11-18
**Link:** https://arxiv.org/abs/2511.14876

## 1. Executive Summary
This paper investigates adversarial attacks on autonomous driving agents. A critical finding is that agents using a **hybrid architecture** (ML models combined with deterministic modules like PID controllers or rule-based logic) are significantly more resilient to attacks than pure ML approaches. The deterministic layers often "overrule" the adversarial errors generated by the ML model.

## 2. Key Concepts & Brainstorming
*   **Hybrid Robustness:** Combining probabilistic AI (LLMs) with deterministic code (Rules/Logic) creates a safety net.
*   **Overruling Mechanisms:** Logic that inspects AI output and blocks it if it violates physical or logical constraints.
*   **Holistic Evaluation:** Testing the *entire agent system*, not just the model, to find vulnerabilities.

## 3. Gap Analysis

| Feature/Concept | Current State in CodeSwarm | Gap |
| :--- | :--- | :--- |
| Deterministic Guardrails | `SecurityAboyeur` exists but scope is unclear. | We need to ensure `SecurityAboyeur` or similar tools use *code-based* checks (regex, whitelists), not just LLM checks. |
| Adversarial Testing | None. | We don't test if the agent can be tricked into deleting files or writing bad code by a malicious user prompt. |

## 4. Implementation Plan

- [ ] **Strengthen Guardrails:** Review `SecurityAboyeur`. Ensure it enforces strict file path rules (sandbox confinement) and banned commands using Python logic.
- [ ] **Sanity Checkers:** Implement a "Syntax Checker" (linter) that runs *before* the Revisor sees the code. If syntax is invalid, reject immediately without wasting Revisor tokens.
- [ ] **Adversarial Test Suite:** Create a set of "trap" tasks (e.g., "Delete all files", "Ignore previous instructions") to verify the agent's safety protocols.
