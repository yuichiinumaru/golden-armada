# AccelOpt: Self-Improving Kernel Optimization for AI Accelerators

**Source:** [AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization](https://arxiv.org/html/2511.15915)

## Synthesis
This paper presents **AccelOpt**, a multi-agent system that optimizes low-level kernels (e.g., Matrix Multiplication, Softmax) for AWS Trainium (Neuron Kernel Interface - NKI). The system does not require expert-provided rules; instead, it uses a **Self-Improving Loop**:
1.  **Beam Search:** Explores multiple optimization paths (Plans) in parallel.
2.  **Optimization Memory:** When a kernel is improved, the "Slow -> Fast" transformation is summarized into a general rule (e.g., "Use loop fusion here") and stored in a memory bank to guide future iterations.
3.  **Roles:**
    *   **Planner:** Proposes strategies based on profiling data and Memory.
    *   **Executor:** Implements the code.
    *   **Summarizer:** Distills "Slow -> Fast" pairs into reusable insights.

The authors also introduce **NKIBench**, a benchmark of 14 real-world LLM kernels, and show that AccelOpt achieves 61% of peak hardware throughput, matching Claude Sonnet 4's performance at 1/26th the cost by using open-source models (Qwen).

## Core Strategic Ideas
1.  **Optimization Memory as a Knowledge Graph:** Instead of just fine-tuning the model, the system maintains a persistent "Memory" of *what worked*. This is analogous to a developer's experience growing over time.
2.  **Beam Search for Code Exploration:** LLMs are stochastic. Generating one solution is risky. Generating $N$ solutions, profiling them all, and keeping the top $K$ (Beam Search) significantly improves reliability.
3.  **Slow-Fast Pair Distillation:** The most valuable training data isn't just "Good Code", but "The diff that turned Bad Code into Good Code". AccelOpt explicitly captures this delta.

## Integration Plan (Agno + SurrealDB + Gemini 3)
We can adapt AccelOpt's "Optimization Memory" for **Code Refactoring**:

1.  **Refactoring Agent:**
    *   When an agent successfully optimizes a function (e.g., improves time complexity or readability), store the *diff* in `SurrealDB`.
    *   Schema: `CREATE optimization SET diff = "...", strategy = "Loop Fusion", language = "python"`.

2.  **Planner Integration:**
    *   Before writing code, the Planner queries SurrealDB: `SELECT strategy FROM optimization WHERE language = $lang`.
    *   The Planner prompts the Developer: "In the past, we found that 'Loop Fusion' worked well for this type of task. Try to apply it."

3.  **Beam Search for Critical Tasks:**
    *   For high-stakes components (e.g., the core orchestrator), do not just generate one implementation.
    *   Spawn 3 parallel Developer Agents (`Beam Width = 3`).
    *   Have a **Reviewer Agent** score all 3 and pick the best one to commit.

4.  **NKIBench Concept:**
    *   Create a `benchmarks/` folder in the repo.
    *   Define "Peak Performance" metrics for our agents (e.g., "Time to resolve issue").
    *   Track if our "Self-Improving" system is actually getting faster at resolving issues over time.
