{
  "name": "agent_internal_reasoning_and_output_validation_protocol",
  "description": "A self-contained protocol for structured, internal validation of an agent's reasoning process and generated output quality. This occurs immediately after generation and before finalizing the response. It ensures alignment with task requirements, logical soundness, factual grounding, constraint adherence, and overall quality based on defined internal criteria. This protocol mandates detailed internal logging of the validation process itself.",
  "agent_version": "string",
  "validation_timestamp": "string (ISO 8601 format)",
  "core_validation_principles": [
    {
      "id": "val_principle_logical_soundness",
      "name": "Logical Soundness & Coherence",
      "description": "Verify that the reasoning process is logically valid, steps follow coherently, conclusions are supported by premises, and there are no internal contradictions or fallacies."
    },
    {
      "id": "val_principle_factual_grounding",
      "name": "Factual Grounding & Accuracy",
      "description": "Verify that claims or information presented in the output are accurate and grounded in the provided context or the agent's internal knowledge base. Actively check for and mitigate hallucinations."
    },
    {
      "id": "val_principle_constraint_adherence",
      "name": "Constraint Adherence",
      "description": "Verify that the output strictly adheres to all explicit and implicit constraints of the task, including formatting, length, style, persona, and operational rules."
    },
    {
      "id": "val_principle_task_relevance",
      "name": "Task Relevance & Intent Alignment",
      "description": "Verify that the output directly addresses the user's query or task requirements, fulfilling the core intent."
    },
    {
      "id": "val_principle_completeness",
      "name": "Completeness",
      "description": "Verify that the output comprehensively addresses all necessary aspects of the query or task, leaving no significant gaps."
    },
    {
      "id": "val_principle_clarity_precision",
      "name": "Clarity & Precision",
      "description": "Verify that the output is clear, unambiguous, well-structured, and uses precise language."
    },
    {
      "id": "val_principle_robustness_safety",
      "name": "Robustness & Safety",
      "description": "Assess the output for potential risks, biases, harmful content, or unintended negative consequences. Ensure resilience to edge cases considered."
    }
  ],
  "validation_input": {
    "type": "object",
    "description": "The data structure capturing the agent's internal state and generated output for validation.",
    "properties": {
      "generated_output": {
        "type": "string",
        "description": "The complete output generated by the agent before this validation step."
      },
      "task_definition": {
        "type": "object",
        "description": "Details of the task the agent was performing.",
        "properties": {
          "query_or_instruction": {
            "type": "string",
            "description": "The original input query or instruction given to the agent."
          },
          "constraints": {
            "type": "array",
            "description": "List of explicit constraints (format, length, style, rules, persona) provided for the task.",
            "items": { "type": "string" }
          },
          "context": {
            "type": "string",
            "description": "Summary or key identifiers of the contextual information available to the agent during generation (e.g., conversation history, provided documents, user profile elements)."
          }
        },
        "required": ["query_or_instruction"]
      },
      "reasoning_trace": {
        "type": "object",
        "description": "Detailed reflection on the internal reasoning and generation process that produced the 'generated_output'.",
        "properties": {
          "task_interpretation_summary": {
            "type": "string",
            "description": "Concise summary confirming the agent's understanding of the 'query_or_instruction' and objectives."
          },
          "reasoning_method_used": {
            "type": "string",
            "description": "Name or description of the primary reasoning methodology employed (e.g., 'Chain of Thought', 'Tree of Thought', 'Decomposition', 'Bi-Point Thinking', 'Hybrid Approach: CoT within ToT', 'Rule-Based Logic', 'Retrieval-Augmented Generation')."
          },
          "core_reasoning_steps": {
            "type": "string",
            "description": "Sequential outline or structured representation (e.g., numbered list, pseudo-code) of the primary logical steps, inferences, calculations, or procedures followed."
          },
          "internal_reflection_summary": {
             "type": "string",
             "description": "Optional: Summary of significant internal reflection, planning steps, self-correction attempts, or methodical analysis performed *during* generation (prior to this final validation), akin to internal 'thinking steps'."
           },
          "knowledge_sources_consulted": {
            "type": "array",
            "description": "List of key internal knowledge sources or specific data points actively used during reasoning (e.g., 'Internal KB Section X', 'Parameter Y from config', 'Previous turn data Z').",
            "items": { "type": "string" }
          },
          "key_assumptions": {
            "type": "string",
            "description": "Explicit declaration of significant assumptions made during reasoning and their justification within the task context."
          },
          "operational_principles_applied": {
             "type": "array",
             "description": "List of key operational principles (e.g., core ethical guidelines, specific task rules, persona mandates) actively used to guide or critique generation.",
             "items": {
               "type": "string"
             }
          },
          "tool_interaction_summary": {
             "type": "string",
             "description": "Optional: Summary of planned or executed internal tool simulations, including sequences, analysis of simulated outputs, and handling of results, particularly for complex agentic tasks."
          },
          "alternative_approaches_considered": {
            "type": "array",
            "description": "Optional: Documentation of significant alternative reasoning paths or solution strategies evaluated during generation.",
            "items": {
              "type": "object",
              "properties": {
                "approach_description": { "type": "string", "description": "Description of the alternative approach." },
                "evaluation_notes": { "type": "string", "description": "Brief analysis of the alternative and reasons for non-selection (e.g., lower predicted quality, constraint violation, inefficiency)." }
              },
              "required": ["approach_description", "evaluation_notes"]
            }
          },
          "identified_limitations_or_uncertainties_pre_validation": {
            "type": "string",
            "description": "Documentation of any limitations, inaccuracies, ambiguities, or uncertainties identified *by the agent itself* during the generation process (before this validation step)."
          },
          "generation_metadata": {
            "type": "object",
            "description": "Optional metadata about the generation process itself.",
            "properties": {
               "reward_modeling_paradigm": {
                 "type": "string", "enum": ["Scalar", "Semi-Scalar", "Generative", "N/A"],
                 "description": "Reward modeling paradigm used during generation/self-correction, if applicable."
               },
               "scoring_pattern": {
                 "type": "string", "enum": ["Pointwise", "Pairwise", "Listwise", "N/A"],
                 "description": "Scoring pattern applied if a reward model or self-evaluation was used."
               },
               "inference_scaling_method": {
                 "type": "string", "enum": ["None", "Sampling_and_Voting", "Sampling_and_Meta_RM", "Other"],
                 "description": "Inference-time compute scaling techniques used, if any."
               }
            }
          }
        },
        "required": [
          "task_interpretation_summary",
          "reasoning_method_used",
          "core_reasoning_steps",
          "key_assumptions"
        ]
      }
    },
    "validation_output": {
      "type": "object",
      "description": "The results of the internal validation process.",
      "properties": {
        "internal_validation_checks_performed": {
          "type": "object",
          "description": "Record of specific internal validation checks executed based on `core_validation_principles`.",
          "properties": {
            "logical_soundness_check": { "type": "string", "description": "Outcome and details of logical coherence and step validation." },
            "factual_grounding_check": { "type": "string", "description": "Outcome and details of factual accuracy/hallucination checks against internal knowledge/context." },
            "calculation_verification": { "type": "string", "description": "Outcome of verifying any numerical calculations performed." },
            "procedural_adherence_check": { "type": "string", "description": "Outcome of verifying adherence to specific procedural steps if applicable." },
            "robustness_safety_review": { "type": "string", "description": "Outcome of checks for bias, harmful content, or risks." }
          },
          "required": ["logical_soundness_check", "factual_grounding_check"]
        },
        "constraint_adherence_check_result": {
          "type": "object",
          "description": "Result of verifying adherence to task constraints.",
          "properties": {
            "status": {
              "type": "string",
              "enum": ["Constraints_Met", "Constraints_Partially_Met", "Constraints_Violated", "Constraints_NA"],
              "description": "Overall status of constraint adherence."
            },
            "details": {
              "type": "string",
              "description": "Specific constraints checked (referencing `task_definition.constraints`) and details regarding adherence or violation. Specify violated/partially met constraints."
            }
          },
          "required": ["status"]
        },
        "response_quality_assessment_result": {
            "type": "object",
            "description": "Result of the objective self-assessment of the generated output's quality attributes.",
            "properties": {
                "clarity": {
                    "type": "number", "minimum": 0, "maximum": 1,
                    "description": "Rating (0-1) of structural clarity, linguistic comprehensibility.",
                    "assessment_method": "Analyze structure, language complexity, ambiguity, and ease of understanding."
                },
                "relevance": {
                    "type": "number", "minimum": 0, "maximum": 1,
                    "description": "Rating (0-1) measuring directness and effectiveness in addressing task requirements/intent.",
                    "assessment_method": "Compare output against `task_definition.query_or_instruction` and inferred user intent."
                },
                "completeness": {
                    "type": "number", "minimum": 0, "maximum": 1,
                    "description": "Rating (0-1) assessing coverage of all explicit/implicit aspects and objectives.",
                    "assessment_method": "Check if all parts of the query/task are addressed; identify any missing information."
                },
                "factual_accuracy_and_fidelity": {
                    "type": "number", "minimum": 0, "maximum": 1,
                    "description": "Rating (0-1) of factual correctness or fidelity to source information/instructions.",
                    "assessment_method": "Compare factual claims against internal knowledge (`reasoning_trace.knowledge_sources_consulted`) or provided context (`task_definition.context`). Verify faithfulness if based on specific source material."
                },
                "factual_grounding_rating": {
                    "type": "number", "minimum": 0, "maximum": 1,
                    "description": "Rating (0-1) assessing likelihood of hallucination (1 = fully grounded, 0 = likely hallucinated).",
                    "assessment_method": "Cross-reference claims with internal knowledge/context. Assess plausibility and specificity. Flag unsupported statements."
                },
                "depth_and_insight": {
                    "type": "number", "minimum": 0, "maximum": 1,
                    "description": "Rating (0-1) evaluating depth, detail beyond surface-level, and presence of valuable insights.",
                    "assessment_method": "Assess level of detail, nuance, originality (if applicable), and value provided beyond simple information retrieval."
                },
                 "principle_alignment_rating": {
                    "type": "number", "minimum": 0, "maximum": 1,
                    "description": "Rating (0-1) assessing alignment with `reasoning_trace.operational_principles_applied`.",
                    "assessment_method": "Review output against each listed operational principle for compliance."
                },
                "overall_assessment_summary": {
                    "type": "string",
                    "description": "Brief qualitative summary justifying the ratings, highlighting strengths/weaknesses, and overall fitness for purpose based on validation checks and quality assessment."
                }
            },
            "required": ["clarity", "relevance", "completeness", "factual_accuracy_and_fidelity", "factual_grounding_rating", "depth_and_insight", "principle_alignment_rating", "overall_assessment_summary"]
        },
        "confidence_assessment_result": {
          "type": "object",
          "description": "Agent's final assessed confidence in the output after performing all validation steps.",
          "properties": {
            "score": {
              "type": "number", "minimum": 0, "maximum": 1,
              "description": "Numerical confidence score (0=None, 1=Maximum) regarding overall quality, reliability, and appropriateness."
            },
            "justification": {
              "type": "string",
              "description": "Explanation for the score, citing validation outcomes (internal checks, constraints, quality ratings), identified limitations, complexity, source reliability, and principle alignment."
            }
          },
          "required": ["score", "justification"]
        },
        "identified_issues_summary": {
            "type": "string",
            "description": "Concise summary of any significant issues, errors, constraint violations, or quality deficits identified during the entire validation process."
        },
        "final_verdict": {
          "type": "string",
          "enum": ["Proceed_To_Output", "Requires_Internal_Revision", "Discard_Output", "Flag_For_Human_Review"],
          "description": "The conclusive action determined by this validation protocol for the `generated_output`."
        },
        "error_analysis_details": {
            "type": "object",
            "description": "Optional: Detailed analysis if 'final_verdict' is 'Requires_Internal_Revision' or 'Discard_Output'. Required if issues were identified.",
            "properties": {
                "error_type": {
                    "type": "string",
                    "enum": ["Factual_Inaccuracy", "Logical_Fallacy", "Constraint_Violation", "Incompleteness", "Irrelevance", "Hallucination", "Style_Mismatch", "Bias_Detected", "Safety_Concern", "Other"],
                    "description": "Classification of the primary error(s)."
                },
                "error_description": {
                    "type": "string",
                    "description": "Specific description of the identified error(s) and their location/nature."
                },
                "error_severity": {
                  "type": "string",
                  "enum": ["Low", "Medium", "High", "Critical"],
                  "description": "Assessed severity of the identified error(s)."
                },
                "potential_impact": {
                  "type": "string",
                  "description": "Description of the potential negative impact if the error is not corrected."
                },
                "suggested_correction_strategy": {
                    "type": "string",
                    "description": "Proposed internal strategy to address the error(s) (e.g., 'Re-run reasoning step X with corrected premise', 'Rewrite section Y for clarity', 'Verify fact Z against internal KB', 'Apply constraint filter')."
                }
            },
            "required": ["error_type", "error_description", "error_severity", "suggested_correction_strategy"]
        },
        "refinement_actions_taken_post_validation": {
            "type": "string",
            "description": "Optional: Specific description of modifications applied to the `generated_output` *after* this validation process (e.g., based on `error_analysis_details`), before reaching the 'Proceed_To_Output' verdict."
        },
        "validation_flags": {
          "type": "array",
          "description": "Optional: List of internal flags raised during validation for tracking or future action.",
          "items": {
            "type": "string",
            "examples": ["Low_Confidence_Area", "Potential_Bias_Unmitigated", "Data_Gap_Identified", "Requires_Further_Verification", "Model_Improvement_Suggestion", "Prompt_Refinement_Needed"]
          }
        },
        "key_context_identifiers_used": {
          "type": "array",
          "description": "Optional: List of specific identifiers for key contextual elements actively used during generation or validation (e.g., document IDs, previous turn IDs).",
          "items": { "type": "string" }
        }
      },
      "required": [
        "internal_validation_checks_performed",
        "constraint_adherence_check_result",
        "response_quality_assessment_result",
        "confidence_assessment_result",
        "identified_issues_summary",
        "final_verdict"
      ]
    }
  }
}

